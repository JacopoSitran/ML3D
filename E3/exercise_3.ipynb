{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise 3: Shape Reconstruction\n",
    "\n",
    "**Submission Deadline**: 09.06.2021, 23:55\n",
    "\n",
    "We will take a look at two major approaches for 3D shape reconstruction in this last exercise.\n",
    "\n",
    "Like in exercise 2, you can run all trainings either locally or on Google Colab. Just follow the instructions below. \n",
    "\n",
    "Note that training reconstruction methods generally takes relatively long, even for simple shape completion. Training the generalization will take a few hours. *Thus, please make sure to start training well before the submission deadline.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0. Running this notebook\n",
    "We recommend running this notebook on a cuda compatible local gpu. You can also run training on cpu, it will just take longer.\n",
    "\n",
    "We describe two options for executing the training parts of this exercise below: Using Google Colab or running it locally on your machine. If you are not planning on using Colab, just skip forward to Local Execution.\n",
    "\n",
    "### Google Colab\n",
    "\n",
    "If you don't have access to gpu and don't wish to train on CPU, you can use Google Colab. However, we experienced the issue that inline visualization of shapes or inline images didn't work on colab, so just keep that in mind.\n",
    "What you can also do is only train networks on colab, download the checkpoint, and visualize inference locally.\n",
    "\n",
    "In case you're using Google Colab, you can upload the exercise folder (containing `exercise_3.ipynb`, directory `exercise_3` and the file `requirements.txt`) as `3d-machine-learning` to google drive (make sure you don't upload extracted datasets files).\n",
    "Additionally you'd need to open the notebook `exercise_3.ipynb` in Colab using `File > Open Notebook > Upload`.\n",
    "\n",
    "Next you'll need to run these two cells for setting up the environment. Before you do that make sure your instance has a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# We assume you uploaded the exercise folder in root Google Drive folder\n",
    "\n",
    "!cp -r /content/drive/MyDrive/3d-machine-learning 3d-machine-learning/\n",
    "os.chdir('/content/3d-machine-learning/')\n",
    "print('Installing requirements')\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Make sure you restart runtime when directed by Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell after restarting your colab runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "os.chdir('/content/3d-machine-learning/')\n",
    "sys.path.insert(1, \"/content/3d-machine-learning/\")\n",
    "print('CUDA availability:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Execution\n",
    "\n",
    "If you run this notebook locally, you have to first install the python dependiencies again. They are the same as for exercise 1 so you can re-use the environment you used last time. If you use [poetry](https://python-poetry.org), you can also simply re-install everything (`poetry install`) and then run this notebook via `poetry run jupyter notebook`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "The following imports should work regardless of whether you are using Colab or local execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import k3d\n",
    "import trimesh\n",
    "import torch\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the next cell to test whether a GPU was detected by pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Shape Reconstruction from 3D SDF grids with 3D-EPN\n",
    "\n",
    "In the first part of this exercise, we will take a look at shape complation using [3D-EPN](https://arxiv.org/abs/1612.00101). This approach was also introduced in the lecture.\n",
    "\n",
    "The visualization below shows an overview of the method: From an incomplete shape observation (which you would get when scanning an object with a depth sensor for example), we use a 3D encoder-predictor network that first encodes the incomplete shapes into a common latent space using several 3D convolution layers and then decodes them again using multiple 3D transpose convolutions.\n",
    "\n",
    "This way, we get from a 32^3 SDF voxel grid to a 32^3 DF (unsigned) voxel grid that represents the completed shape. We only focus on this part here; in the original implementation, this 32^3 completed prediction would then be further improved (in an offline step after inference) by sampling parts from a shape database to get the final resolution to 128^3.\n",
    "\n",
    "<img src=\"exercise_3/images/3depn_teaser.png\" alt=\"3D-EPN Teaser\" style=\"width: 800px;\"/>\n",
    "\n",
    "The next steps will follow the structure we established in exercise 2: Taking a look at the dataset structure and downloading the data; then, implementing dataset, model, and training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Downloading the data\n",
    "We will use the original dataset used in the official implementation. It consists of SDF and DF grids (representing incomplete input data and complete target data) with a resolution of 32^3 each. Each input-target pair is generated from a ShapeNet shape.\n",
    "\n",
    "The incomplete SDF data are generated by sampling virtual camera trajectories around every object. Each trajectory is assigned an ID which is part of the file names (see below). The camera views for each trajectory are combined into a common SDF grid by volumetric fusion. It is easy to generate an SDF here since we know both camera location and object surface: Everything between camera and surface is known free space and outside the object, leading to a positive SDF sign. Everything behind the surface has a negative sign. For the complete shapes, however, deciding whether a voxel in the DF grid is inside or outside an object is not a trivial problem. This is why we use unsigned distance fields as target and prediction representation instead. This still encodes the distance to the closest surface but does not contain explicit information about the inside/outside location.\n",
    "\n",
    "In terms of dataset layout, we follow the ShapeNet directory structure as seen in the last exercise:\n",
    "Each folder in the `exercise_3/data/shapenet_dim32_sdf` and `exercise_3/data/shapenet_dim32_df` directories contains one shape category represented by a number, e.g. `02691156`.\n",
    "We provide the mapping between these numbers and the corresponding names in `exercise_3/data/shape_info.json`. Each of these shape category folders contains lots of shapes in sdf or df format. In addition to that, every shape now also contains multiple trajectories: 0 to 7, encoded as `__0__` to `__7__`. These 8 files are just different input representations, meaning they vary in the level of completeness and location of missing parts; they all map to the `.df` file with corresponding shape ID and `__0__` at the end.\n",
    "\n",
    "```\n",
    "# contents of exercise_2/data/shapenet_dim32_sdf\n",
    "02691156/                                           # Shape category folder with all its shapes\n",
    "    ├── 10155655850468db78d106ce0a280f87__0__.sdf   # Trajectory 0 for a shape of the category\n",
    "    ├── 10155655850468db78d106ce0a280f87__1__.sdf   # Trajectory 1 for the same shape\n",
    "    ├── :                                      \n",
    "    ├── 10155655850468db78d106ce0a280f87__7__.sdf   # Trajectory 7 for the same shape\n",
    "    ├── 10155655850468db78d106ce0a280f87__0__.sdf   # Trajectory 0 for another shape\n",
    "    ├── :                                           # And so on ...\n",
    "02933112/                                           # Another shape category folder\n",
    "02958343/                                           # In total you should have 8 shape category folders\n",
    ":\n",
    "\n",
    "# contents of exercise_2/data/shapenet_dim32_df\n",
    "02691156/                                           # Shape category folder with all its shapes\n",
    "    ├── 10155655850468db78d106ce0a280f87__0__.df    # A single shape of the category\n",
    "    ├── 1021a0914a7207aff927ed529ad90a11__0__.df    # Another shape of the category\n",
    "    ├── :                                           # And so on ...\n",
    "02933112/                                           # Another shape category folder\n",
    "02958343/                                           # In total you should have 55 shape category folders\n",
    ":\n",
    "```\n",
    "\n",
    "Download and extract the data with the code cell below.\n",
    "\n",
    "**Note**: If you are training on Google Colab and are running out of disk space, you can do the following:\n",
    "- Only download the zip files below without extracting them (comment out all lines after `print('Extracting ...')`)\n",
    "- Change `from exercise_3.data.shapenet import ShapeNet` to `from exercise_3.data.shapenet_zip import ShapeNet`\n",
    "- Implement your dataset in `shapenet_zip.py`. This implementation extracts the data on-the-fly without taking up any additional disk space. Your training will therefore run a bit slower.\n",
    "- Make sure you uncomment the lines setting the worker_init_fn in `train_3depn.py` (marked with TODOs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Downloading ...')\n",
    "# File sizes: 11GB for shapenet_dim32_sdf.zip (incomplete scans), 4GB for shapenet_dim32_df.zip (target shapes)\n",
    "!wget http://kaldir.vc.in.tum.de/adai/CNNComplete/shapenet_dim32_sdf.zip -P exercise_3/data\n",
    "!wget http://kaldir.vc.in.tum.de/adai/CNNComplete/shapenet_dim32_df.zip -P exercise_3/data\n",
    "print('Extracting ...')\n",
    "!unzip -q exercise_3/data/shapenet_dim32_sdf.zip -d exercise_3/data\n",
    "!unzip -q exercise_3/data/shapenet_dim32_df.zip -d exercise_3/data\n",
    "!rm exercise_3/data/shapenet_dim32_sdf.zip\n",
    "!rm exercise_3/data/shapenet_dim32_df.zip\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Dataset\n",
    "\n",
    "The dataset implementation follows the same general structure as in exercise 2. We prepared an initial implementation already in `exercise_3/data/shapenet.py`; your task is to resolve all TODOs there.\n",
    "\n",
    "The data for SDFs and DFs in `.sdf`/`.df` files are stored in binary form as follows:\n",
    "```\n",
    "dimX    #uint64 \n",
    "dimY    #uint64 \n",
    "dimZ    #uint64 \n",
    "data    #(dimX*dimY*dimZ) floats for sdf/df values\n",
    "```\n",
    "The SDF values stored per-voxel represent the distance to the closest surface *in voxels*.\n",
    "\n",
    "You have to take care of three important steps before returning the SDF and DF for the corresponding `index` in `__getitem__`:\n",
    "1. **Truncation**: 3D-EPN uses a truncated SDF which means that for each voxel, the distance to the closest surface will be clamped to a max absolute value. This is helpful since we do not care about longer distances (Marching Cubes only cares about distances close to the surface). It allows us to focus our predictions on the voxels near the surface. We use a `truncation_distance` of 3 (voxels) which means we expect to get an SDF with values between -3 and 3 as input to the model.\n",
    "2. **Separation** of distances and sign: 3D-EPN uses as input a 2x32x32x32 SDF grid, with absolute distance values of the SDF in channel 0 and the signs (-1 or 1) in channel 1.\n",
    "3. **Log** scaling: We scale targets and prediction with a log operation to further guide predictions to focus on the surface voxels. Therefore, you should return target DFs as `log(df + 1)`.\n",
    "\n",
    "**Hint**: An easy way to load the data from `.sdf` and `.df` files is to use `np.fromfile`. First, load the dimensions, then the data, then reshape everything into the shape you loaded in the beginning. Make sure you get the datatypes and byte offsets right! If you are using the zip version of the dataset as explained above, you should use `np.frombuffer` instead of `np.fromfile` to load from the `data`-buffer. The syntax is identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set: 153540\n",
      "Length of val set: 32304\n",
      "Length of overfit set: 64\n"
     ]
    }
   ],
   "source": [
    "from exercise_3.data.shapenet import ShapeNet\n",
    "\n",
    "# Create a dataset with train split\n",
    "train_dataset = ShapeNet('train')\n",
    "val_dataset = ShapeNet('val')\n",
    "overfit_dataset = ShapeNet('overfit')\n",
    "\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of train set: {len(train_dataset)}')  # expected output: 153540\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of val set: {len(val_dataset)}')  # expected output: 32304\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of overfit set: {len(overfit_dataset)}')  # expected output: 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 03001627/798a46965d9e0edfcea003eff0268278__3__-03001627/798a46965d9e0edfcea003eff0268278__0__\n",
      "Input SDF: (2, 32, 32, 32)\n",
      "Target DF: (32, 32, 32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc9e984753045588507d156da272eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize some shapes\n",
    "from exercise_3.util.visualization import visualize_mesh\n",
    "from skimage.measure import marching_cubes\n",
    "\n",
    "train_sample = train_dataset[1]\n",
    "print(f'Name: {train_sample[\"name\"]}')  # expected output: 03001627/798a46965d9e0edfcea003eff0268278__3__-03001627/798a46965d9e0edfcea003eff0268278__0__\n",
    "print(f'Input SDF: {train_sample[\"input_sdf\"].shape}')  # expected output: (2, 32, 32, 32)\n",
    "print(f'Target DF: {train_sample[\"target_df\"].shape}')  # expected output: (32, 32, 32)\n",
    "\n",
    "input_mesh = marching_cubes(train_sample['input_sdf'][0], level=1)\n",
    "visualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 04379243/a1be21c9a71d133dc5beea20858a99d5__5__-04379243/a1be21c9a71d133dc5beea20858a99d5__0__\n",
      "Input SDF: (2, 32, 32, 32)\n",
      "Target DF: (32, 32, 32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd07f8e3c2494218b0ba749917c2375d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_sample = train_dataset[223]\n",
    "print(f'Name: {train_sample[\"name\"]}')  # expected output: 04379243/a1be21c9a71d133dc5beea20858a99d5__5__-04379243/a1be21c9a71d133dc5beea20858a99d5__0__\n",
    "print(f'Input SDF: {train_sample[\"input_sdf\"].shape}')  # expected output: (2, 32, 32, 32)\n",
    "print(f'Target DF: {train_sample[\"target_df\"].shape}')  # expected output: (32, 32, 32)\n",
    "\n",
    "input_mesh = marching_cubes(train_sample['input_sdf'][0], level=1)\n",
    "visualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 03636649/3889631e42a84b0f51f77a6d7299806__2__-03636649/3889631e42a84b0f51f77a6d7299806__0__\n",
      "Input SDF: (2, 32, 32, 32)\n",
      "Target DF: (32, 32, 32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d2cbea8941419ea71388070d1bb07a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_sample = train_dataset[95]\n",
    "print(f'Name: {train_sample[\"name\"]}')  # expected output: 03636649/3889631e42a84b0f51f77a6d7299806__2__-03636649/3889631e42a84b0f51f77a6d7299806__0__\n",
    "print(f'Input SDF: {train_sample[\"input_sdf\"].shape}')  # expected output: (2, 32, 32, 32)\n",
    "print(f'Target DF: {train_sample[\"target_df\"].shape}')  # expected output: (32, 32, 32)\n",
    "\n",
    "input_mesh = marching_cubes(train_sample['input_sdf'][0], level=1)\n",
    "visualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Model\n",
    "\n",
    "The model architecture of 3D-EPN is visualized below:\n",
    "\n",
    "<img src=\"exercise_3/images/3depn.png\" alt=\"3D-EPN Architecture\" style=\"width: 800px;\"/>\n",
    "\n",
    "For this exercise, we simplify the model by omitting the classification part - this will not have a big impact since most of the shape completion performance comes from the 3D encoder-decoder unet.\n",
    "\n",
    "The model consists of three parts: The encoder, the bottleneck, and the decoder. Encoder and decoder are constructed with the same architecture, just mirrored.\n",
    "\n",
    "The details of each part are:\n",
    "- **Encoder**: 4 layers, each one containing a 3D convolution (with kernel size 4, as seen in the visualization), a 3D batch norm (except the very first layer), and a leaky ReLU with a negative slope of 0.2. Our goal is to reduce the spatial dimension from 32x32x32 to 1x1x1 and to get the feature dimension from 2 (absolute values and sign) to `num_features * 8`. We do this by using a stride of 2 and padding of 1 for all convolutions except for the last one where we use a stride of 1 and no padding. The feature channels are increased from 2 to `num_features` in the first layer and then doubled with every subsequent layer.\n",
    "- **Decoder**: Same architecture as encoder, just mirrored: Going from `num_features * 8 * 2` (the 2 will be explained later) to 1 (the DF values). The spatial dimensions go from 1x1x1 to 32x32x32. Each layer use a 3D Transpose convolution now, together with 3D batch norm and ReLU (no leaky ReLUs anymore). Note that the last layer uses neither Batch Norms nor a ReLU since we do not want to constrain the range of possible values for the prediction.\n",
    "- **Bottleneck**: This is realized with 2 fully connected layers, each one going from a vector of size 640 (which is `num_features * 8`) to a vector of size 640. Each such layer is followed by a ReLU activation.\n",
    "\n",
    "Some minor details:\n",
    "- **Skip connections** allow the decoder to use information from the encoder and also improve gradient flow. We use it here to connect the output of encoder layer 1 to decoder layer 4, the output of encoder layer 2 to decoder layer 3, and so on. This means that the input to a decoder layer is the concatenation of the previous decoder output with the corresponding encoder output, along the feature dimension. Hence, the number of input features for each decoder layer are twice those of the encoder layers, as mentioned above.\n",
    "- **Log scaling**: You also need to scale the final outputs of the network logarithmically: `out = log(out + 1)`. This is the same transformation you applied to the target shapes in the dataloader before and ensures that prediction and target volumes are comparable.\n",
    "\n",
    "With this in mind, implement the network architecture and `forward()` function in `exercise_3/model/threedepn.py`. You can check your architecture with the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   | Name         | Type            | Params  \n",
      "----------------------------------------------------\n",
      "0  | encoder1     | Sequential      | 10320   \n",
      "1  | encoder1.0   | Conv3d          | 10320   \n",
      "2  | encoder1.1   | LeakyReLU       | 0       \n",
      "3  | encoder2     | Sequential      | 819680  \n",
      "4  | encoder2.0   | Conv3d          | 819360  \n",
      "5  | encoder2.1   | BatchNorm3d     | 320     \n",
      "6  | encoder2.2   | LeakyReLU       | 0       \n",
      "7  | encoder3     | Sequential      | 3277760 \n",
      "8  | encoder3.0   | Conv3d          | 3277120 \n",
      "9  | encoder3.1   | BatchNorm3d     | 640     \n",
      "10 | encoder3.2   | LeakyReLU       | 0       \n",
      "11 | encoder4     | Sequential      | 13109120\n",
      "12 | encoder4.0   | Conv3d          | 13107840\n",
      "13 | encoder4.1   | BatchNorm3d     | 1280    \n",
      "14 | encoder4.2   | LeakyReLU       | 0       \n",
      "15 | bottleneck   | Sequential      | 820480  \n",
      "16 | bottleneck.0 | Linear          | 410240  \n",
      "17 | bottleneck.1 | ReLU            | 0       \n",
      "18 | bottleneck.2 | Linear          | 410240  \n",
      "19 | bottleneck.3 | ReLU            | 0       \n",
      "20 | decoder1     | Sequential      | 26215360\n",
      "21 | decoder1.0   | ConvTranspose3d | 26214720\n",
      "22 | decoder1.1   | BatchNorm3d     | 640     \n",
      "23 | decoder1.2   | ReLU            | 0       \n",
      "24 | decoder2     | Sequential      | 6554080 \n",
      "25 | decoder2.0   | ConvTranspose3d | 6553760 \n",
      "26 | decoder2.1   | BatchNorm3d     | 320     \n",
      "27 | decoder2.2   | ReLU            | 0       \n",
      "28 | decoder3     | Sequential      | 1638640 \n",
      "29 | decoder3.0   | ConvTranspose3d | 1638480 \n",
      "30 | decoder3.1   | BatchNorm3d     | 160     \n",
      "31 | decoder3.2   | ReLU            | 0       \n",
      "32 | decoder4     | ConvTranspose3d | 10241   \n",
      "33 | TOTAL        | ThreeDEPN       | 52455681\n",
      "Output tensor shape:  torch.Size([4, 32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "from exercise_3.model.threedepn import ThreeDEPN\n",
    "from exercise_3.util.model import summarize_model\n",
    "\n",
    "threedepn = ThreeDEPN()\n",
    "print(summarize_model(threedepn))  # Expected: Rows 0-34 and TOTAL = 52455681\n",
    "\n",
    "sdf = torch.randn(4, 1, 32, 32, 32) * 2. - 1.\n",
    "input_tensor = torch.cat([torch.abs(sdf), torch.sign(sdf)], dim=1)\n",
    "predictions = threedepn(input_tensor)\n",
    "\n",
    "print('Output tensor shape: ', predictions.shape)  # Expected: torch.Size([4, 32, 32, 32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Training script and overfitting to a single shape reconstruction\n",
    "\n",
    "You can now go to the train script in `exercise_3/training/train_3depn.py` and fill in the missing pieces as you did for exercise 2. Then, verify that your training work by overfitting to a few samples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "[004/00001] train_loss: 0.084842\n",
      "[009/00001] train_loss: 0.021994\n",
      "[012/00000] val_loss: 0.672084 | best_loss_val: 0.672084\n",
      "[014/00001] train_loss: 0.013375\n",
      "[019/00001] train_loss: 0.009296\n",
      "[024/00001] train_loss: 0.007340\n",
      "[024/00001] val_loss: 0.230212 | best_loss_val: 0.230212\n",
      "[029/00001] train_loss: 0.006378\n",
      "[034/00001] train_loss: 0.005611\n",
      "[037/00000] val_loss: 0.200113 | best_loss_val: 0.200113\n",
      "[039/00001] train_loss: 0.004952\n",
      "[044/00001] train_loss: 0.004522\n",
      "[049/00001] train_loss: 0.004258\n",
      "[049/00001] val_loss: 0.174148 | best_loss_val: 0.174148\n",
      "[054/00001] train_loss: 0.003988\n",
      "[059/00001] train_loss: 0.003746\n",
      "[062/00000] val_loss: 0.158687 | best_loss_val: 0.158687\n",
      "[064/00001] train_loss: 0.003585\n",
      "[069/00001] train_loss: 0.003469\n",
      "[074/00001] train_loss: 0.003347\n",
      "[074/00001] val_loss: 0.149586 | best_loss_val: 0.149586\n",
      "[079/00001] train_loss: 0.003235\n",
      "[084/00001] train_loss: 0.003159\n",
      "[087/00000] val_loss: 0.143514 | best_loss_val: 0.143514\n",
      "[089/00001] train_loss: 0.003084\n",
      "[094/00001] train_loss: 0.003037\n",
      "[099/00001] train_loss: 0.002981\n",
      "[099/00001] val_loss: 0.139475 | best_loss_val: 0.139475\n",
      "[104/00001] train_loss: 0.002954\n",
      "[109/00001] train_loss: 0.002907\n",
      "[112/00000] val_loss: 0.137124 | best_loss_val: 0.137124\n",
      "[114/00001] train_loss: 0.002902\n",
      "[119/00001] train_loss: 0.002864\n",
      "[124/00001] train_loss: 0.002828\n",
      "[124/00001] val_loss: 0.135315 | best_loss_val: 0.135315\n",
      "[129/00001] train_loss: 0.002815\n",
      "[134/00001] train_loss: 0.002807\n",
      "[137/00000] val_loss: 0.134208 | best_loss_val: 0.134208\n",
      "[139/00001] train_loss: 0.002788\n",
      "[144/00001] train_loss: 0.002781\n",
      "[149/00001] train_loss: 0.002780\n",
      "[149/00001] val_loss: 0.133509 | best_loss_val: 0.133509\n",
      "[154/00001] train_loss: 0.002771\n",
      "[159/00001] train_loss: 0.002756\n",
      "[162/00000] val_loss: 0.132990 | best_loss_val: 0.132990\n",
      "[164/00001] train_loss: 0.002749\n",
      "[169/00001] train_loss: 0.002746\n",
      "[174/00001] train_loss: 0.002742\n",
      "[174/00001] val_loss: 0.132702 | best_loss_val: 0.132702\n",
      "[179/00001] train_loss: 0.002741\n",
      "[184/00001] train_loss: 0.002743\n",
      "[187/00000] val_loss: 0.132488 | best_loss_val: 0.132488\n",
      "[189/00001] train_loss: 0.002731\n",
      "[194/00001] train_loss: 0.002728\n",
      "[199/00001] train_loss: 0.002727\n",
      "[199/00001] val_loss: 0.132322 | best_loss_val: 0.132322\n",
      "[204/00001] train_loss: 0.002723\n",
      "[209/00001] train_loss: 0.002725\n",
      "[212/00000] val_loss: 0.132212 | best_loss_val: 0.132212\n",
      "[214/00001] train_loss: 0.002730\n",
      "[219/00001] train_loss: 0.002716\n",
      "[224/00001] train_loss: 0.002723\n",
      "[224/00001] val_loss: 0.132172 | best_loss_val: 0.132172\n",
      "[229/00001] train_loss: 0.002714\n",
      "[234/00001] train_loss: 0.002730\n",
      "[237/00000] val_loss: 0.132092 | best_loss_val: 0.132092\n",
      "[239/00001] train_loss: 0.002736\n",
      "[244/00001] train_loss: 0.002726\n",
      "[249/00001] train_loss: 0.002714\n",
      "[249/00001] val_loss: 0.132095 | best_loss_val: 0.132092\n"
     ]
    }
   ],
   "source": [
    "from exercise_3.training import train_3depn\n",
    "config = {\n",
    "    'experiment_name': '3_1_3depn_overfitting',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': True,\n",
    "    'batch_size': 32,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate': 0.001,\n",
    "    'max_epochs': 250,\n",
    "    'print_every_n': 10,\n",
    "    'validate_every_n': 25,\n",
    "}\n",
    "train_3depn.main(config)  # should be able to get <0.0025 train_loss and <0.13 val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Training over the entire training set\n",
    "If the overfitting works, we can go ahead with training on the entire dataset.\n",
    "\n",
    "**Note**: As is the case with most reconstruction networks and considering the size of the model (> 50M parameters), this training will take a few hours on a GPU. *Please make sure to start training early enough before the submission deadline.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "[000/00049] train_loss: 0.034444\n",
      "[000/00099] train_loss: 0.014526\n",
      "[000/00149] train_loss: 0.011572\n",
      "[000/00199] train_loss: 0.010452\n",
      "[000/00249] train_loss: 0.009456\n",
      "[000/00299] train_loss: 0.008952\n",
      "[000/00349] train_loss: 0.008701\n",
      "[000/00399] train_loss: 0.008090\n",
      "[000/00449] train_loss: 0.007697\n",
      "[000/00499] train_loss: 0.007603\n",
      "[000/00549] train_loss: 0.007666\n",
      "[000/00599] train_loss: 0.007099\n",
      "[000/00649] train_loss: 0.007397\n",
      "[000/00699] train_loss: 0.006949\n",
      "[000/00749] train_loss: 0.007345\n",
      "[000/00799] train_loss: 0.006401\n",
      "[000/00849] train_loss: 0.006270\n",
      "[000/00899] train_loss: 0.006830\n",
      "[000/00949] train_loss: 0.006238\n",
      "[000/00999] train_loss: 0.006345\n",
      "[000/00999] val_loss: 0.157648 | best_loss_val: 0.157648\n",
      "[000/01049] train_loss: 0.006328\n",
      "[000/01099] train_loss: 0.005966\n",
      "[000/01149] train_loss: 0.005879\n",
      "[000/01199] train_loss: 0.006177\n",
      "[000/01249] train_loss: 0.006004\n",
      "[000/01299] train_loss: 0.005956\n",
      "[000/01349] train_loss: 0.005497\n",
      "[000/01399] train_loss: 0.005893\n",
      "[000/01449] train_loss: 0.005894\n",
      "[000/01499] train_loss: 0.005636\n",
      "[000/01549] train_loss: 0.005724\n",
      "[000/01599] train_loss: 0.005558\n",
      "[000/01649] train_loss: 0.005864\n",
      "[000/01699] train_loss: 0.005749\n",
      "[000/01749] train_loss: 0.005144\n",
      "[000/01799] train_loss: 0.005390\n",
      "[000/01849] train_loss: 0.005485\n",
      "[000/01899] train_loss: 0.005236\n",
      "[000/01949] train_loss: 0.005427\n",
      "[000/01999] train_loss: 0.005742\n",
      "[000/01999] val_loss: 0.431066 | best_loss_val: 0.157648\n",
      "[000/02049] train_loss: 0.005621\n",
      "[000/02099] train_loss: 0.005265\n",
      "[000/02149] train_loss: 0.005021\n",
      "[000/02199] train_loss: 0.005370\n",
      "[000/02249] train_loss: 0.005096\n",
      "[000/02299] train_loss: 0.005199\n",
      "[000/02349] train_loss: 0.004911\n",
      "[000/02399] train_loss: 0.004960\n",
      "[000/02449] train_loss: 0.005456\n",
      "[000/02499] train_loss: 0.005386\n",
      "[000/02549] train_loss: 0.004943\n",
      "[000/02599] train_loss: 0.004855\n",
      "[000/02649] train_loss: 0.004847\n",
      "[000/02699] train_loss: 0.005077\n",
      "[000/02749] train_loss: 0.005373\n",
      "[000/02799] train_loss: 0.005362\n",
      "[000/02849] train_loss: 0.005210\n",
      "[000/02899] train_loss: 0.005109\n",
      "[000/02949] train_loss: 0.004641\n",
      "[000/02999] train_loss: 0.004831\n",
      "[000/02999] val_loss: 0.185709 | best_loss_val: 0.157648\n",
      "[000/03049] train_loss: 0.004803\n",
      "[000/03099] train_loss: 0.004754\n",
      "[000/03149] train_loss: 0.005102\n",
      "[000/03199] train_loss: 0.004783\n",
      "[000/03249] train_loss: 0.005013\n",
      "[000/03299] train_loss: 0.004353\n",
      "[000/03349] train_loss: 0.004724\n",
      "[000/03399] train_loss: 0.004591\n",
      "[000/03449] train_loss: 0.004706\n",
      "[000/03499] train_loss: 0.004660\n",
      "[000/03549] train_loss: 0.004469\n",
      "[000/03599] train_loss: 0.004729\n",
      "[000/03649] train_loss: 0.004734\n",
      "[000/03699] train_loss: 0.004565\n",
      "[000/03749] train_loss: 0.004664\n",
      "[000/03799] train_loss: 0.004712\n",
      "[000/03849] train_loss: 0.004486\n",
      "[000/03899] train_loss: 0.004648\n",
      "[000/03949] train_loss: 0.004531\n",
      "[000/03999] train_loss: 0.004585\n",
      "[000/03999] val_loss: 0.191739 | best_loss_val: 0.157648\n",
      "[000/04049] train_loss: 0.004378\n",
      "[000/04099] train_loss: 0.004572\n",
      "[000/04149] train_loss: 0.004359\n",
      "[000/04199] train_loss: 0.004466\n",
      "[000/04249] train_loss: 0.004405\n",
      "[000/04299] train_loss: 0.004558\n",
      "[000/04349] train_loss: 0.004428\n",
      "[000/04399] train_loss: 0.004292\n",
      "[000/04449] train_loss: 0.004501\n",
      "[000/04499] train_loss: 0.004650\n",
      "[000/04549] train_loss: 0.004514\n",
      "[000/04599] train_loss: 0.004622\n",
      "[000/04649] train_loss: 0.004477\n",
      "[000/04699] train_loss: 0.004283\n",
      "[000/04749] train_loss: 0.004170\n",
      "[001/00000] train_loss: 0.004345\n",
      "[001/00050] train_loss: 0.004413\n",
      "[001/00100] train_loss: 0.004424\n",
      "[001/00150] train_loss: 0.004486\n",
      "[001/00200] train_loss: 0.004161\n",
      "[001/00200] val_loss: 0.202682 | best_loss_val: 0.157648\n",
      "[001/00250] train_loss: 0.004186\n",
      "[001/00300] train_loss: 0.004216\n",
      "[001/00350] train_loss: 0.003825\n",
      "[001/00400] train_loss: 0.004072\n",
      "[001/00450] train_loss: 0.004257\n",
      "[001/00500] train_loss: 0.004217\n",
      "[001/00550] train_loss: 0.003949\n",
      "[001/00600] train_loss: 0.004042\n",
      "[001/00650] train_loss: 0.004128\n",
      "[001/00700] train_loss: 0.004348\n",
      "[001/00750] train_loss: 0.004232\n",
      "[001/00800] train_loss: 0.004103\n",
      "[001/00850] train_loss: 0.004035\n",
      "[001/00900] train_loss: 0.004383\n",
      "[001/00950] train_loss: 0.003963\n",
      "[001/01000] train_loss: 0.004247\n",
      "[001/01050] train_loss: 0.004276\n",
      "[001/01100] train_loss: 0.004162\n",
      "[001/01150] train_loss: 0.004193\n",
      "[001/01200] train_loss: 0.004286\n",
      "[001/01200] val_loss: 0.120705 | best_loss_val: 0.120705\n",
      "[001/01250] train_loss: 0.004102\n",
      "[001/01300] train_loss: 0.003989\n",
      "[001/01350] train_loss: 0.004197\n",
      "[001/01400] train_loss: 0.004431\n",
      "[001/01450] train_loss: 0.004155\n",
      "[001/01500] train_loss: 0.004103\n",
      "[001/01550] train_loss: 0.004353\n",
      "[001/01600] train_loss: 0.004120\n",
      "[001/01650] train_loss: 0.003918\n",
      "[001/01700] train_loss: 0.004004\n",
      "[001/01750] train_loss: 0.004313\n",
      "[001/01800] train_loss: 0.004267\n",
      "[001/01850] train_loss: 0.004127\n",
      "[001/01900] train_loss: 0.004083\n",
      "[001/01950] train_loss: 0.003989\n",
      "[001/02000] train_loss: 0.003753\n",
      "[001/02050] train_loss: 0.003918\n",
      "[001/02100] train_loss: 0.003747\n",
      "[001/02150] train_loss: 0.003791\n",
      "[001/02200] train_loss: 0.003892\n",
      "[001/02200] val_loss: 0.271724 | best_loss_val: 0.120705\n",
      "[001/02250] train_loss: 0.003871\n",
      "[001/02300] train_loss: 0.003847\n",
      "[001/02350] train_loss: 0.003814\n",
      "[001/02400] train_loss: 0.003902\n",
      "[001/02450] train_loss: 0.003842\n",
      "[001/02500] train_loss: 0.004171\n",
      "[001/02550] train_loss: 0.003934\n",
      "[001/02600] train_loss: 0.003958\n",
      "[001/02650] train_loss: 0.003969\n",
      "[001/02700] train_loss: 0.004137\n",
      "[001/02750] train_loss: 0.003944\n",
      "[001/02800] train_loss: 0.004099\n",
      "[001/02850] train_loss: 0.003725\n",
      "[001/02900] train_loss: 0.003791\n",
      "[001/02950] train_loss: 0.003948\n",
      "[001/03000] train_loss: 0.004063\n",
      "[001/03050] train_loss: 0.003807\n",
      "[001/03100] train_loss: 0.003550\n",
      "[001/03150] train_loss: 0.004006\n",
      "[001/03200] train_loss: 0.003873\n",
      "[001/03200] val_loss: 0.137527 | best_loss_val: 0.120705\n",
      "[001/03250] train_loss: 0.003754\n",
      "[001/03300] train_loss: 0.003654\n",
      "[001/03350] train_loss: 0.003750\n",
      "[001/03400] train_loss: 0.003730\n",
      "[001/03450] train_loss: 0.003636\n",
      "[001/03500] train_loss: 0.003882\n",
      "[001/03550] train_loss: 0.004133\n",
      "[001/03600] train_loss: 0.003677\n",
      "[001/03650] train_loss: 0.003873\n",
      "[001/03700] train_loss: 0.004051\n",
      "[001/03750] train_loss: 0.003649\n",
      "[001/03800] train_loss: 0.003830\n",
      "[001/03850] train_loss: 0.003717\n",
      "[001/03900] train_loss: 0.003882\n",
      "[001/03950] train_loss: 0.003705\n",
      "[001/04000] train_loss: 0.003747\n",
      "[001/04050] train_loss: 0.003722\n",
      "[001/04100] train_loss: 0.003593\n",
      "[001/04150] train_loss: 0.003939\n",
      "[001/04200] train_loss: 0.003950\n",
      "[001/04200] val_loss: 0.156287 | best_loss_val: 0.120705\n",
      "[001/04250] train_loss: 0.003718\n",
      "[001/04300] train_loss: 0.003662\n",
      "[001/04350] train_loss: 0.003888\n",
      "[001/04400] train_loss: 0.003797\n",
      "[001/04450] train_loss: 0.003798\n",
      "[001/04500] train_loss: 0.003890\n",
      "[001/04550] train_loss: 0.003827\n",
      "[001/04600] train_loss: 0.003614\n",
      "[001/04650] train_loss: 0.003661\n",
      "[001/04700] train_loss: 0.003570\n",
      "[001/04750] train_loss: 0.003736\n",
      "[002/00001] train_loss: 0.003722\n",
      "[002/00051] train_loss: 0.003589\n",
      "[002/00101] train_loss: 0.003405\n",
      "[002/00151] train_loss: 0.003430\n",
      "[002/00201] train_loss: 0.003646\n",
      "[002/00251] train_loss: 0.003419\n",
      "[002/00301] train_loss: 0.003176\n",
      "[002/00351] train_loss: 0.003541\n",
      "[002/00401] train_loss: 0.003469\n",
      "[002/00401] val_loss: 0.136483 | best_loss_val: 0.120705\n",
      "[002/00451] train_loss: 0.003371\n",
      "[002/00501] train_loss: 0.003677\n",
      "[002/00551] train_loss: 0.003600\n",
      "[002/00601] train_loss: 0.003448\n",
      "[002/00651] train_loss: 0.003319\n",
      "[002/00701] train_loss: 0.003528\n",
      "[002/00751] train_loss: 0.003390\n",
      "[002/00801] train_loss: 0.003389\n",
      "[002/00851] train_loss: 0.003521\n",
      "[002/00901] train_loss: 0.003346\n",
      "[002/00951] train_loss: 0.003480\n",
      "[002/01001] train_loss: 0.003303\n",
      "[002/01051] train_loss: 0.003269\n",
      "[002/01101] train_loss: 0.003427\n",
      "[002/01151] train_loss: 0.003244\n",
      "[002/01201] train_loss: 0.003337\n",
      "[002/01251] train_loss: 0.003618\n",
      "[002/01301] train_loss: 0.003247\n",
      "[002/01351] train_loss: 0.003517\n",
      "[002/01401] train_loss: 0.003448\n",
      "[002/01401] val_loss: 0.130138 | best_loss_val: 0.120705\n",
      "[002/01451] train_loss: 0.003481\n",
      "[002/01501] train_loss: 0.003430\n",
      "[002/01551] train_loss: 0.003398\n",
      "[002/01601] train_loss: 0.003346\n",
      "[002/01651] train_loss: 0.003449\n",
      "[002/01701] train_loss: 0.003646\n",
      "[002/01751] train_loss: 0.003396\n",
      "[002/01801] train_loss: 0.003492\n",
      "[002/01851] train_loss: 0.003694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[002/01901] train_loss: 0.003193\n",
      "[002/01951] train_loss: 0.003143\n",
      "[002/02001] train_loss: 0.003232\n",
      "[002/02051] train_loss: 0.003288\n",
      "[002/02101] train_loss: 0.003311\n",
      "[002/02151] train_loss: 0.003639\n",
      "[002/02201] train_loss: 0.003265\n",
      "[002/02251] train_loss: 0.003580\n",
      "[002/02301] train_loss: 0.003555\n",
      "[002/02351] train_loss: 0.003162\n",
      "[002/02401] train_loss: 0.003618\n",
      "[002/02401] val_loss: 0.116835 | best_loss_val: 0.116835\n",
      "[002/02451] train_loss: 0.003353\n",
      "[002/02501] train_loss: 0.003251\n",
      "[002/02551] train_loss: 0.003196\n",
      "[002/02601] train_loss: 0.003251\n",
      "[002/02651] train_loss: 0.003422\n",
      "[002/02701] train_loss: 0.003413\n",
      "[002/02751] train_loss: 0.003490\n",
      "[002/02801] train_loss: 0.003328\n",
      "[002/02851] train_loss: 0.003530\n",
      "[002/02901] train_loss: 0.003395\n",
      "[002/02951] train_loss: 0.003485\n",
      "[002/03001] train_loss: 0.003466\n",
      "[002/03051] train_loss: 0.003595\n",
      "[002/03101] train_loss: 0.003441\n",
      "[002/03151] train_loss: 0.003360\n",
      "[002/03201] train_loss: 0.003483\n",
      "[002/03251] train_loss: 0.003539\n",
      "[002/03301] train_loss: 0.003312\n",
      "[002/03351] train_loss: 0.003442\n",
      "[002/03401] train_loss: 0.003293\n",
      "[002/03401] val_loss: 0.158377 | best_loss_val: 0.116835\n",
      "[002/03451] train_loss: 0.003288\n",
      "[002/03501] train_loss: 0.003153\n",
      "[002/03551] train_loss: 0.003482\n",
      "[002/03601] train_loss: 0.003362\n",
      "[002/03651] train_loss: 0.003225\n",
      "[002/03701] train_loss: 0.003413\n",
      "[002/03751] train_loss: 0.003255\n",
      "[002/03801] train_loss: 0.003456\n",
      "[002/03851] train_loss: 0.003298\n",
      "[002/03901] train_loss: 0.003232\n",
      "[002/03951] train_loss: 0.003424\n",
      "[002/04001] train_loss: 0.003264\n",
      "[002/04051] train_loss: 0.003242\n",
      "[002/04101] train_loss: 0.003396\n",
      "[002/04151] train_loss: 0.003386\n",
      "[002/04201] train_loss: 0.003364\n",
      "[002/04251] train_loss: 0.003461\n",
      "[002/04301] train_loss: 0.003030\n",
      "[002/04351] train_loss: 0.003170\n",
      "[002/04401] train_loss: 0.003223\n",
      "[002/04401] val_loss: 0.127972 | best_loss_val: 0.116835\n",
      "[002/04451] train_loss: 0.003145\n",
      "[002/04501] train_loss: 0.003407\n",
      "[002/04551] train_loss: 0.003150\n",
      "[002/04601] train_loss: 0.003566\n",
      "[002/04651] train_loss: 0.003356\n",
      "[002/04701] train_loss: 0.003327\n",
      "[002/04751] train_loss: 0.003073\n",
      "[003/00002] train_loss: 0.003159\n",
      "[003/00052] train_loss: 0.003063\n",
      "[003/00102] train_loss: 0.003110\n",
      "[003/00152] train_loss: 0.002908\n",
      "[003/00202] train_loss: 0.002910\n",
      "[003/00252] train_loss: 0.002888\n",
      "[003/00302] train_loss: 0.002997\n",
      "[003/00352] train_loss: 0.003012\n",
      "[003/00402] train_loss: 0.003042\n",
      "[003/00452] train_loss: 0.002952\n",
      "[003/00502] train_loss: 0.003154\n",
      "[003/00552] train_loss: 0.002992\n",
      "[003/00602] train_loss: 0.003067\n",
      "[003/00602] val_loss: 0.109667 | best_loss_val: 0.109667\n",
      "[003/00652] train_loss: 0.002995\n",
      "[003/00702] train_loss: 0.003117\n",
      "[003/00752] train_loss: 0.003038\n",
      "[003/00802] train_loss: 0.002888\n",
      "[003/00852] train_loss: 0.002923\n",
      "[003/00902] train_loss: 0.003074\n",
      "[003/00952] train_loss: 0.003125\n",
      "[003/01002] train_loss: 0.002953\n",
      "[003/01052] train_loss: 0.002913\n",
      "[003/01102] train_loss: 0.002943\n",
      "[003/01152] train_loss: 0.003034\n",
      "[003/01202] train_loss: 0.002948\n",
      "[003/01252] train_loss: 0.002929\n",
      "[003/01302] train_loss: 0.002911\n",
      "[003/01352] train_loss: 0.002999\n",
      "[003/01402] train_loss: 0.002919\n",
      "[003/01452] train_loss: 0.003033\n",
      "[003/01502] train_loss: 0.002933\n",
      "[003/01552] train_loss: 0.003027\n",
      "[003/01602] train_loss: 0.002861\n",
      "[003/01602] val_loss: 0.086990 | best_loss_val: 0.086990\n",
      "[003/01652] train_loss: 0.003002\n",
      "[003/01702] train_loss: 0.003200\n",
      "[003/01752] train_loss: 0.002974\n",
      "[003/01802] train_loss: 0.002919\n",
      "[003/01852] train_loss: 0.003070\n",
      "[003/01902] train_loss: 0.002925\n",
      "[003/01952] train_loss: 0.003040\n",
      "[003/02002] train_loss: 0.002994\n",
      "[003/02052] train_loss: 0.002874\n",
      "[003/02102] train_loss: 0.002966\n",
      "[003/02152] train_loss: 0.003166\n",
      "[003/02202] train_loss: 0.002861\n",
      "[003/02252] train_loss: 0.003081\n",
      "[003/02302] train_loss: 0.002907\n",
      "[003/02352] train_loss: 0.002913\n",
      "[003/02402] train_loss: 0.003164\n",
      "[003/02452] train_loss: 0.003043\n",
      "[003/02502] train_loss: 0.003040\n",
      "[003/02552] train_loss: 0.002959\n",
      "[003/02602] train_loss: 0.003228\n",
      "[003/02602] val_loss: 0.097315 | best_loss_val: 0.086990\n",
      "[003/02652] train_loss: 0.002797\n",
      "[003/02702] train_loss: 0.003106\n",
      "[003/02752] train_loss: 0.002952\n",
      "[003/02802] train_loss: 0.003099\n",
      "[003/02852] train_loss: 0.003003\n",
      "[003/02902] train_loss: 0.002985\n",
      "[003/02952] train_loss: 0.003026\n",
      "[003/03002] train_loss: 0.002978\n",
      "[003/03052] train_loss: 0.002855\n",
      "[003/03102] train_loss: 0.002905\n",
      "[003/03152] train_loss: 0.002973\n",
      "[003/03202] train_loss: 0.003157\n",
      "[003/03252] train_loss: 0.003001\n",
      "[003/03302] train_loss: 0.002887\n",
      "[003/03352] train_loss: 0.003047\n",
      "[003/03402] train_loss: 0.003139\n",
      "[003/03452] train_loss: 0.003212\n",
      "[003/03502] train_loss: 0.003185\n",
      "[003/03552] train_loss: 0.002955\n",
      "[003/03602] train_loss: 0.002894\n",
      "[003/03602] val_loss: 0.132340 | best_loss_val: 0.086990\n",
      "[003/03652] train_loss: 0.002888\n",
      "[003/03702] train_loss: 0.002834\n",
      "[003/03752] train_loss: 0.002902\n",
      "[003/03802] train_loss: 0.002963\n",
      "[003/03852] train_loss: 0.002979\n",
      "[003/03902] train_loss: 0.003230\n",
      "[003/03952] train_loss: 0.003081\n",
      "[003/04002] train_loss: 0.003114\n",
      "[003/04052] train_loss: 0.002973\n",
      "[003/04102] train_loss: 0.002941\n",
      "[003/04152] train_loss: 0.003098\n",
      "[003/04202] train_loss: 0.003063\n",
      "[003/04252] train_loss: 0.003238\n",
      "[003/04302] train_loss: 0.003060\n",
      "[003/04352] train_loss: 0.002788\n",
      "[003/04402] train_loss: 0.003036\n",
      "[003/04452] train_loss: 0.002856\n",
      "[003/04502] train_loss: 0.003021\n",
      "[003/04552] train_loss: 0.003002\n",
      "[003/04602] train_loss: 0.002856\n",
      "[003/04602] val_loss: 0.104441 | best_loss_val: 0.086990\n",
      "[003/04652] train_loss: 0.002872\n",
      "[003/04702] train_loss: 0.003152\n",
      "[003/04752] train_loss: 0.002876\n",
      "[004/00003] train_loss: 0.002863\n",
      "[004/00053] train_loss: 0.002668\n",
      "[004/00103] train_loss: 0.002741\n",
      "[004/00153] train_loss: 0.002472\n",
      "[004/00203] train_loss: 0.002516\n",
      "[004/00253] train_loss: 0.002825\n",
      "[004/00303] train_loss: 0.002661\n",
      "[004/00353] train_loss: 0.002720\n",
      "[004/00403] train_loss: 0.002805\n",
      "[004/00453] train_loss: 0.002557\n",
      "[004/00503] train_loss: 0.002631\n",
      "[004/00553] train_loss: 0.002771\n",
      "[004/00603] train_loss: 0.002786\n",
      "[004/00653] train_loss: 0.002763\n",
      "[004/00703] train_loss: 0.002541\n",
      "[004/00753] train_loss: 0.002725\n",
      "[004/00803] train_loss: 0.002860\n",
      "[004/00803] val_loss: 0.128433 | best_loss_val: 0.086990\n",
      "[004/00853] train_loss: 0.002757\n",
      "[004/00903] train_loss: 0.002940\n",
      "[004/00953] train_loss: 0.002689\n",
      "[004/01003] train_loss: 0.002599\n",
      "[004/01053] train_loss: 0.002521\n",
      "[004/01103] train_loss: 0.002630\n",
      "[004/01153] train_loss: 0.002646\n",
      "[004/01203] train_loss: 0.002683\n",
      "[004/01253] train_loss: 0.002607\n",
      "[004/01303] train_loss: 0.002644\n",
      "[004/01353] train_loss: 0.002852\n",
      "[004/01403] train_loss: 0.002557\n",
      "[004/01453] train_loss: 0.002894\n",
      "[004/01503] train_loss: 0.002762\n",
      "[004/01553] train_loss: 0.002742\n",
      "[004/01603] train_loss: 0.002579\n",
      "[004/01653] train_loss: 0.002871\n",
      "[004/01703] train_loss: 0.002766\n",
      "[004/01753] train_loss: 0.002778\n",
      "[004/01803] train_loss: 0.002711\n",
      "[004/01803] val_loss: 0.539517 | best_loss_val: 0.086990\n",
      "[004/01853] train_loss: 0.002925\n",
      "[004/01903] train_loss: 0.002628\n",
      "[004/01953] train_loss: 0.002915\n",
      "[004/02003] train_loss: 0.002797\n",
      "[004/02053] train_loss: 0.002614\n",
      "[004/02103] train_loss: 0.002684\n",
      "[004/02153] train_loss: 0.002650\n",
      "[004/02203] train_loss: 0.002713\n",
      "[004/02253] train_loss: 0.002720\n",
      "[004/02303] train_loss: 0.002612\n",
      "[004/02353] train_loss: 0.002858\n",
      "[004/02403] train_loss: 0.002628\n",
      "[004/02453] train_loss: 0.002463\n",
      "[004/02503] train_loss: 0.002593\n",
      "[004/02553] train_loss: 0.002647\n",
      "[004/02603] train_loss: 0.002625\n",
      "[004/02653] train_loss: 0.002611\n",
      "[004/02703] train_loss: 0.002570\n",
      "[004/02753] train_loss: 0.002512\n",
      "[004/02803] train_loss: 0.002647\n",
      "[004/02803] val_loss: 0.085872 | best_loss_val: 0.085872\n",
      "[004/02853] train_loss: 0.002837\n",
      "[004/02903] train_loss: 0.002694\n",
      "[004/02953] train_loss: 0.002730\n",
      "[004/03003] train_loss: 0.002699\n",
      "[004/03053] train_loss: 0.002693\n",
      "[004/03103] train_loss: 0.002562\n",
      "[004/03153] train_loss: 0.002930\n",
      "[004/03203] train_loss: 0.002726\n",
      "[004/03253] train_loss: 0.002688\n",
      "[004/03303] train_loss: 0.002790\n",
      "[004/03353] train_loss: 0.002816\n",
      "[004/03403] train_loss: 0.002621\n",
      "[004/03453] train_loss: 0.002592\n",
      "[004/03503] train_loss: 0.002862\n",
      "[004/03553] train_loss: 0.002812\n",
      "[004/03603] train_loss: 0.002681\n",
      "[004/03653] train_loss: 0.002767\n",
      "[004/03703] train_loss: 0.002787\n",
      "[004/03753] train_loss: 0.002675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[004/03803] train_loss: 0.002747\n",
      "[004/03803] val_loss: 0.085675 | best_loss_val: 0.085675\n",
      "[004/03853] train_loss: 0.002753\n",
      "[004/03903] train_loss: 0.002770\n",
      "[004/03953] train_loss: 0.002659\n",
      "[004/04003] train_loss: 0.002601\n",
      "[004/04053] train_loss: 0.002664\n",
      "[004/04103] train_loss: 0.002756\n",
      "[004/04153] train_loss: 0.002840\n",
      "[004/04203] train_loss: 0.002600\n",
      "[004/04253] train_loss: 0.002822\n",
      "[004/04303] train_loss: 0.002756\n",
      "[004/04353] train_loss: 0.002803\n",
      "[004/04403] train_loss: 0.002709\n",
      "[004/04453] train_loss: 0.002767\n",
      "[004/04503] train_loss: 0.002567\n",
      "[004/04553] train_loss: 0.002663\n",
      "[004/04603] train_loss: 0.002786\n",
      "[004/04653] train_loss: 0.002679\n",
      "[004/04703] train_loss: 0.002612\n",
      "[004/04753] train_loss: 0.002588\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'experiment_name': '3_1_3depn_generalization',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 32,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate': 0.001,\n",
    "    'max_epochs': 5,\n",
    "    'print_every_n': 50,\n",
    "    'validate_every_n': 1000,\n",
    "}\n",
    "train_3depn.main(config)  # should be able to get best_loss_val < 0.1 after a few hours and 5 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f) Inference\n",
    "\n",
    "Implement the missing bits in `exercise_3/inference/infer_3depn.py`. You should then be able to see your reconstructions below.\n",
    "\n",
    "The outputs of our provided visualization functions are, from left to right:\n",
    "- Input, partial shape\n",
    "- Predicted completion\n",
    "- Target shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_3.util.visualization import visualize_meshes\n",
    "from exercise_3.inference.infer_3depn import InferenceHandler3DEPN\n",
    "\n",
    "# create a handler for inference using a trained checkpoint\n",
    "inferer = InferenceHandler3DEPN('exercise_3/runs/3_1_3depn_generalization/model_best.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702f050ec8024ee3ac053f26c9552e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_sdf = ShapeNet.get_shape_sdf('03636649/b286c9c136784db2af1744fdb1fbe7df__0__')\n",
    "target_df = ShapeNet.get_shape_df('03636649/b286c9c136784db2af1744fdb1fbe7df__0__')\n",
    "\n",
    "input_mesh, reconstructed_mesh, target_mesh = inferer.infer_single(input_sdf, target_df)\n",
    "visualize_meshes([input_mesh, reconstructed_mesh, target_mesh], flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6d4715480f45839bcd4a3bd2bec8f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_sdf = ShapeNet.get_shape_sdf('03636649/23eaba9bdd51a5b0dfe9cab879fd37e8__1__')\n",
    "target_df = ShapeNet.get_shape_df('03636649/23eaba9bdd51a5b0dfe9cab879fd37e8__0__')\n",
    "\n",
    "input_mesh, reconstructed_mesh, target_mesh = inferer.infer_single(input_sdf, target_df)\n",
    "visualize_meshes([input_mesh, reconstructed_mesh, target_mesh], flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993094a3c4044528a36cc7633089b6ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_sdf = ShapeNet.get_shape_sdf('02691156/5de2cc606b65b960e0b6546e08902f28__0__')\n",
    "target_df = ShapeNet.get_shape_df('02691156/5de2cc606b65b960e0b6546e08902f28__0__')\n",
    "\n",
    "input_mesh, reconstructed_mesh, target_mesh = inferer.infer_single(input_sdf, target_df)\n",
    "visualize_meshes([input_mesh, reconstructed_mesh, target_mesh], flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 DeepSDF\n",
    "\n",
    "\n",
    "Here, we will take a look at 3D-reconstruction using [DeepSDF](https://arxiv.org/abs/1901.05103). We recommend reading the paper before attempting the exercise.\n",
    "\n",
    "DeepSDF is an auto-decoder based approach that learns a continuous SDF representation for a class of shapes. Once trained, it can be used for shape representation, interpolation and shape completion. We'll look at each of these\n",
    "applications.\n",
    "\n",
    "<img src=\"exercise_3/images/deepsdf_teaser.png\" alt=\"deepsdf_teaser\" style=\"width: 800px;\"/>\n",
    "\n",
    "During training, the autodecoder optimizes both the network parameters and the latent codes representing each of the training shapes. Once trained, to reconstruct a shape given its SDF observations, a latent code is\n",
    "optimized keeping the network parameters fixed, such that the optimized latent code gives the lowest error with observed SDF values.\n",
    "\n",
    "An advantage that implicit representations have over voxel/grid based approaches is that they are not tied to a particular grid resolution, and can be evaluated at any resolution once trained.\n",
    "\n",
    "Similar to previous exercise, we'll first download the processed dataset, look at the implementation of the dataset, the model and the trainer, try out overfitting and generalization over the entire dataset, and finally inference on unseen samples.\n",
    "\n",
    "### (a) Downloading the data\n",
    "\n",
    "Whereas volumetric models output entire 3d shape representations, implicit models like DeepSDF work on per point basis. The network takes in a 3D-coordinate (and additionally the latent vector) and outputs the SDF value at the queried point. To train such a model,\n",
    "we therefore need, for each of the training shapes, a bunch of points with their corresponding SDF values for supervision. Points are sampled more aggressively near the surface of the object as we want to capture a more detailed SDF near the surface. For those curious,\n",
    "data preparation is decribed in more detail in section 5 of the paper.\n",
    "\n",
    "We'll be using the ShapeNet Sofa class for the experiments in this exercise. We've already prepared this data, so that you don't need to deal with the preprocessing. For each shape, the following files are provided:\n",
    "- `mesh.obj` representing the mesh representation of the shape\n",
    "- `sdf.npz` file containing large number of points sampled on and around the mesh and their sdf values; contains numpy arrays under keys \"pos\" and \"neg\", containing points with positive and negative sdf values respectively\n",
    "\n",
    "```\n",
    "# contents of exercise_3/data/sdf_sofas\n",
    "1faa4c299b93a3e5593ebeeedbff73b/                    # shape 0\n",
    "    ├── mesh.obj                                    # shape 0 mesh\n",
    "    ├── sdf.npz                                     # shape 0 sdf\n",
    "    ├── surface.obj                                 # shape 0 surface\n",
    "1fde48d83065ef5877a929f61fea4d0/                    # shape 1\n",
    "1fe1411b6c8097acf008d8a3590fb522/                   # shape 2\n",
    ":\n",
    "```\n",
    "Download and extract the data with the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Downloading ...')\n",
    "# File sizes: ~10GB\n",
    "!wget https://www.dropbox.com/s/4k5pw126nzus8ef/sdf_sofas.zip\\?dl\\=0 -O exercise_3/data/sdf_sofas.zip -P exercise_3/data\n",
    "\n",
    "print('Extracting ...')\n",
    "!unzip -q exercise_3/data/sdf_sofas.zip -d exercise_3/data\n",
    "!rm exercise_3/data/sdf_sofas.zip\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Dataset\n",
    "\n",
    "We provide a partial implementation of the dataset in `exercise_3/data/shape_implicit.py`.\n",
    "Your task is to complete the `#TODOs` so that the dataset works as specified by the docstrings.\n",
    "\n",
    "Once done, you can try running the following code blocks as sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set: 1226\n",
      "Length of val set: 137\n",
      "Length of overfit set: 1\n"
     ]
    }
   ],
   "source": [
    "from exercise_3.data.shape_implicit import ShapeImplicit\n",
    "\n",
    "num_points_to_samples = 40000\n",
    "train_dataset = ShapeImplicit(num_points_to_samples, \"train\")\n",
    "val_dataset = ShapeImplicit(num_points_to_samples, \"val\")\n",
    "overfit_dataset = ShapeImplicit(num_points_to_samples, \"overfit\")\n",
    "\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of train set: {len(train_dataset)}')  # expected output: 1226\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of val set: {len(val_dataset)}')  # expected output: 137\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of overfit set: {len(overfit_dataset)}')  # expected output: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's take a look at the points sampled for a particular shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_3.util.visualization import visualize_mesh, visualize_pointcloud\n",
    "\n",
    "shape_id = train_dataset[0]['name']\n",
    "points = train_dataset[0]['points']\n",
    "sdf = train_dataset[0]['sdf']\n",
    "\n",
    "# sampled points inside the shape\n",
    "inside_points = points[sdf[:, 0] < 0, :].numpy()\n",
    "\n",
    "# sampled points outside the shape\n",
    "outside_points = points[sdf[:, 0] > 0, :].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mesh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacopo/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-Sb4P0Pf9-py3.9/lib/python3.9/site-packages/traittypes/traittypes.py:97: UserWarning: Given trait value dtype \"uint32\" does not match required type \"uint32\". A coerced copy has been created.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee460a8ed3449998a6c97efd29dba34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mesh = ShapeImplicit.get_mesh(shape_id)\n",
    "print('Mesh')\n",
    "visualize_mesh(mesh.vertices, mesh.faces, flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled points with negative SDF (inside)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08396dda6afb4dcb8acbd8839e81b50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Sampled points with negative SDF (inside)')\n",
    "visualize_pointcloud(inside_points, 0.025, flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled points with positive SDF (outside)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf9b838da9f46978b7ca8ba9c9fdf73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Sampled points with positive SDF (outside)')\n",
    "visualize_pointcloud(outside_points, 0.025, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that more points are sampled close to the surface rather than away from the surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### (c) Model\n",
    "\n",
    "The DeepSDF auto-decoder architecture is visualized below:\n",
    "\n",
    "<img src=\"exercise_3/images/deepsdf_architecture.png\" alt=\"deepsdf_arch\" style=\"width: 640px;\"/>\n",
    "\n",
    "Things to note:\n",
    "\n",
    "- The network takes in the latent code for a shape concatenated with the query 3d coordinate, making up a 259 length vector (assuming latent code length is 256).\n",
    "- The network consist of a sequence of weight-normed linear layers, each followed by a ReLU and a dropout. For weight norming a layer, check out `torch.nn.utils.weight_norm`. Each of these linear layers outputs a 512 dimensional vector, except the 4th layer which outputs a 253 dimensional vector.\n",
    "- The output of the 4th layer is concatenated with the input, making the input to the 5th layer a 512 dimensional vector.\n",
    "- The final layer is a simple linear layer without any norm, dropout or non-linearity, with a single dimensional output representing the SDF value.\n",
    "\n",
    "Implement this architecture in file `exercise_3/model/deepsdf.py`.\n",
    "\n",
    "Here are some basic sanity tests once you're done with your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   | Name      | Type           | Params \n",
      "-----------------------------------------------\n",
      "0  | model1    | Sequential     | 790010 \n",
      "1  | model1.0  | Linear         | 133632 \n",
      "2  | model1.1  | ReLU           | 0      \n",
      "3  | model1.2  | Dropout        | 0      \n",
      "4  | model1.3  | Linear         | 263168 \n",
      "5  | model1.4  | ReLU           | 0      \n",
      "6  | model1.5  | Dropout        | 0      \n",
      "7  | model1.6  | Linear         | 263168 \n",
      "8  | model1.7  | ReLU           | 0      \n",
      "9  | model1.8  | Dropout        | 0      \n",
      "10 | model1.9  | Linear         | 130042 \n",
      "11 | model1.10 | ReLU           | 0      \n",
      "12 | model1.11 | Dropout        | 0      \n",
      "13 | model2    | Sequential     | 1053185\n",
      "14 | model2.0  | Linear         | 263168 \n",
      "15 | model2.1  | ReLU           | 0      \n",
      "16 | model2.2  | Dropout        | 0      \n",
      "17 | model2.3  | Linear         | 263168 \n",
      "18 | model2.4  | ReLU           | 0      \n",
      "19 | model2.5  | Dropout        | 0      \n",
      "20 | model2.6  | Linear         | 263168 \n",
      "21 | model2.7  | ReLU           | 0      \n",
      "22 | model2.8  | Dropout        | 0      \n",
      "23 | model2.9  | Linear         | 263168 \n",
      "24 | model2.10 | ReLU           | 0      \n",
      "25 | model2.11 | Dropout        | 0      \n",
      "26 | model2.12 | Linear         | 513    \n",
      "27 | TOTAL     | DeepSDFDecoder | 1843195\n",
      "\n",
      "Output tensor shape:  torch.Size([4096, 1])\n",
      "\n",
      "Number of traininable params: 1.84M\n"
     ]
    }
   ],
   "source": [
    "from exercise_3.model.deepsdf import DeepSDFDecoder\n",
    "from exercise_3.util.model import summarize_model\n",
    "\n",
    "deepsdf = DeepSDFDecoder(latent_size=256)\n",
    "print(summarize_model(deepsdf))\n",
    "\n",
    "# input to the network is a concatenation of point coordinates (3) and the latent code (256 in this example);\n",
    "# here we use a batch of 4096 points\n",
    "input_tensor = torch.randn(4096, 3 + 256)\n",
    "predictions = deepsdf(input_tensor)\n",
    "\n",
    "print('\\nOutput tensor shape: ', predictions.shape)  # expected output: 4096, 1\n",
    "\n",
    "num_trainable_params = sum(p.numel() for p in deepsdf.parameters() if p.requires_grad) / 1e6\n",
    "print(f'\\nNumber of traininable params: {num_trainable_params:.2f}M')  # expected output: ~1.8M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Training script and overfitting to a single shape\n",
    "\n",
    "Fill in the train script in `exercise_3/training/train_deepsdf.py`, and verify that your training work by overfitting to a few samples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "[049/00000] train_loss: 0.035651\n",
      "[099/00000] train_loss: 0.023768\n",
      "[149/00000] train_loss: 0.018041\n",
      "[199/00000] train_loss: 0.014182\n",
      "[249/00000] train_loss: 0.012589\n",
      "[299/00000] train_loss: 0.011668\n",
      "[349/00000] train_loss: 0.010462\n",
      "[399/00000] train_loss: 0.010124\n",
      "[449/00000] train_loss: 0.009703\n",
      "[499/00000] train_loss: 0.008977\n",
      "[549/00000] train_loss: 0.008453\n",
      "[599/00000] train_loss: 0.008194\n",
      "[649/00000] train_loss: 0.008078\n",
      "[699/00000] train_loss: 0.007943\n",
      "[749/00000] train_loss: 0.007826\n",
      "[799/00000] train_loss: 0.007685\n",
      "[849/00000] train_loss: 0.007587\n",
      "[899/00000] train_loss: 0.007489\n",
      "[949/00000] train_loss: 0.007314\n",
      "[999/00000] train_loss: 0.007270\n",
      "[1049/00000] train_loss: 0.007035\n",
      "[1099/00000] train_loss: 0.006993\n",
      "[1149/00000] train_loss: 0.006934\n",
      "[1199/00000] train_loss: 0.006874\n",
      "[1249/00000] train_loss: 0.006860\n",
      "[1299/00000] train_loss: 0.006801\n",
      "[1349/00000] train_loss: 0.006739\n",
      "[1399/00000] train_loss: 0.006733\n",
      "[1449/00000] train_loss: 0.006681\n",
      "[1499/00000] train_loss: 0.006632\n",
      "[1549/00000] train_loss: 0.006561\n",
      "[1599/00000] train_loss: 0.006481\n",
      "[1649/00000] train_loss: 0.006452\n",
      "[1699/00000] train_loss: 0.006446\n",
      "[1749/00000] train_loss: 0.006423\n",
      "[1799/00000] train_loss: 0.006385\n",
      "[1849/00000] train_loss: 0.006387\n",
      "[1899/00000] train_loss: 0.006358\n",
      "[1949/00000] train_loss: 0.006328\n",
      "[1999/00000] train_loss: 0.006298\n"
     ]
    }
   ],
   "source": [
    "from exercise_3.training import train_deepsdf\n",
    "\n",
    "overfit_config = {\n",
    "    'experiment_name': '3_2_deepsdf_overfit',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': True,\n",
    "    'num_sample_points': 4096,\n",
    "    'latent_code_length': 256,\n",
    "    'batch_size': 1,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate_model': 0.0005,\n",
    "    'learning_rate_code': 0.001,\n",
    "    'lambda_code_regularization': 0.0001,\n",
    "    'max_epochs': 2000,\n",
    "    'print_every_n': 50,\n",
    "    'visualize_every_n': 250,\n",
    "}\n",
    "\n",
    "train_deepsdf.main(overfit_config)  # expected loss around 0.0062"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the overfitted shape reconstruction to check if it looks reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5530798e393483ba6f782f5f731d688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overfit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c74c724e0d94f40a117c9b906d63bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and visualize GT mesh of the overfit sample\n",
    "gt_mesh = ShapeImplicit.get_mesh('7e728818848f191bee7d178666aae23d')\n",
    "print('GT')\n",
    "visualize_mesh(gt_mesh.vertices, gt_mesh.faces, flip_axes=True)\n",
    "\n",
    "# Load and visualize reconstructed overfit sample; it's okay if they don't look visually exact, since we don't run \n",
    "# the training too long and have a learning rate decay while training \n",
    "mesh_path = \"exercise_3/runs/3_2_deepsdf_overfit/meshes/01999_000.obj\"\n",
    "overfit_output = trimesh.load(mesh_path)\n",
    "print('Overfit')\n",
    "visualize_mesh(overfit_output.vertices, overfit_output.faces, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Training over entire train set\n",
    "\n",
    "Once overfitting works, we can train on the entire train set.\n",
    "\n",
    "Note: This training will take a few hours on a GPU (took ~3 hrs for 500 epochs on our 2080Ti, which already gave decent results). Please make sure to start training early enough before the submission deadline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "[000/00049] train_loss: 0.037183\n",
      "[000/00099] train_loss: 0.034311\n",
      "[000/00149] train_loss: 0.033090\n",
      "[000/00199] train_loss: 0.035163\n",
      "[000/00249] train_loss: 0.033212\n",
      "[000/00299] train_loss: 0.032074\n",
      "[000/00349] train_loss: 0.033058\n",
      "[000/00399] train_loss: 0.032187\n",
      "[000/00449] train_loss: 0.031307\n",
      "[000/00499] train_loss: 0.032676\n",
      "[000/00549] train_loss: 0.032901\n",
      "[000/00599] train_loss: 0.032898\n",
      "[000/00649] train_loss: 0.032959\n",
      "[000/00699] train_loss: 0.032153\n",
      "[000/00749] train_loss: 0.032347\n",
      "[000/00799] train_loss: 0.032420\n",
      "[000/00849] train_loss: 0.031097\n",
      "[000/00899] train_loss: 0.031309\n",
      "[000/00949] train_loss: 0.031723\n",
      "[000/00999] train_loss: 0.030653\n",
      "[000/01049] train_loss: 0.031637\n",
      "[000/01099] train_loss: 0.032063\n",
      "[000/01149] train_loss: 0.032251\n",
      "[000/01199] train_loss: 0.030945\n",
      "[001/00023] train_loss: 0.031817\n",
      "[001/00073] train_loss: 0.031532\n",
      "[001/00123] train_loss: 0.030341\n",
      "[001/00173] train_loss: 0.032764\n",
      "[001/00223] train_loss: 0.030575\n",
      "[001/00273] train_loss: 0.029475\n",
      "[001/00323] train_loss: 0.031451\n",
      "[001/00373] train_loss: 0.030504\n",
      "[001/00423] train_loss: 0.031755\n",
      "[001/00473] train_loss: 0.030549\n",
      "[001/00523] train_loss: 0.031742\n",
      "[001/00573] train_loss: 0.029917\n",
      "[001/00623] train_loss: 0.031295\n",
      "[001/00673] train_loss: 0.030842\n",
      "[001/00723] train_loss: 0.032053\n",
      "[001/00773] train_loss: 0.030932\n",
      "[001/00823] train_loss: 0.030148\n",
      "[001/00873] train_loss: 0.030145\n",
      "[001/00923] train_loss: 0.030043\n",
      "[001/00973] train_loss: 0.030181\n",
      "[001/01023] train_loss: 0.030407\n",
      "[001/01073] train_loss: 0.031429\n",
      "[001/01123] train_loss: 0.031045\n",
      "[001/01173] train_loss: 0.030584\n",
      "[001/01223] train_loss: 0.029084\n",
      "[002/00047] train_loss: 0.030748\n",
      "[002/00097] train_loss: 0.030468\n",
      "[002/00147] train_loss: 0.030004\n",
      "[002/00197] train_loss: 0.029795\n",
      "[002/00247] train_loss: 0.029603\n",
      "[002/00297] train_loss: 0.030225\n",
      "[002/00347] train_loss: 0.030575\n",
      "[002/00397] train_loss: 0.028411\n",
      "[002/00447] train_loss: 0.030032\n",
      "[002/00497] train_loss: 0.029998\n",
      "[002/00547] train_loss: 0.029433\n",
      "[002/00597] train_loss: 0.028968\n",
      "[002/00647] train_loss: 0.029341\n",
      "[002/00697] train_loss: 0.029728\n",
      "[002/00747] train_loss: 0.029543\n",
      "[002/00797] train_loss: 0.029240\n",
      "[002/00847] train_loss: 0.029218\n",
      "[002/00897] train_loss: 0.028513\n",
      "[002/00947] train_loss: 0.028724\n",
      "[002/00997] train_loss: 0.029281\n",
      "[002/01047] train_loss: 0.029390\n",
      "[002/01097] train_loss: 0.030647\n",
      "[002/01147] train_loss: 0.028405\n",
      "[002/01197] train_loss: 0.029536\n",
      "[003/00021] train_loss: 0.029237\n",
      "[003/00071] train_loss: 0.028474\n",
      "[003/00121] train_loss: 0.028715\n",
      "[003/00171] train_loss: 0.028439\n",
      "[003/00221] train_loss: 0.027738\n",
      "[003/00271] train_loss: 0.027739\n",
      "[003/00321] train_loss: 0.028006\n",
      "[003/00371] train_loss: 0.028467\n",
      "[003/00421] train_loss: 0.028084\n",
      "[003/00471] train_loss: 0.027417\n",
      "[003/00521] train_loss: 0.027791\n",
      "[003/00571] train_loss: 0.029204\n",
      "[003/00621] train_loss: 0.028540\n",
      "[003/00671] train_loss: 0.027243\n",
      "[003/00721] train_loss: 0.027959\n",
      "[003/00771] train_loss: 0.027421\n",
      "[003/00821] train_loss: 0.027321\n",
      "[003/00871] train_loss: 0.027332\n",
      "[003/00921] train_loss: 0.027314\n",
      "[003/00971] train_loss: 0.028760\n",
      "[003/01021] train_loss: 0.027290\n",
      "[003/01071] train_loss: 0.028162\n",
      "[003/01121] train_loss: 0.026819\n",
      "[003/01171] train_loss: 0.028256\n",
      "[003/01221] train_loss: 0.027235\n",
      "[004/00045] train_loss: 0.028735\n",
      "[004/00095] train_loss: 0.027616\n",
      "[004/00145] train_loss: 0.027407\n",
      "[004/00195] train_loss: 0.026696\n",
      "[004/00245] train_loss: 0.026589\n",
      "[004/00295] train_loss: 0.026225\n",
      "[004/00345] train_loss: 0.027234\n",
      "[004/00395] train_loss: 0.025748\n",
      "[004/00445] train_loss: 0.026603\n",
      "[004/00495] train_loss: 0.026771\n",
      "[004/00545] train_loss: 0.025489\n",
      "[004/00595] train_loss: 0.025638\n",
      "[004/00645] train_loss: 0.027242\n",
      "[004/00695] train_loss: 0.025995\n",
      "[004/00745] train_loss: 0.025681\n",
      "[004/00795] train_loss: 0.026510\n",
      "[004/00845] train_loss: 0.025796\n",
      "[004/00895] train_loss: 0.025904\n",
      "[004/00945] train_loss: 0.025589\n",
      "[004/00995] train_loss: 0.026426\n",
      "[004/01045] train_loss: 0.026236\n",
      "[004/01095] train_loss: 0.025282\n",
      "[004/01145] train_loss: 0.025875\n",
      "[004/01195] train_loss: 0.025616\n",
      "[005/00019] train_loss: 0.028494\n",
      "[005/00069] train_loss: 0.027892\n",
      "[005/00119] train_loss: 0.026446\n",
      "[005/00169] train_loss: 0.025150\n",
      "[005/00219] train_loss: 0.025246\n",
      "[005/00269] train_loss: 0.025363\n",
      "[005/00319] train_loss: 0.026332\n",
      "[005/00369] train_loss: 0.024099\n",
      "[005/00419] train_loss: 0.025564\n",
      "[005/00469] train_loss: 0.025972\n",
      "[005/00519] train_loss: 0.024705\n",
      "[005/00569] train_loss: 0.026132\n",
      "[005/00619] train_loss: 0.024786\n",
      "[005/00669] train_loss: 0.025142\n",
      "[005/00719] train_loss: 0.025322\n",
      "[005/00769] train_loss: 0.025519\n",
      "[005/00819] train_loss: 0.025173\n",
      "[005/00869] train_loss: 0.025346\n",
      "[005/00919] train_loss: 0.024351\n",
      "[005/00969] train_loss: 0.025784\n",
      "[005/01019] train_loss: 0.025225\n",
      "[005/01069] train_loss: 0.025216\n",
      "[005/01119] train_loss: 0.025207\n",
      "[005/01169] train_loss: 0.024958\n",
      "[005/01219] train_loss: 0.024180\n",
      "[006/00043] train_loss: 0.027474\n",
      "[006/00093] train_loss: 0.025627\n",
      "[006/00143] train_loss: 0.025116\n",
      "[006/00193] train_loss: 0.024621\n",
      "[006/00243] train_loss: 0.024456\n",
      "[006/00293] train_loss: 0.023427\n",
      "[006/00343] train_loss: 0.024875\n",
      "[006/00393] train_loss: 0.023391\n",
      "[006/00443] train_loss: 0.024803\n",
      "[006/00493] train_loss: 0.024034\n",
      "[006/00543] train_loss: 0.023565\n",
      "[006/00593] train_loss: 0.023737\n",
      "[006/00643] train_loss: 0.023409\n",
      "[006/00693] train_loss: 0.024367\n",
      "[006/00743] train_loss: 0.024384\n",
      "[006/00793] train_loss: 0.024010\n",
      "[006/00843] train_loss: 0.024011\n",
      "[006/00893] train_loss: 0.024260\n",
      "[006/00943] train_loss: 0.024135\n",
      "[006/00993] train_loss: 0.023377\n",
      "[006/01043] train_loss: 0.023911\n",
      "[006/01093] train_loss: 0.023467\n",
      "[006/01143] train_loss: 0.023991\n",
      "[006/01193] train_loss: 0.024885\n",
      "[007/00017] train_loss: 0.024052\n",
      "[007/00067] train_loss: 0.025824\n",
      "[007/00117] train_loss: 0.025370\n",
      "[007/00167] train_loss: 0.024862\n",
      "[007/00217] train_loss: 0.024489\n",
      "[007/00267] train_loss: 0.024130\n",
      "[007/00317] train_loss: 0.024102\n",
      "[007/00367] train_loss: 0.023783\n",
      "[007/00417] train_loss: 0.023853\n",
      "[007/00467] train_loss: 0.023021\n",
      "[007/00517] train_loss: 0.024075\n",
      "[007/00567] train_loss: 0.023612\n",
      "[007/00617] train_loss: 0.023368\n",
      "[007/00667] train_loss: 0.022318\n",
      "[007/00717] train_loss: 0.023455\n",
      "[007/00767] train_loss: 0.022564\n",
      "[007/00817] train_loss: 0.022839\n",
      "[007/00867] train_loss: 0.022874\n",
      "[007/00917] train_loss: 0.023322\n",
      "[007/00967] train_loss: 0.022939\n",
      "[007/01017] train_loss: 0.023944\n",
      "[007/01067] train_loss: 0.023477\n",
      "[007/01117] train_loss: 0.022928\n",
      "[007/01167] train_loss: 0.024118\n",
      "[007/01217] train_loss: 0.023316\n",
      "[008/00041] train_loss: 0.024899\n",
      "[008/00091] train_loss: 0.023862\n",
      "[008/00141] train_loss: 0.023359\n",
      "[008/00191] train_loss: 0.024370\n",
      "[008/00241] train_loss: 0.024483\n",
      "[008/00291] train_loss: 0.024332\n",
      "[008/00341] train_loss: 0.023746\n",
      "[008/00391] train_loss: 0.023021\n",
      "[008/00441] train_loss: 0.022977\n",
      "[008/00491] train_loss: 0.023283\n",
      "[008/00541] train_loss: 0.022815\n",
      "[008/00591] train_loss: 0.023289\n",
      "[008/00641] train_loss: 0.023065\n",
      "[008/00691] train_loss: 0.022636\n",
      "[008/00741] train_loss: 0.022590\n",
      "[008/00791] train_loss: 0.021678\n",
      "[008/00841] train_loss: 0.023142\n",
      "[008/00891] train_loss: 0.021459\n",
      "[008/00941] train_loss: 0.022454\n",
      "[008/00991] train_loss: 0.022316\n",
      "[008/01041] train_loss: 0.022390\n",
      "[008/01091] train_loss: 0.023171\n",
      "[008/01141] train_loss: 0.023014\n",
      "[008/01191] train_loss: 0.022466\n",
      "[009/00015] train_loss: 0.023177\n",
      "[009/00065] train_loss: 0.025209\n",
      "[009/00115] train_loss: 0.023980\n",
      "[009/00165] train_loss: 0.023301\n",
      "[009/00215] train_loss: 0.022882\n",
      "[009/00265] train_loss: 0.021742\n",
      "[009/00315] train_loss: 0.022691\n",
      "[009/00365] train_loss: 0.022177\n",
      "[009/00415] train_loss: 0.023901\n",
      "[009/00465] train_loss: 0.022106\n",
      "[009/00515] train_loss: 0.021578\n",
      "[009/00565] train_loss: 0.022226\n",
      "[009/00615] train_loss: 0.022584\n",
      "[009/00665] train_loss: 0.022177\n",
      "[009/00715] train_loss: 0.021836\n",
      "[009/00765] train_loss: 0.022635\n",
      "[009/00815] train_loss: 0.022343\n",
      "[009/00865] train_loss: 0.023013\n",
      "[009/00915] train_loss: 0.021724\n",
      "[009/00965] train_loss: 0.021727\n",
      "[009/01015] train_loss: 0.021534\n",
      "[009/01065] train_loss: 0.021575\n",
      "[009/01115] train_loss: 0.022025\n",
      "[009/01165] train_loss: 0.021975\n",
      "[009/01215] train_loss: 0.021758\n",
      "[010/00039] train_loss: 0.024935\n",
      "[010/00089] train_loss: 0.022267\n",
      "[010/00139] train_loss: 0.023703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[010/00189] train_loss: 0.022058\n",
      "[010/00239] train_loss: 0.022038\n",
      "[010/00289] train_loss: 0.023212\n",
      "[010/00339] train_loss: 0.022129\n",
      "[010/00389] train_loss: 0.022106\n",
      "[010/00439] train_loss: 0.021284\n",
      "[010/00489] train_loss: 0.021575\n",
      "[010/00539] train_loss: 0.021927\n",
      "[010/00589] train_loss: 0.021162\n",
      "[010/00639] train_loss: 0.021255\n",
      "[010/00689] train_loss: 0.022066\n",
      "[010/00739] train_loss: 0.021733\n",
      "[010/00789] train_loss: 0.021013\n",
      "[010/00839] train_loss: 0.021867\n",
      "[010/00889] train_loss: 0.021505\n",
      "[010/00939] train_loss: 0.020922\n",
      "[010/00989] train_loss: 0.021851\n",
      "[010/01039] train_loss: 0.021552\n",
      "[010/01089] train_loss: 0.021873\n",
      "[010/01139] train_loss: 0.021926\n",
      "[010/01189] train_loss: 0.021646\n",
      "[011/00013] train_loss: 0.022548\n",
      "[011/00063] train_loss: 0.023652\n",
      "[011/00113] train_loss: 0.022555\n",
      "[011/00163] train_loss: 0.022444\n",
      "[011/00213] train_loss: 0.022359\n",
      "[011/00263] train_loss: 0.022926\n",
      "[011/00313] train_loss: 0.021540\n",
      "[011/00363] train_loss: 0.021080\n",
      "[011/00413] train_loss: 0.021705\n",
      "[011/00463] train_loss: 0.020778\n",
      "[011/00513] train_loss: 0.020971\n",
      "[011/00563] train_loss: 0.022200\n",
      "[011/00613] train_loss: 0.021811\n",
      "[011/00663] train_loss: 0.020998\n",
      "[011/00713] train_loss: 0.021079\n",
      "[011/00763] train_loss: 0.020873\n",
      "[011/00813] train_loss: 0.021263\n",
      "[011/00863] train_loss: 0.021682\n",
      "[011/00913] train_loss: 0.021368\n",
      "[011/00963] train_loss: 0.021363\n",
      "[011/01013] train_loss: 0.021520\n",
      "[011/01063] train_loss: 0.020620\n",
      "[011/01113] train_loss: 0.020625\n",
      "[011/01163] train_loss: 0.021103\n",
      "[011/01213] train_loss: 0.021104\n",
      "[012/00037] train_loss: 0.022647\n",
      "[012/00087] train_loss: 0.023166\n",
      "[012/00137] train_loss: 0.022660\n",
      "[012/00187] train_loss: 0.022162\n",
      "[012/00237] train_loss: 0.020492\n",
      "[012/00287] train_loss: 0.021956\n",
      "[012/00337] train_loss: 0.021767\n",
      "[012/00387] train_loss: 0.021385\n",
      "[012/00437] train_loss: 0.021154\n",
      "[012/00487] train_loss: 0.021016\n",
      "[012/00537] train_loss: 0.021715\n",
      "[012/00587] train_loss: 0.020973\n",
      "[012/00637] train_loss: 0.019810\n",
      "[012/00687] train_loss: 0.021249\n",
      "[012/00737] train_loss: 0.020783\n",
      "[012/00787] train_loss: 0.020160\n",
      "[012/00837] train_loss: 0.021609\n",
      "[012/00887] train_loss: 0.020612\n",
      "[012/00937] train_loss: 0.021294\n",
      "[012/00987] train_loss: 0.020549\n",
      "[012/01037] train_loss: 0.021279\n",
      "[012/01087] train_loss: 0.021360\n",
      "[012/01137] train_loss: 0.021446\n",
      "[012/01187] train_loss: 0.020091\n",
      "[013/00011] train_loss: 0.022066\n",
      "[013/00061] train_loss: 0.023274\n",
      "[013/00111] train_loss: 0.023676\n",
      "[013/00161] train_loss: 0.021870\n",
      "[013/00211] train_loss: 0.021361\n",
      "[013/00261] train_loss: 0.022471\n",
      "[013/00311] train_loss: 0.021228\n",
      "[013/00361] train_loss: 0.021490\n",
      "[013/00411] train_loss: 0.020567\n",
      "[013/00461] train_loss: 0.021359\n",
      "[013/00511] train_loss: 0.020992\n",
      "[013/00561] train_loss: 0.020919\n",
      "[013/00611] train_loss: 0.021200\n",
      "[013/00661] train_loss: 0.020380\n",
      "[013/00711] train_loss: 0.021073\n",
      "[013/00761] train_loss: 0.021668\n",
      "[013/00811] train_loss: 0.020286\n",
      "[013/00861] train_loss: 0.020881\n",
      "[013/00911] train_loss: 0.019786\n",
      "[013/00961] train_loss: 0.020417\n",
      "[013/01011] train_loss: 0.020346\n",
      "[013/01061] train_loss: 0.020655\n",
      "[013/01111] train_loss: 0.020148\n",
      "[013/01161] train_loss: 0.020601\n",
      "[013/01211] train_loss: 0.020588\n",
      "[014/00035] train_loss: 0.022797\n",
      "[014/00085] train_loss: 0.023209\n",
      "[014/00135] train_loss: 0.021368\n",
      "[014/00185] train_loss: 0.020529\n",
      "[014/00235] train_loss: 0.021857\n",
      "[014/00285] train_loss: 0.021942\n",
      "[014/00335] train_loss: 0.020274\n",
      "[014/00385] train_loss: 0.020666\n",
      "[014/00435] train_loss: 0.020971\n",
      "[014/00485] train_loss: 0.019995\n",
      "[014/00535] train_loss: 0.019657\n",
      "[014/00585] train_loss: 0.020464\n",
      "[014/00635] train_loss: 0.020693\n",
      "[014/00685] train_loss: 0.021231\n",
      "[014/00735] train_loss: 0.021127\n",
      "[014/00785] train_loss: 0.019534\n",
      "[014/00835] train_loss: 0.020588\n",
      "[014/00885] train_loss: 0.019995\n",
      "[014/00935] train_loss: 0.019775\n",
      "[014/00985] train_loss: 0.020271\n",
      "[014/01035] train_loss: 0.019985\n",
      "[014/01085] train_loss: 0.019208\n",
      "[014/01135] train_loss: 0.021105\n",
      "[014/01185] train_loss: 0.019923\n",
      "[015/00009] train_loss: 0.020648\n",
      "[015/00059] train_loss: 0.023313\n",
      "[015/00109] train_loss: 0.022471\n",
      "[015/00159] train_loss: 0.021957\n",
      "[015/00209] train_loss: 0.021497\n",
      "[015/00259] train_loss: 0.021777\n",
      "[015/00309] train_loss: 0.020699\n",
      "[015/00359] train_loss: 0.020173\n",
      "[015/00409] train_loss: 0.020322\n",
      "[015/00459] train_loss: 0.019806\n",
      "[015/00509] train_loss: 0.019881\n",
      "[015/00559] train_loss: 0.019766\n",
      "[015/00609] train_loss: 0.020628\n",
      "[015/00659] train_loss: 0.020042\n",
      "[015/00709] train_loss: 0.019946\n",
      "[015/00759] train_loss: 0.020151\n",
      "[015/00809] train_loss: 0.020120\n",
      "[015/00859] train_loss: 0.020052\n",
      "[015/00909] train_loss: 0.019810\n",
      "[015/00959] train_loss: 0.019169\n",
      "[015/01009] train_loss: 0.020136\n",
      "[015/01059] train_loss: 0.019456\n",
      "[015/01109] train_loss: 0.019832\n",
      "[015/01159] train_loss: 0.020223\n",
      "[015/01209] train_loss: 0.020311\n",
      "[016/00033] train_loss: 0.022042\n",
      "[016/00083] train_loss: 0.023859\n",
      "[016/00133] train_loss: 0.022014\n",
      "[016/00183] train_loss: 0.022112\n",
      "[016/00233] train_loss: 0.021691\n",
      "[016/00283] train_loss: 0.020240\n",
      "[016/00333] train_loss: 0.019555\n",
      "[016/00383] train_loss: 0.019612\n",
      "[016/00433] train_loss: 0.019999\n",
      "[016/00483] train_loss: 0.020803\n",
      "[016/00533] train_loss: 0.019829\n",
      "[016/00583] train_loss: 0.019956\n",
      "[016/00633] train_loss: 0.019408\n",
      "[016/00683] train_loss: 0.021193\n",
      "[016/00733] train_loss: 0.019163\n",
      "[016/00783] train_loss: 0.020561\n",
      "[016/00833] train_loss: 0.019558\n",
      "[016/00883] train_loss: 0.020052\n",
      "[016/00933] train_loss: 0.020020\n",
      "[016/00983] train_loss: 0.019094\n",
      "[016/01033] train_loss: 0.019918\n",
      "[016/01083] train_loss: 0.018676\n",
      "[016/01133] train_loss: 0.018993\n",
      "[016/01183] train_loss: 0.019357\n",
      "[017/00007] train_loss: 0.018981\n",
      "[017/00057] train_loss: 0.021379\n",
      "[017/00107] train_loss: 0.022174\n",
      "[017/00157] train_loss: 0.021537\n",
      "[017/00207] train_loss: 0.020795\n",
      "[017/00257] train_loss: 0.019939\n",
      "[017/00307] train_loss: 0.020000\n",
      "[017/00357] train_loss: 0.019985\n",
      "[017/00407] train_loss: 0.019703\n",
      "[017/00457] train_loss: 0.020090\n",
      "[017/00507] train_loss: 0.019494\n",
      "[017/00557] train_loss: 0.019197\n",
      "[017/00607] train_loss: 0.020022\n",
      "[017/00657] train_loss: 0.019055\n",
      "[017/00707] train_loss: 0.019188\n",
      "[017/00757] train_loss: 0.019592\n",
      "[017/00807] train_loss: 0.018869\n",
      "[017/00857] train_loss: 0.019261\n",
      "[017/00907] train_loss: 0.019227\n",
      "[017/00957] train_loss: 0.019029\n",
      "[017/01007] train_loss: 0.019559\n",
      "[017/01057] train_loss: 0.020287\n",
      "[017/01107] train_loss: 0.020245\n",
      "[017/01157] train_loss: 0.019782\n",
      "[017/01207] train_loss: 0.019560\n",
      "[018/00031] train_loss: 0.021552\n",
      "[018/00081] train_loss: 0.021299\n",
      "[018/00131] train_loss: 0.021336\n",
      "[018/00181] train_loss: 0.020510\n",
      "[018/00231] train_loss: 0.019711\n",
      "[018/00281] train_loss: 0.019511\n",
      "[018/00331] train_loss: 0.019487\n",
      "[018/00381] train_loss: 0.020389\n",
      "[018/00431] train_loss: 0.019495\n",
      "[018/00481] train_loss: 0.019999\n",
      "[018/00531] train_loss: 0.019060\n",
      "[018/00581] train_loss: 0.019877\n",
      "[018/00631] train_loss: 0.018776\n",
      "[018/00681] train_loss: 0.019658\n",
      "[018/00731] train_loss: 0.020227\n",
      "[018/00781] train_loss: 0.019246\n",
      "[018/00831] train_loss: 0.019889\n",
      "[018/00881] train_loss: 0.020686\n",
      "[018/00931] train_loss: 0.019239\n",
      "[018/00981] train_loss: 0.018646\n",
      "[018/01031] train_loss: 0.019672\n",
      "[018/01081] train_loss: 0.019620\n",
      "[018/01131] train_loss: 0.018936\n",
      "[018/01181] train_loss: 0.019752\n",
      "[019/00005] train_loss: 0.019678\n",
      "[019/00055] train_loss: 0.021889\n",
      "[019/00105] train_loss: 0.021835\n",
      "[019/00155] train_loss: 0.021022\n",
      "[019/00205] train_loss: 0.020328\n",
      "[019/00255] train_loss: 0.020101\n",
      "[019/00305] train_loss: 0.021215\n",
      "[019/00355] train_loss: 0.019512\n",
      "[019/00405] train_loss: 0.020294\n",
      "[019/00455] train_loss: 0.018949\n",
      "[019/00505] train_loss: 0.018985\n",
      "[019/00555] train_loss: 0.017953\n",
      "[019/00605] train_loss: 0.019533\n",
      "[019/00655] train_loss: 0.019134\n",
      "[019/00705] train_loss: 0.019829\n",
      "[019/00755] train_loss: 0.019882\n",
      "[019/00805] train_loss: 0.018515\n",
      "[019/00855] train_loss: 0.019354\n",
      "[019/00905] train_loss: 0.019911\n",
      "[019/00955] train_loss: 0.019308\n",
      "[019/01005] train_loss: 0.019122\n",
      "[019/01055] train_loss: 0.018966\n",
      "[019/01105] train_loss: 0.019104\n",
      "[019/01155] train_loss: 0.018994\n",
      "[019/01205] train_loss: 0.019342\n",
      "[020/00029] train_loss: 0.020936\n",
      "[020/00079] train_loss: 0.022425\n",
      "[020/00129] train_loss: 0.021025\n",
      "[020/00179] train_loss: 0.021563\n",
      "[020/00229] train_loss: 0.020243\n",
      "[020/00279] train_loss: 0.020959\n",
      "[020/00329] train_loss: 0.019908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[020/00379] train_loss: 0.018465\n",
      "[020/00429] train_loss: 0.019927\n",
      "[020/00479] train_loss: 0.019705\n",
      "[020/00529] train_loss: 0.019604\n",
      "[020/00579] train_loss: 0.019036\n",
      "[020/00629] train_loss: 0.019429\n",
      "[020/00679] train_loss: 0.018734\n",
      "[020/00729] train_loss: 0.019196\n",
      "[020/00779] train_loss: 0.018447\n",
      "[020/00829] train_loss: 0.018476\n",
      "[020/00879] train_loss: 0.018486\n",
      "[020/00929] train_loss: 0.018784\n",
      "[020/00979] train_loss: 0.018875\n",
      "[020/01029] train_loss: 0.018952\n",
      "[020/01079] train_loss: 0.018934\n",
      "[020/01129] train_loss: 0.019406\n",
      "[020/01179] train_loss: 0.017841\n",
      "[021/00003] train_loss: 0.019405\n",
      "[021/00053] train_loss: 0.021816\n",
      "[021/00103] train_loss: 0.022189\n",
      "[021/00153] train_loss: 0.020247\n",
      "[021/00203] train_loss: 0.019194\n",
      "[021/00253] train_loss: 0.019523\n",
      "[021/00303] train_loss: 0.020053\n",
      "[021/00353] train_loss: 0.019537\n",
      "[021/00403] train_loss: 0.019621\n",
      "[021/00453] train_loss: 0.018779\n",
      "[021/00503] train_loss: 0.018078\n",
      "[021/00553] train_loss: 0.019839\n",
      "[021/00603] train_loss: 0.019287\n",
      "[021/00653] train_loss: 0.018280\n",
      "[021/00703] train_loss: 0.019389\n",
      "[021/00753] train_loss: 0.018507\n",
      "[021/00803] train_loss: 0.018407\n",
      "[021/00853] train_loss: 0.018944\n",
      "[021/00903] train_loss: 0.019224\n",
      "[021/00953] train_loss: 0.018159\n",
      "[021/01003] train_loss: 0.019740\n",
      "[021/01053] train_loss: 0.018705\n",
      "[021/01103] train_loss: 0.018458\n",
      "[021/01153] train_loss: 0.018439\n",
      "[021/01203] train_loss: 0.018463\n",
      "[022/00027] train_loss: 0.019626\n",
      "[022/00077] train_loss: 0.020675\n",
      "[022/00127] train_loss: 0.020712\n",
      "[022/00177] train_loss: 0.020134\n",
      "[022/00227] train_loss: 0.019916\n",
      "[022/00277] train_loss: 0.020310\n",
      "[022/00327] train_loss: 0.019511\n",
      "[022/00377] train_loss: 0.020749\n",
      "[022/00427] train_loss: 0.018964\n",
      "[022/00477] train_loss: 0.018828\n",
      "[022/00527] train_loss: 0.018883\n",
      "[022/00577] train_loss: 0.019078\n",
      "[022/00627] train_loss: 0.018844\n",
      "[022/00677] train_loss: 0.019673\n",
      "[022/00727] train_loss: 0.018349\n",
      "[022/00777] train_loss: 0.018268\n",
      "[022/00827] train_loss: 0.017957\n",
      "[022/00877] train_loss: 0.018560\n",
      "[022/00927] train_loss: 0.018481\n",
      "[022/00977] train_loss: 0.018847\n",
      "[022/01027] train_loss: 0.018064\n",
      "[022/01077] train_loss: 0.018630\n",
      "[022/01127] train_loss: 0.018789\n",
      "[022/01177] train_loss: 0.018477\n",
      "[023/00001] train_loss: 0.018795\n",
      "[023/00051] train_loss: 0.021766\n",
      "[023/00101] train_loss: 0.022067\n",
      "[023/00151] train_loss: 0.019678\n",
      "[023/00201] train_loss: 0.019968\n",
      "[023/00251] train_loss: 0.019399\n",
      "[023/00301] train_loss: 0.019943\n",
      "[023/00351] train_loss: 0.018473\n",
      "[023/00401] train_loss: 0.018366\n",
      "[023/00451] train_loss: 0.018115\n",
      "[023/00501] train_loss: 0.019187\n",
      "[023/00551] train_loss: 0.018532\n",
      "[023/00601] train_loss: 0.019150\n",
      "[023/00651] train_loss: 0.018273\n",
      "[023/00701] train_loss: 0.018471\n",
      "[023/00751] train_loss: 0.018355\n",
      "[023/00801] train_loss: 0.018482\n",
      "[023/00851] train_loss: 0.018885\n",
      "[023/00901] train_loss: 0.019387\n",
      "[023/00951] train_loss: 0.018294\n",
      "[023/01001] train_loss: 0.018521\n",
      "[023/01051] train_loss: 0.018596\n",
      "[023/01101] train_loss: 0.017343\n",
      "[023/01151] train_loss: 0.017855\n",
      "[023/01201] train_loss: 0.017873\n",
      "[024/00025] train_loss: 0.020546\n",
      "[024/00075] train_loss: 0.021702\n",
      "[024/00125] train_loss: 0.020620\n",
      "[024/00175] train_loss: 0.019837\n",
      "[024/00225] train_loss: 0.019822\n",
      "[024/00275] train_loss: 0.018955\n",
      "[024/00325] train_loss: 0.018679\n",
      "[024/00375] train_loss: 0.019091\n",
      "[024/00425] train_loss: 0.019142\n",
      "[024/00475] train_loss: 0.018552\n",
      "[024/00525] train_loss: 0.018646\n",
      "[024/00575] train_loss: 0.018650\n",
      "[024/00625] train_loss: 0.019155\n",
      "[024/00675] train_loss: 0.018751\n",
      "[024/00725] train_loss: 0.018557\n",
      "[024/00775] train_loss: 0.017314\n",
      "[024/00825] train_loss: 0.018915\n",
      "[024/00875] train_loss: 0.018155\n",
      "[024/00925] train_loss: 0.018414\n",
      "[024/00975] train_loss: 0.018698\n",
      "[024/01025] train_loss: 0.018490\n",
      "[024/01075] train_loss: 0.018365\n",
      "[024/01125] train_loss: 0.017720\n",
      "[024/01175] train_loss: 0.018003\n",
      "[024/01225] train_loss: 0.018865\n",
      "[025/00049] train_loss: 0.022570\n",
      "[025/00099] train_loss: 0.021168\n",
      "[025/00149] train_loss: 0.019729\n",
      "[025/00199] train_loss: 0.019265\n",
      "[025/00249] train_loss: 0.019015\n",
      "[025/00299] train_loss: 0.019455\n",
      "[025/00349] train_loss: 0.018597\n",
      "[025/00399] train_loss: 0.018368\n",
      "[025/00449] train_loss: 0.019100\n",
      "[025/00499] train_loss: 0.018407\n",
      "[025/00549] train_loss: 0.019255\n",
      "[025/00599] train_loss: 0.018049\n",
      "[025/00649] train_loss: 0.018441\n",
      "[025/00699] train_loss: 0.018046\n",
      "[025/00749] train_loss: 0.017186\n",
      "[025/00799] train_loss: 0.018118\n",
      "[025/00849] train_loss: 0.017822\n",
      "[025/00899] train_loss: 0.018993\n",
      "[025/00949] train_loss: 0.018404\n",
      "[025/00999] train_loss: 0.018704\n",
      "[025/01049] train_loss: 0.018157\n",
      "[025/01099] train_loss: 0.017553\n",
      "[025/01149] train_loss: 0.018190\n",
      "[025/01199] train_loss: 0.018159\n",
      "[026/00023] train_loss: 0.018742\n",
      "[026/00073] train_loss: 0.021330\n",
      "[026/00123] train_loss: 0.019272\n",
      "[026/00173] train_loss: 0.018680\n",
      "[026/00223] train_loss: 0.019452\n",
      "[026/00273] train_loss: 0.019296\n",
      "[026/00323] train_loss: 0.019701\n",
      "[026/00373] train_loss: 0.018689\n",
      "[026/00423] train_loss: 0.017850\n",
      "[026/00473] train_loss: 0.018796\n",
      "[026/00523] train_loss: 0.018342\n",
      "[026/00573] train_loss: 0.018666\n",
      "[026/00623] train_loss: 0.018465\n",
      "[026/00673] train_loss: 0.019339\n",
      "[026/00723] train_loss: 0.017813\n",
      "[026/00773] train_loss: 0.017640\n",
      "[026/00823] train_loss: 0.018450\n",
      "[026/00873] train_loss: 0.017563\n",
      "[026/00923] train_loss: 0.018565\n",
      "[026/00973] train_loss: 0.017918\n",
      "[026/01023] train_loss: 0.018527\n",
      "[026/01073] train_loss: 0.018976\n",
      "[026/01123] train_loss: 0.018364\n",
      "[026/01173] train_loss: 0.018145\n",
      "[026/01223] train_loss: 0.018553\n",
      "[027/00047] train_loss: 0.020586\n",
      "[027/00097] train_loss: 0.021404\n",
      "[027/00147] train_loss: 0.020883\n",
      "[027/00197] train_loss: 0.019305\n",
      "[027/00247] train_loss: 0.019682\n",
      "[027/00297] train_loss: 0.019065\n",
      "[027/00347] train_loss: 0.018443\n",
      "[027/00397] train_loss: 0.017926\n",
      "[027/00447] train_loss: 0.018639\n",
      "[027/00497] train_loss: 0.018170\n",
      "[027/00547] train_loss: 0.017866\n",
      "[027/00597] train_loss: 0.018592\n",
      "[027/00647] train_loss: 0.017359\n",
      "[027/00697] train_loss: 0.019052\n",
      "[027/00747] train_loss: 0.018165\n",
      "[027/00797] train_loss: 0.018445\n",
      "[027/00847] train_loss: 0.017547\n",
      "[027/00897] train_loss: 0.017427\n",
      "[027/00947] train_loss: 0.018034\n",
      "[027/00997] train_loss: 0.018134\n",
      "[027/01047] train_loss: 0.017684\n",
      "[027/01097] train_loss: 0.018171\n",
      "[027/01147] train_loss: 0.017927\n",
      "[027/01197] train_loss: 0.018505\n",
      "[028/00021] train_loss: 0.019501\n",
      "[028/00071] train_loss: 0.021266\n",
      "[028/00121] train_loss: 0.020413\n",
      "[028/00171] train_loss: 0.018939\n",
      "[028/00221] train_loss: 0.019469\n",
      "[028/00271] train_loss: 0.018424\n",
      "[028/00321] train_loss: 0.018794\n",
      "[028/00371] train_loss: 0.018220\n",
      "[028/00421] train_loss: 0.017908\n",
      "[028/00471] train_loss: 0.017957\n",
      "[028/00521] train_loss: 0.018024\n",
      "[028/00571] train_loss: 0.018483\n",
      "[028/00621] train_loss: 0.018975\n",
      "[028/00671] train_loss: 0.017085\n",
      "[028/00721] train_loss: 0.018455\n",
      "[028/00771] train_loss: 0.018309\n",
      "[028/00821] train_loss: 0.017476\n",
      "[028/00871] train_loss: 0.017954\n",
      "[028/00921] train_loss: 0.016711\n",
      "[028/00971] train_loss: 0.018501\n",
      "[028/01021] train_loss: 0.017726\n",
      "[028/01071] train_loss: 0.016880\n",
      "[028/01121] train_loss: 0.018383\n",
      "[028/01171] train_loss: 0.018065\n",
      "[028/01221] train_loss: 0.017474\n",
      "[029/00045] train_loss: 0.021481\n",
      "[029/00095] train_loss: 0.020180\n",
      "[029/00145] train_loss: 0.019364\n",
      "[029/00195] train_loss: 0.018882\n",
      "[029/00245] train_loss: 0.018371\n",
      "[029/00295] train_loss: 0.018209\n",
      "[029/00345] train_loss: 0.018872\n",
      "[029/00395] train_loss: 0.018053\n",
      "[029/00445] train_loss: 0.018750\n",
      "[029/00495] train_loss: 0.018296\n",
      "[029/00545] train_loss: 0.018308\n",
      "[029/00595] train_loss: 0.018189\n",
      "[029/00645] train_loss: 0.017402\n",
      "[029/00695] train_loss: 0.017672\n",
      "[029/00745] train_loss: 0.017481\n",
      "[029/00795] train_loss: 0.017550\n",
      "[029/00845] train_loss: 0.018015\n",
      "[029/00895] train_loss: 0.017844\n",
      "[029/00945] train_loss: 0.016982\n",
      "[029/00995] train_loss: 0.018051\n",
      "[029/01045] train_loss: 0.017614\n",
      "[029/01095] train_loss: 0.017952\n",
      "[029/01145] train_loss: 0.018096\n",
      "[029/01195] train_loss: 0.016894\n",
      "[030/00019] train_loss: 0.018768\n",
      "[030/00069] train_loss: 0.021222\n",
      "[030/00119] train_loss: 0.021438\n",
      "[030/00169] train_loss: 0.019340\n",
      "[030/00219] train_loss: 0.018995\n",
      "[030/00269] train_loss: 0.018496\n",
      "[030/00319] train_loss: 0.017991\n",
      "[030/00369] train_loss: 0.018073\n",
      "[030/00419] train_loss: 0.016734\n",
      "[030/00469] train_loss: 0.017661\n",
      "[030/00519] train_loss: 0.018357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[030/00569] train_loss: 0.017610\n",
      "[030/00619] train_loss: 0.018364\n",
      "[030/00669] train_loss: 0.016965\n",
      "[030/00719] train_loss: 0.017698\n",
      "[030/00769] train_loss: 0.017696\n",
      "[030/00819] train_loss: 0.018520\n",
      "[030/00869] train_loss: 0.017807\n",
      "[030/00919] train_loss: 0.018092\n",
      "[030/00969] train_loss: 0.017304\n",
      "[030/01019] train_loss: 0.018035\n",
      "[030/01069] train_loss: 0.018307\n",
      "[030/01119] train_loss: 0.018047\n",
      "[030/01169] train_loss: 0.017838\n",
      "[030/01219] train_loss: 0.017369\n",
      "[031/00043] train_loss: 0.020355\n",
      "[031/00093] train_loss: 0.020755\n",
      "[031/00143] train_loss: 0.019645\n",
      "[031/00193] train_loss: 0.018862\n",
      "[031/00243] train_loss: 0.018757\n",
      "[031/00293] train_loss: 0.018674\n",
      "[031/00343] train_loss: 0.017785\n",
      "[031/00393] train_loss: 0.017095\n",
      "[031/00443] train_loss: 0.017994\n",
      "[031/00493] train_loss: 0.017509\n",
      "[031/00543] train_loss: 0.017420\n",
      "[031/00593] train_loss: 0.019255\n",
      "[031/00643] train_loss: 0.017531\n",
      "[031/00693] train_loss: 0.017771\n",
      "[031/00743] train_loss: 0.018180\n",
      "[031/00793] train_loss: 0.017289\n",
      "[031/00843] train_loss: 0.018638\n",
      "[031/00893] train_loss: 0.017519\n",
      "[031/00943] train_loss: 0.016849\n",
      "[031/00993] train_loss: 0.018831\n",
      "[031/01043] train_loss: 0.017584\n",
      "[031/01093] train_loss: 0.017039\n",
      "[031/01143] train_loss: 0.016943\n",
      "[031/01193] train_loss: 0.017504\n",
      "[032/00017] train_loss: 0.018788\n",
      "[032/00067] train_loss: 0.020991\n",
      "[032/00117] train_loss: 0.019631\n",
      "[032/00167] train_loss: 0.019212\n",
      "[032/00217] train_loss: 0.018215\n",
      "[032/00267] train_loss: 0.018132\n",
      "[032/00317] train_loss: 0.018277\n",
      "[032/00367] train_loss: 0.017512\n",
      "[032/00417] train_loss: 0.018690\n",
      "[032/00467] train_loss: 0.017659\n",
      "[032/00517] train_loss: 0.018482\n",
      "[032/00567] train_loss: 0.017726\n",
      "[032/00617] train_loss: 0.017250\n",
      "[032/00667] train_loss: 0.017967\n",
      "[032/00717] train_loss: 0.016855\n",
      "[032/00767] train_loss: 0.018748\n",
      "[032/00817] train_loss: 0.017346\n",
      "[032/00867] train_loss: 0.017517\n",
      "[032/00917] train_loss: 0.017592\n",
      "[032/00967] train_loss: 0.017813\n",
      "[032/01017] train_loss: 0.017491\n",
      "[032/01067] train_loss: 0.016677\n",
      "[032/01117] train_loss: 0.017150\n",
      "[032/01167] train_loss: 0.017302\n",
      "[032/01217] train_loss: 0.017638\n",
      "[033/00041] train_loss: 0.020830\n",
      "[033/00091] train_loss: 0.019532\n",
      "[033/00141] train_loss: 0.019804\n",
      "[033/00191] train_loss: 0.018448\n",
      "[033/00241] train_loss: 0.018378\n",
      "[033/00291] train_loss: 0.018634\n",
      "[033/00341] train_loss: 0.018116\n",
      "[033/00391] train_loss: 0.017362\n",
      "[033/00441] train_loss: 0.017392\n",
      "[033/00491] train_loss: 0.018007\n",
      "[033/00541] train_loss: 0.017432\n",
      "[033/00591] train_loss: 0.017586\n",
      "[033/00641] train_loss: 0.017565\n",
      "[033/00691] train_loss: 0.017678\n",
      "[033/00741] train_loss: 0.017978\n",
      "[033/00791] train_loss: 0.018058\n",
      "[033/00841] train_loss: 0.016873\n",
      "[033/00891] train_loss: 0.017597\n",
      "[033/00941] train_loss: 0.016491\n",
      "[033/00991] train_loss: 0.016458\n",
      "[033/01041] train_loss: 0.017740\n",
      "[033/01091] train_loss: 0.017978\n",
      "[033/01141] train_loss: 0.016501\n",
      "[033/01191] train_loss: 0.017257\n",
      "[034/00015] train_loss: 0.018140\n",
      "[034/00065] train_loss: 0.019933\n",
      "[034/00115] train_loss: 0.019057\n",
      "[034/00165] train_loss: 0.019139\n",
      "[034/00215] train_loss: 0.018887\n",
      "[034/00265] train_loss: 0.017831\n",
      "[034/00315] train_loss: 0.017811\n",
      "[034/00365] train_loss: 0.017383\n",
      "[034/00415] train_loss: 0.017660\n",
      "[034/00465] train_loss: 0.017524\n",
      "[034/00515] train_loss: 0.017611\n",
      "[034/00565] train_loss: 0.018153\n",
      "[034/00615] train_loss: 0.016875\n",
      "[034/00665] train_loss: 0.017939\n",
      "[034/00715] train_loss: 0.017263\n",
      "[034/00765] train_loss: 0.017252\n",
      "[034/00815] train_loss: 0.016892\n",
      "[034/00865] train_loss: 0.018190\n",
      "[034/00915] train_loss: 0.017997\n",
      "[034/00965] train_loss: 0.018014\n",
      "[034/01015] train_loss: 0.017408\n",
      "[034/01065] train_loss: 0.017104\n",
      "[034/01115] train_loss: 0.017549\n",
      "[034/01165] train_loss: 0.017662\n",
      "[034/01215] train_loss: 0.017215\n",
      "[035/00039] train_loss: 0.018569\n",
      "[035/00089] train_loss: 0.018936\n",
      "[035/00139] train_loss: 0.018424\n",
      "[035/00189] train_loss: 0.018680\n",
      "[035/00239] train_loss: 0.018946\n",
      "[035/00289] train_loss: 0.017724\n",
      "[035/00339] train_loss: 0.018118\n",
      "[035/00389] train_loss: 0.017531\n",
      "[035/00439] train_loss: 0.017866\n",
      "[035/00489] train_loss: 0.017582\n",
      "[035/00539] train_loss: 0.017237\n",
      "[035/00589] train_loss: 0.017792\n",
      "[035/00639] train_loss: 0.017402\n",
      "[035/00689] train_loss: 0.017855\n",
      "[035/00739] train_loss: 0.017012\n",
      "[035/00789] train_loss: 0.017481\n",
      "[035/00839] train_loss: 0.016292\n",
      "[035/00889] train_loss: 0.017460\n",
      "[035/00939] train_loss: 0.017103\n",
      "[035/00989] train_loss: 0.018385\n",
      "[035/01039] train_loss: 0.016952\n",
      "[035/01089] train_loss: 0.017016\n",
      "[035/01139] train_loss: 0.016941\n",
      "[035/01189] train_loss: 0.017464\n",
      "[036/00013] train_loss: 0.017390\n",
      "[036/00063] train_loss: 0.019788\n",
      "[036/00113] train_loss: 0.018999\n",
      "[036/00163] train_loss: 0.019062\n",
      "[036/00213] train_loss: 0.019220\n",
      "[036/00263] train_loss: 0.017211\n",
      "[036/00313] train_loss: 0.018025\n",
      "[036/00363] train_loss: 0.017457\n",
      "[036/00413] train_loss: 0.018750\n",
      "[036/00463] train_loss: 0.018100\n",
      "[036/00513] train_loss: 0.016968\n",
      "[036/00563] train_loss: 0.017111\n",
      "[036/00613] train_loss: 0.017323\n",
      "[036/00663] train_loss: 0.017115\n",
      "[036/00713] train_loss: 0.017332\n",
      "[036/00763] train_loss: 0.017233\n",
      "[036/00813] train_loss: 0.017467\n",
      "[036/00863] train_loss: 0.016971\n",
      "[036/00913] train_loss: 0.017458\n",
      "[036/00963] train_loss: 0.016683\n",
      "[036/01013] train_loss: 0.016734\n",
      "[036/01063] train_loss: 0.017172\n",
      "[036/01113] train_loss: 0.017039\n",
      "[036/01163] train_loss: 0.017220\n",
      "[036/01213] train_loss: 0.016635\n",
      "[037/00037] train_loss: 0.019061\n",
      "[037/00087] train_loss: 0.019611\n",
      "[037/00137] train_loss: 0.018515\n",
      "[037/00187] train_loss: 0.018013\n",
      "[037/00237] train_loss: 0.017836\n",
      "[037/00287] train_loss: 0.017920\n",
      "[037/00337] train_loss: 0.017663\n",
      "[037/00387] train_loss: 0.017318\n",
      "[037/00437] train_loss: 0.017794\n",
      "[037/00487] train_loss: 0.017930\n",
      "[037/00537] train_loss: 0.018163\n",
      "[037/00587] train_loss: 0.017105\n",
      "[037/00637] train_loss: 0.018238\n",
      "[037/00687] train_loss: 0.017026\n",
      "[037/00737] train_loss: 0.017965\n",
      "[037/00787] train_loss: 0.017794\n",
      "[037/00837] train_loss: 0.017025\n",
      "[037/00887] train_loss: 0.017098\n",
      "[037/00937] train_loss: 0.016204\n",
      "[037/00987] train_loss: 0.017211\n",
      "[037/01037] train_loss: 0.017481\n",
      "[037/01087] train_loss: 0.016555\n",
      "[037/01137] train_loss: 0.016929\n",
      "[037/01187] train_loss: 0.017705\n",
      "[038/00011] train_loss: 0.017902\n",
      "[038/00061] train_loss: 0.019752\n",
      "[038/00111] train_loss: 0.018794\n",
      "[038/00161] train_loss: 0.018785\n",
      "[038/00211] train_loss: 0.018003\n",
      "[038/00261] train_loss: 0.017945\n",
      "[038/00311] train_loss: 0.017604\n",
      "[038/00361] train_loss: 0.017407\n",
      "[038/00411] train_loss: 0.017068\n",
      "[038/00461] train_loss: 0.017617\n",
      "[038/00511] train_loss: 0.017493\n",
      "[038/00561] train_loss: 0.017343\n",
      "[038/00611] train_loss: 0.017623\n",
      "[038/00661] train_loss: 0.017434\n",
      "[038/00711] train_loss: 0.017932\n",
      "[038/00761] train_loss: 0.017790\n",
      "[038/00811] train_loss: 0.016592\n",
      "[038/00861] train_loss: 0.017186\n",
      "[038/00911] train_loss: 0.016852\n",
      "[038/00961] train_loss: 0.017058\n",
      "[038/01011] train_loss: 0.016406\n",
      "[038/01061] train_loss: 0.017485\n",
      "[038/01111] train_loss: 0.016205\n",
      "[038/01161] train_loss: 0.016950\n",
      "[038/01211] train_loss: 0.016687\n",
      "[039/00035] train_loss: 0.018382\n",
      "[039/00085] train_loss: 0.019293\n",
      "[039/00135] train_loss: 0.019432\n",
      "[039/00185] train_loss: 0.017149\n",
      "[039/00235] train_loss: 0.018373\n",
      "[039/00285] train_loss: 0.018330\n",
      "[039/00335] train_loss: 0.018002\n",
      "[039/00385] train_loss: 0.018352\n",
      "[039/00435] train_loss: 0.017268\n",
      "[039/00485] train_loss: 0.017512\n",
      "[039/00535] train_loss: 0.017485\n",
      "[039/00585] train_loss: 0.017539\n",
      "[039/00635] train_loss: 0.017401\n",
      "[039/00685] train_loss: 0.017002\n",
      "[039/00735] train_loss: 0.017014\n",
      "[039/00785] train_loss: 0.016924\n",
      "[039/00835] train_loss: 0.017277\n",
      "[039/00885] train_loss: 0.017869\n",
      "[039/00935] train_loss: 0.016854\n",
      "[039/00985] train_loss: 0.016010\n",
      "[039/01035] train_loss: 0.017999\n",
      "[039/01085] train_loss: 0.017358\n",
      "[039/01135] train_loss: 0.016373\n",
      "[039/01185] train_loss: 0.016759\n",
      "[040/00009] train_loss: 0.017169\n",
      "[040/00059] train_loss: 0.020001\n",
      "[040/00109] train_loss: 0.019105\n",
      "[040/00159] train_loss: 0.017881\n",
      "[040/00209] train_loss: 0.017611\n",
      "[040/00259] train_loss: 0.018082\n",
      "[040/00309] train_loss: 0.017692\n",
      "[040/00359] train_loss: 0.016787\n",
      "[040/00409] train_loss: 0.017100\n",
      "[040/00459] train_loss: 0.017205\n",
      "[040/00509] train_loss: 0.017277\n",
      "[040/00559] train_loss: 0.016756\n",
      "[040/00609] train_loss: 0.017105\n",
      "[040/00659] train_loss: 0.017532\n",
      "[040/00709] train_loss: 0.017950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[040/00759] train_loss: 0.016977\n",
      "[040/00809] train_loss: 0.017209\n",
      "[040/00859] train_loss: 0.017423\n",
      "[040/00909] train_loss: 0.016206\n",
      "[040/00959] train_loss: 0.016952\n",
      "[040/01009] train_loss: 0.017124\n",
      "[040/01059] train_loss: 0.016566\n",
      "[040/01109] train_loss: 0.016163\n",
      "[040/01159] train_loss: 0.016864\n",
      "[040/01209] train_loss: 0.016280\n",
      "[041/00033] train_loss: 0.019295\n",
      "[041/00083] train_loss: 0.018042\n",
      "[041/00133] train_loss: 0.019367\n",
      "[041/00183] train_loss: 0.018540\n",
      "[041/00233] train_loss: 0.017601\n",
      "[041/00283] train_loss: 0.018564\n",
      "[041/00333] train_loss: 0.017320\n",
      "[041/00383] train_loss: 0.017093\n",
      "[041/00433] train_loss: 0.017091\n",
      "[041/00483] train_loss: 0.016602\n",
      "[041/00533] train_loss: 0.017662\n",
      "[041/00583] train_loss: 0.016718\n",
      "[041/00633] train_loss: 0.016754\n",
      "[041/00683] train_loss: 0.016682\n",
      "[041/00733] train_loss: 0.016945\n",
      "[041/00783] train_loss: 0.016915\n",
      "[041/00833] train_loss: 0.017369\n",
      "[041/00883] train_loss: 0.015598\n",
      "[041/00933] train_loss: 0.017212\n",
      "[041/00983] train_loss: 0.017399\n",
      "[041/01033] train_loss: 0.016680\n",
      "[041/01083] train_loss: 0.017055\n",
      "[041/01133] train_loss: 0.017467\n",
      "[041/01183] train_loss: 0.015446\n",
      "[042/00007] train_loss: 0.018319\n",
      "[042/00057] train_loss: 0.019970\n",
      "[042/00107] train_loss: 0.018194\n",
      "[042/00157] train_loss: 0.018202\n",
      "[042/00207] train_loss: 0.017239\n",
      "[042/00257] train_loss: 0.017960\n",
      "[042/00307] train_loss: 0.018107\n",
      "[042/00357] train_loss: 0.017185\n",
      "[042/00407] train_loss: 0.017210\n",
      "[042/00457] train_loss: 0.016667\n",
      "[042/00507] train_loss: 0.016951\n",
      "[042/00557] train_loss: 0.017007\n",
      "[042/00607] train_loss: 0.017226\n",
      "[042/00657] train_loss: 0.015799\n",
      "[042/00707] train_loss: 0.016026\n",
      "[042/00757] train_loss: 0.016815\n",
      "[042/00807] train_loss: 0.016933\n",
      "[042/00857] train_loss: 0.016886\n",
      "[042/00907] train_loss: 0.017546\n",
      "[042/00957] train_loss: 0.017254\n",
      "[042/01007] train_loss: 0.016797\n",
      "[042/01057] train_loss: 0.016368\n",
      "[042/01107] train_loss: 0.016429\n",
      "[042/01157] train_loss: 0.016779\n",
      "[042/01207] train_loss: 0.016864\n",
      "[043/00031] train_loss: 0.017465\n",
      "[043/00081] train_loss: 0.019614\n",
      "[043/00131] train_loss: 0.018153\n",
      "[043/00181] train_loss: 0.016900\n",
      "[043/00231] train_loss: 0.016936\n",
      "[043/00281] train_loss: 0.017773\n",
      "[043/00331] train_loss: 0.016818\n",
      "[043/00381] train_loss: 0.016774\n",
      "[043/00431] train_loss: 0.017280\n",
      "[043/00481] train_loss: 0.016683\n",
      "[043/00531] train_loss: 0.017531\n",
      "[043/00581] train_loss: 0.017225\n",
      "[043/00631] train_loss: 0.016868\n",
      "[043/00681] train_loss: 0.017183\n",
      "[043/00731] train_loss: 0.016323\n",
      "[043/00781] train_loss: 0.015904\n",
      "[043/00831] train_loss: 0.017206\n",
      "[043/00881] train_loss: 0.017088\n",
      "[043/00931] train_loss: 0.017077\n",
      "[043/00981] train_loss: 0.016007\n",
      "[043/01031] train_loss: 0.016784\n",
      "[043/01081] train_loss: 0.016348\n",
      "[043/01131] train_loss: 0.016384\n",
      "[043/01181] train_loss: 0.016790\n",
      "[044/00005] train_loss: 0.016974\n",
      "[044/00055] train_loss: 0.020189\n",
      "[044/00105] train_loss: 0.018355\n",
      "[044/00155] train_loss: 0.018305\n",
      "[044/00205] train_loss: 0.018527\n",
      "[044/00255] train_loss: 0.017044\n",
      "[044/00305] train_loss: 0.017332\n",
      "[044/00355] train_loss: 0.016802\n",
      "[044/00405] train_loss: 0.016673\n",
      "[044/00455] train_loss: 0.017031\n",
      "[044/00505] train_loss: 0.017944\n",
      "[044/00555] train_loss: 0.016816\n",
      "[044/00605] train_loss: 0.016578\n",
      "[044/00655] train_loss: 0.016852\n",
      "[044/00705] train_loss: 0.016266\n",
      "[044/00755] train_loss: 0.016091\n",
      "[044/00805] train_loss: 0.017436\n",
      "[044/00855] train_loss: 0.016407\n",
      "[044/00905] train_loss: 0.016409\n",
      "[044/00955] train_loss: 0.016032\n",
      "[044/01005] train_loss: 0.016507\n",
      "[044/01055] train_loss: 0.016134\n",
      "[044/01105] train_loss: 0.015810\n",
      "[044/01155] train_loss: 0.017076\n",
      "[044/01205] train_loss: 0.017367\n",
      "[045/00029] train_loss: 0.018863\n",
      "[045/00079] train_loss: 0.019009\n",
      "[045/00129] train_loss: 0.017868\n",
      "[045/00179] train_loss: 0.018740\n",
      "[045/00229] train_loss: 0.017440\n",
      "[045/00279] train_loss: 0.017443\n",
      "[045/00329] train_loss: 0.017623\n",
      "[045/00379] train_loss: 0.016864\n",
      "[045/00429] train_loss: 0.017522\n",
      "[045/00479] train_loss: 0.016376\n",
      "[045/00529] train_loss: 0.016933\n",
      "[045/00579] train_loss: 0.016918\n",
      "[045/00629] train_loss: 0.016147\n",
      "[045/00679] train_loss: 0.016728\n",
      "[045/00729] train_loss: 0.016826\n",
      "[045/00779] train_loss: 0.016858\n",
      "[045/00829] train_loss: 0.015988\n",
      "[045/00879] train_loss: 0.016437\n",
      "[045/00929] train_loss: 0.016256\n",
      "[045/00979] train_loss: 0.016293\n",
      "[045/01029] train_loss: 0.017312\n",
      "[045/01079] train_loss: 0.015999\n",
      "[045/01129] train_loss: 0.017159\n",
      "[045/01179] train_loss: 0.016430\n",
      "[046/00003] train_loss: 0.016499\n",
      "[046/00053] train_loss: 0.018704\n",
      "[046/00103] train_loss: 0.018237\n",
      "[046/00153] train_loss: 0.018019\n",
      "[046/00203] train_loss: 0.018012\n",
      "[046/00253] train_loss: 0.017899\n",
      "[046/00303] train_loss: 0.016340\n",
      "[046/00353] train_loss: 0.017561\n",
      "[046/00403] train_loss: 0.018179\n",
      "[046/00453] train_loss: 0.016472\n",
      "[046/00503] train_loss: 0.017219\n",
      "[046/00553] train_loss: 0.017057\n",
      "[046/00603] train_loss: 0.016861\n",
      "[046/00653] train_loss: 0.016860\n",
      "[046/00703] train_loss: 0.016260\n",
      "[046/00753] train_loss: 0.016293\n",
      "[046/00803] train_loss: 0.015975\n",
      "[046/00853] train_loss: 0.015890\n",
      "[046/00903] train_loss: 0.016456\n",
      "[046/00953] train_loss: 0.017687\n",
      "[046/01003] train_loss: 0.016211\n",
      "[046/01053] train_loss: 0.015548\n",
      "[046/01103] train_loss: 0.016896\n",
      "[046/01153] train_loss: 0.016069\n",
      "[046/01203] train_loss: 0.016551\n",
      "[047/00027] train_loss: 0.018427\n",
      "[047/00077] train_loss: 0.018590\n",
      "[047/00127] train_loss: 0.018104\n",
      "[047/00177] train_loss: 0.018567\n",
      "[047/00227] train_loss: 0.017289\n",
      "[047/00277] train_loss: 0.017183\n",
      "[047/00327] train_loss: 0.017126\n",
      "[047/00377] train_loss: 0.015974\n",
      "[047/00427] train_loss: 0.017463\n",
      "[047/00477] train_loss: 0.016290\n",
      "[047/00527] train_loss: 0.016241\n",
      "[047/00577] train_loss: 0.016368\n",
      "[047/00627] train_loss: 0.016668\n",
      "[047/00677] train_loss: 0.016437\n",
      "[047/00727] train_loss: 0.016114\n",
      "[047/00777] train_loss: 0.016402\n",
      "[047/00827] train_loss: 0.016509\n",
      "[047/00877] train_loss: 0.016816\n",
      "[047/00927] train_loss: 0.015555\n",
      "[047/00977] train_loss: 0.016315\n",
      "[047/01027] train_loss: 0.016407\n",
      "[047/01077] train_loss: 0.016361\n",
      "[047/01127] train_loss: 0.016904\n",
      "[047/01177] train_loss: 0.016272\n",
      "[048/00001] train_loss: 0.016726\n",
      "[048/00051] train_loss: 0.019296\n",
      "[048/00101] train_loss: 0.019269\n",
      "[048/00151] train_loss: 0.017679\n",
      "[048/00201] train_loss: 0.018136\n",
      "[048/00251] train_loss: 0.017313\n",
      "[048/00301] train_loss: 0.017090\n",
      "[048/00351] train_loss: 0.017258\n",
      "[048/00401] train_loss: 0.017204\n",
      "[048/00451] train_loss: 0.015940\n",
      "[048/00501] train_loss: 0.016613\n",
      "[048/00551] train_loss: 0.016318\n",
      "[048/00601] train_loss: 0.016796\n",
      "[048/00651] train_loss: 0.017155\n",
      "[048/00701] train_loss: 0.016320\n",
      "[048/00751] train_loss: 0.016425\n",
      "[048/00801] train_loss: 0.016624\n",
      "[048/00851] train_loss: 0.016967\n",
      "[048/00901] train_loss: 0.015974\n",
      "[048/00951] train_loss: 0.017354\n",
      "[048/01001] train_loss: 0.015822\n",
      "[048/01051] train_loss: 0.016819\n",
      "[048/01101] train_loss: 0.016374\n",
      "[048/01151] train_loss: 0.015708\n",
      "[048/01201] train_loss: 0.016029\n",
      "[049/00025] train_loss: 0.017232\n",
      "[049/00075] train_loss: 0.018419\n",
      "[049/00125] train_loss: 0.018026\n",
      "[049/00175] train_loss: 0.016794\n",
      "[049/00225] train_loss: 0.016941\n",
      "[049/00275] train_loss: 0.016925\n",
      "[049/00325] train_loss: 0.016073\n",
      "[049/00375] train_loss: 0.016226\n",
      "[049/00425] train_loss: 0.017058\n",
      "[049/00475] train_loss: 0.016939\n",
      "[049/00525] train_loss: 0.016760\n",
      "[049/00575] train_loss: 0.017911\n",
      "[049/00625] train_loss: 0.016748\n",
      "[049/00675] train_loss: 0.016212\n",
      "[049/00725] train_loss: 0.016600\n",
      "[049/00775] train_loss: 0.016555\n",
      "[049/00825] train_loss: 0.017206\n",
      "[049/00875] train_loss: 0.016464\n",
      "[049/00925] train_loss: 0.016437\n",
      "[049/00975] train_loss: 0.015957\n",
      "[049/01025] train_loss: 0.017277\n",
      "[049/01075] train_loss: 0.015757\n",
      "[049/01125] train_loss: 0.016222\n",
      "[049/01175] train_loss: 0.017405\n",
      "[049/01225] train_loss: 0.016393\n",
      "[050/00049] train_loss: 0.018698\n",
      "[050/00099] train_loss: 0.017826\n",
      "[050/00149] train_loss: 0.017792\n",
      "[050/00199] train_loss: 0.017812\n",
      "[050/00249] train_loss: 0.017441\n",
      "[050/00299] train_loss: 0.017063\n",
      "[050/00349] train_loss: 0.017512\n",
      "[050/00399] train_loss: 0.017492\n",
      "[050/00449] train_loss: 0.017302\n",
      "[050/00499] train_loss: 0.016332\n",
      "[050/00549] train_loss: 0.015905\n",
      "[050/00599] train_loss: 0.015176\n",
      "[050/00649] train_loss: 0.016958\n",
      "[050/00699] train_loss: 0.017016\n",
      "[050/00749] train_loss: 0.016093\n",
      "[050/00799] train_loss: 0.015976\n",
      "[050/00849] train_loss: 0.015816\n",
      "[050/00899] train_loss: 0.016059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[050/00949] train_loss: 0.016279\n",
      "[050/00999] train_loss: 0.016290\n",
      "[050/01049] train_loss: 0.016340\n",
      "[050/01099] train_loss: 0.015712\n",
      "[050/01149] train_loss: 0.016584\n",
      "[050/01199] train_loss: 0.015730\n",
      "[051/00023] train_loss: 0.017326\n",
      "[051/00073] train_loss: 0.019223\n",
      "[051/00123] train_loss: 0.018411\n",
      "[051/00173] train_loss: 0.017430\n",
      "[051/00223] train_loss: 0.018430\n",
      "[051/00273] train_loss: 0.017186\n",
      "[051/00323] train_loss: 0.016815\n",
      "[051/00373] train_loss: 0.016709\n",
      "[051/00423] train_loss: 0.016801\n",
      "[051/00473] train_loss: 0.016404\n",
      "[051/00523] train_loss: 0.016729\n",
      "[051/00573] train_loss: 0.017639\n",
      "[051/00623] train_loss: 0.016602\n",
      "[051/00673] train_loss: 0.016456\n",
      "[051/00723] train_loss: 0.015870\n",
      "[051/00773] train_loss: 0.016487\n",
      "[051/00823] train_loss: 0.016465\n",
      "[051/00873] train_loss: 0.015993\n",
      "[051/00923] train_loss: 0.015881\n",
      "[051/00973] train_loss: 0.015950\n",
      "[051/01023] train_loss: 0.015431\n",
      "[051/01073] train_loss: 0.015790\n",
      "[051/01123] train_loss: 0.016270\n",
      "[051/01173] train_loss: 0.016571\n",
      "[051/01223] train_loss: 0.016089\n",
      "[052/00047] train_loss: 0.019382\n",
      "[052/00097] train_loss: 0.018547\n",
      "[052/00147] train_loss: 0.017976\n",
      "[052/00197] train_loss: 0.018012\n",
      "[052/00247] train_loss: 0.016236\n",
      "[052/00297] train_loss: 0.017618\n",
      "[052/00347] train_loss: 0.017681\n",
      "[052/00397] train_loss: 0.016559\n",
      "[052/00447] train_loss: 0.015549\n",
      "[052/00497] train_loss: 0.016620\n",
      "[052/00547] train_loss: 0.015667\n",
      "[052/00597] train_loss: 0.016708\n",
      "[052/00647] train_loss: 0.016339\n",
      "[052/00697] train_loss: 0.016292\n",
      "[052/00747] train_loss: 0.017091\n",
      "[052/00797] train_loss: 0.016854\n",
      "[052/00847] train_loss: 0.015739\n",
      "[052/00897] train_loss: 0.016264\n",
      "[052/00947] train_loss: 0.015890\n",
      "[052/00997] train_loss: 0.016135\n",
      "[052/01047] train_loss: 0.016224\n",
      "[052/01097] train_loss: 0.016258\n",
      "[052/01147] train_loss: 0.015887\n",
      "[052/01197] train_loss: 0.016025\n",
      "[053/00021] train_loss: 0.017312\n",
      "[053/00071] train_loss: 0.018268\n",
      "[053/00121] train_loss: 0.017163\n",
      "[053/00171] train_loss: 0.017407\n",
      "[053/00221] train_loss: 0.016703\n",
      "[053/00271] train_loss: 0.016301\n",
      "[053/00321] train_loss: 0.016542\n",
      "[053/00371] train_loss: 0.017173\n",
      "[053/00421] train_loss: 0.016612\n",
      "[053/00471] train_loss: 0.015883\n",
      "[053/00521] train_loss: 0.015886\n",
      "[053/00571] train_loss: 0.016654\n",
      "[053/00621] train_loss: 0.016898\n",
      "[053/00671] train_loss: 0.016194\n",
      "[053/00721] train_loss: 0.015235\n",
      "[053/00771] train_loss: 0.015581\n",
      "[053/00821] train_loss: 0.016862\n",
      "[053/00871] train_loss: 0.016271\n",
      "[053/00921] train_loss: 0.016119\n",
      "[053/00971] train_loss: 0.015575\n",
      "[053/01021] train_loss: 0.016883\n",
      "[053/01071] train_loss: 0.015848\n",
      "[053/01121] train_loss: 0.015828\n",
      "[053/01171] train_loss: 0.016331\n",
      "[053/01221] train_loss: 0.015083\n",
      "[054/00045] train_loss: 0.018180\n",
      "[054/00095] train_loss: 0.017902\n",
      "[054/00145] train_loss: 0.017472\n",
      "[054/00195] train_loss: 0.017292\n",
      "[054/00245] train_loss: 0.017578\n",
      "[054/00295] train_loss: 0.016623\n",
      "[054/00345] train_loss: 0.017001\n",
      "[054/00395] train_loss: 0.016606\n",
      "[054/00445] train_loss: 0.015637\n",
      "[054/00495] train_loss: 0.015484\n",
      "[054/00545] train_loss: 0.016244\n",
      "[054/00595] train_loss: 0.015717\n",
      "[054/00645] train_loss: 0.015653\n",
      "[054/00695] train_loss: 0.016827\n",
      "[054/00745] train_loss: 0.016456\n",
      "[054/00795] train_loss: 0.016411\n",
      "[054/00845] train_loss: 0.015818\n",
      "[054/00895] train_loss: 0.015944\n",
      "[054/00945] train_loss: 0.016204\n",
      "[054/00995] train_loss: 0.016319\n",
      "[054/01045] train_loss: 0.015651\n",
      "[054/01095] train_loss: 0.015753\n",
      "[054/01145] train_loss: 0.016244\n",
      "[054/01195] train_loss: 0.016849\n",
      "[055/00019] train_loss: 0.016564\n",
      "[055/00069] train_loss: 0.017866\n",
      "[055/00119] train_loss: 0.017877\n",
      "[055/00169] train_loss: 0.016565\n",
      "[055/00219] train_loss: 0.017201\n",
      "[055/00269] train_loss: 0.015875\n",
      "[055/00319] train_loss: 0.015914\n",
      "[055/00369] train_loss: 0.017169\n",
      "[055/00419] train_loss: 0.017127\n",
      "[055/00469] train_loss: 0.015828\n",
      "[055/00519] train_loss: 0.016526\n",
      "[055/00569] train_loss: 0.015885\n",
      "[055/00619] train_loss: 0.016541\n",
      "[055/00669] train_loss: 0.016305\n",
      "[055/00719] train_loss: 0.015603\n",
      "[055/00769] train_loss: 0.016453\n",
      "[055/00819] train_loss: 0.016369\n",
      "[055/00869] train_loss: 0.015498\n",
      "[055/00919] train_loss: 0.016038\n",
      "[055/00969] train_loss: 0.015762\n",
      "[055/01019] train_loss: 0.016093\n",
      "[055/01069] train_loss: 0.016417\n",
      "[055/01119] train_loss: 0.015949\n",
      "[055/01169] train_loss: 0.015565\n",
      "[055/01219] train_loss: 0.015837\n",
      "[056/00043] train_loss: 0.017628\n",
      "[056/00093] train_loss: 0.018276\n",
      "[056/00143] train_loss: 0.017690\n",
      "[056/00193] train_loss: 0.016775\n",
      "[056/00243] train_loss: 0.016722\n",
      "[056/00293] train_loss: 0.016802\n",
      "[056/00343] train_loss: 0.016572\n",
      "[056/00393] train_loss: 0.016052\n",
      "[056/00443] train_loss: 0.016862\n",
      "[056/00493] train_loss: 0.015699\n",
      "[056/00543] train_loss: 0.016642\n",
      "[056/00593] train_loss: 0.017712\n",
      "[056/00643] train_loss: 0.015417\n",
      "[056/00693] train_loss: 0.015956\n",
      "[056/00743] train_loss: 0.016339\n",
      "[056/00793] train_loss: 0.017000\n",
      "[056/00843] train_loss: 0.015926\n",
      "[056/00893] train_loss: 0.015807\n",
      "[056/00943] train_loss: 0.016901\n",
      "[056/00993] train_loss: 0.015830\n",
      "[056/01043] train_loss: 0.016335\n",
      "[056/01093] train_loss: 0.015812\n",
      "[056/01143] train_loss: 0.016162\n",
      "[056/01193] train_loss: 0.015910\n",
      "[057/00017] train_loss: 0.016522\n",
      "[057/00067] train_loss: 0.017790\n",
      "[057/00117] train_loss: 0.017420\n",
      "[057/00167] train_loss: 0.016783\n",
      "[057/00217] train_loss: 0.017792\n",
      "[057/00267] train_loss: 0.016372\n",
      "[057/00317] train_loss: 0.016555\n",
      "[057/00367] train_loss: 0.015605\n",
      "[057/00417] train_loss: 0.016436\n",
      "[057/00467] train_loss: 0.015503\n",
      "[057/00517] train_loss: 0.016300\n",
      "[057/00567] train_loss: 0.017438\n",
      "[057/00617] train_loss: 0.016505\n",
      "[057/00667] train_loss: 0.015372\n",
      "[057/00717] train_loss: 0.016312\n",
      "[057/00767] train_loss: 0.015510\n",
      "[057/00817] train_loss: 0.016019\n",
      "[057/00867] train_loss: 0.015820\n",
      "[057/00917] train_loss: 0.015343\n",
      "[057/00967] train_loss: 0.016653\n",
      "[057/01017] train_loss: 0.016822\n",
      "[057/01067] train_loss: 0.016784\n",
      "[057/01117] train_loss: 0.016255\n",
      "[057/01167] train_loss: 0.016478\n",
      "[057/01217] train_loss: 0.015566\n",
      "[058/00041] train_loss: 0.017096\n",
      "[058/00091] train_loss: 0.017524\n",
      "[058/00141] train_loss: 0.017777\n",
      "[058/00191] train_loss: 0.016401\n",
      "[058/00241] train_loss: 0.016932\n",
      "[058/00291] train_loss: 0.017194\n",
      "[058/00341] train_loss: 0.016052\n",
      "[058/00391] train_loss: 0.017093\n",
      "[058/00441] train_loss: 0.016876\n",
      "[058/00491] train_loss: 0.015803\n",
      "[058/00541] train_loss: 0.016683\n",
      "[058/00591] train_loss: 0.016026\n",
      "[058/00641] train_loss: 0.015691\n",
      "[058/00691] train_loss: 0.016097\n",
      "[058/00741] train_loss: 0.015589\n",
      "[058/00791] train_loss: 0.015693\n",
      "[058/00841] train_loss: 0.015542\n",
      "[058/00891] train_loss: 0.015577\n",
      "[058/00941] train_loss: 0.016013\n",
      "[058/00991] train_loss: 0.015614\n",
      "[058/01041] train_loss: 0.016254\n",
      "[058/01091] train_loss: 0.015973\n",
      "[058/01141] train_loss: 0.015240\n",
      "[058/01191] train_loss: 0.015894\n",
      "[059/00015] train_loss: 0.016578\n",
      "[059/00065] train_loss: 0.017717\n",
      "[059/00115] train_loss: 0.016826\n",
      "[059/00165] train_loss: 0.017789\n",
      "[059/00215] train_loss: 0.016740\n",
      "[059/00265] train_loss: 0.016751\n",
      "[059/00315] train_loss: 0.016497\n",
      "[059/00365] train_loss: 0.016341\n",
      "[059/00415] train_loss: 0.015774\n",
      "[059/00465] train_loss: 0.016093\n",
      "[059/00515] train_loss: 0.016106\n",
      "[059/00565] train_loss: 0.016496\n",
      "[059/00615] train_loss: 0.016271\n",
      "[059/00665] train_loss: 0.015363\n",
      "[059/00715] train_loss: 0.016508\n",
      "[059/00765] train_loss: 0.014923\n",
      "[059/00815] train_loss: 0.015639\n",
      "[059/00865] train_loss: 0.016952\n",
      "[059/00915] train_loss: 0.015200\n",
      "[059/00965] train_loss: 0.016407\n",
      "[059/01015] train_loss: 0.016880\n",
      "[059/01065] train_loss: 0.014849\n",
      "[059/01115] train_loss: 0.016604\n",
      "[059/01165] train_loss: 0.016452\n",
      "[059/01215] train_loss: 0.016518\n",
      "[060/00039] train_loss: 0.017757\n",
      "[060/00089] train_loss: 0.016887\n",
      "[060/00139] train_loss: 0.017799\n",
      "[060/00189] train_loss: 0.016485\n",
      "[060/00239] train_loss: 0.016598\n",
      "[060/00289] train_loss: 0.016060\n",
      "[060/00339] train_loss: 0.016200\n",
      "[060/00389] train_loss: 0.015965\n",
      "[060/00439] train_loss: 0.017025\n",
      "[060/00489] train_loss: 0.016295\n",
      "[060/00539] train_loss: 0.015631\n",
      "[060/00589] train_loss: 0.016482\n",
      "[060/00639] train_loss: 0.016031\n",
      "[060/00689] train_loss: 0.015638\n",
      "[060/00739] train_loss: 0.015645\n",
      "[060/00789] train_loss: 0.014767\n",
      "[060/00839] train_loss: 0.015728\n",
      "[060/00889] train_loss: 0.015147\n",
      "[060/00939] train_loss: 0.015579\n",
      "[060/00989] train_loss: 0.015677\n",
      "[060/01039] train_loss: 0.016407\n",
      "[060/01089] train_loss: 0.016100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[060/01139] train_loss: 0.015888\n",
      "[060/01189] train_loss: 0.015738\n",
      "[061/00013] train_loss: 0.016301\n",
      "[061/00063] train_loss: 0.017526\n",
      "[061/00113] train_loss: 0.017197\n",
      "[061/00163] train_loss: 0.016838\n",
      "[061/00213] train_loss: 0.016350\n",
      "[061/00263] train_loss: 0.016280\n",
      "[061/00313] train_loss: 0.015905\n",
      "[061/00363] train_loss: 0.016344\n",
      "[061/00413] train_loss: 0.015512\n",
      "[061/00463] train_loss: 0.016490\n",
      "[061/00513] train_loss: 0.015934\n",
      "[061/00563] train_loss: 0.015833\n",
      "[061/00613] train_loss: 0.016014\n",
      "[061/00663] train_loss: 0.014921\n",
      "[061/00713] train_loss: 0.015968\n",
      "[061/00763] train_loss: 0.015990\n",
      "[061/00813] train_loss: 0.015579\n",
      "[061/00863] train_loss: 0.016049\n",
      "[061/00913] train_loss: 0.015612\n",
      "[061/00963] train_loss: 0.015361\n",
      "[061/01013] train_loss: 0.015257\n",
      "[061/01063] train_loss: 0.016385\n",
      "[061/01113] train_loss: 0.015570\n",
      "[061/01163] train_loss: 0.016363\n",
      "[061/01213] train_loss: 0.015675\n",
      "[062/00037] train_loss: 0.018634\n",
      "[062/00087] train_loss: 0.017495\n",
      "[062/00137] train_loss: 0.017588\n",
      "[062/00187] train_loss: 0.016915\n",
      "[062/00237] train_loss: 0.017271\n",
      "[062/00287] train_loss: 0.016577\n",
      "[062/00337] train_loss: 0.016407\n",
      "[062/00387] train_loss: 0.016414\n",
      "[062/00437] train_loss: 0.016776\n",
      "[062/00487] train_loss: 0.016475\n",
      "[062/00537] train_loss: 0.016481\n",
      "[062/00587] train_loss: 0.015154\n",
      "[062/00637] train_loss: 0.015623\n",
      "[062/00687] train_loss: 0.015656\n",
      "[062/00737] train_loss: 0.016571\n",
      "[062/00787] train_loss: 0.015855\n",
      "[062/00837] train_loss: 0.015525\n",
      "[062/00887] train_loss: 0.015457\n",
      "[062/00937] train_loss: 0.015997\n",
      "[062/00987] train_loss: 0.015104\n",
      "[062/01037] train_loss: 0.015104\n",
      "[062/01087] train_loss: 0.015375\n",
      "[062/01137] train_loss: 0.015900\n",
      "[062/01187] train_loss: 0.015670\n",
      "[063/00011] train_loss: 0.016034\n",
      "[063/00061] train_loss: 0.017916\n",
      "[063/00111] train_loss: 0.018353\n",
      "[063/00161] train_loss: 0.016768\n",
      "[063/00211] train_loss: 0.016825\n",
      "[063/00261] train_loss: 0.016847\n",
      "[063/00311] train_loss: 0.016532\n",
      "[063/00361] train_loss: 0.016213\n",
      "[063/00411] train_loss: 0.015333\n",
      "[063/00461] train_loss: 0.016462\n",
      "[063/00511] train_loss: 0.015345\n",
      "[063/00561] train_loss: 0.016870\n",
      "[063/00611] train_loss: 0.015557\n",
      "[063/00661] train_loss: 0.016398\n",
      "[063/00711] train_loss: 0.015581\n",
      "[063/00761] train_loss: 0.015003\n",
      "[063/00811] train_loss: 0.015971\n",
      "[063/00861] train_loss: 0.015999\n",
      "[063/00911] train_loss: 0.016447\n",
      "[063/00961] train_loss: 0.016865\n",
      "[063/01011] train_loss: 0.016058\n",
      "[063/01061] train_loss: 0.015211\n",
      "[063/01111] train_loss: 0.015826\n",
      "[063/01161] train_loss: 0.015304\n",
      "[063/01211] train_loss: 0.016244\n",
      "[064/00035] train_loss: 0.017899\n",
      "[064/00085] train_loss: 0.017028\n",
      "[064/00135] train_loss: 0.016976\n",
      "[064/00185] train_loss: 0.017145\n",
      "[064/00235] train_loss: 0.017086\n",
      "[064/00285] train_loss: 0.016370\n",
      "[064/00335] train_loss: 0.015649\n",
      "[064/00385] train_loss: 0.016317\n",
      "[064/00435] train_loss: 0.016915\n",
      "[064/00485] train_loss: 0.015950\n",
      "[064/00535] train_loss: 0.015700\n",
      "[064/00585] train_loss: 0.016006\n",
      "[064/00635] train_loss: 0.015869\n",
      "[064/00685] train_loss: 0.015130\n",
      "[064/00735] train_loss: 0.015753\n",
      "[064/00785] train_loss: 0.016186\n",
      "[064/00835] train_loss: 0.015223\n",
      "[064/00885] train_loss: 0.015418\n",
      "[064/00935] train_loss: 0.015771\n",
      "[064/00985] train_loss: 0.015899\n",
      "[064/01035] train_loss: 0.015470\n",
      "[064/01085] train_loss: 0.015821\n",
      "[064/01135] train_loss: 0.015428\n",
      "[064/01185] train_loss: 0.016059\n",
      "[065/00009] train_loss: 0.015878\n",
      "[065/00059] train_loss: 0.018362\n",
      "[065/00109] train_loss: 0.017455\n",
      "[065/00159] train_loss: 0.016105\n",
      "[065/00209] train_loss: 0.016753\n",
      "[065/00259] train_loss: 0.015825\n",
      "[065/00309] train_loss: 0.015222\n",
      "[065/00359] train_loss: 0.016131\n",
      "[065/00409] train_loss: 0.016516\n",
      "[065/00459] train_loss: 0.016140\n",
      "[065/00509] train_loss: 0.015628\n",
      "[065/00559] train_loss: 0.015381\n",
      "[065/00609] train_loss: 0.015415\n",
      "[065/00659] train_loss: 0.015530\n",
      "[065/00709] train_loss: 0.016246\n",
      "[065/00759] train_loss: 0.014600\n",
      "[065/00809] train_loss: 0.016147\n",
      "[065/00859] train_loss: 0.016553\n",
      "[065/00909] train_loss: 0.015935\n",
      "[065/00959] train_loss: 0.016061\n",
      "[065/01009] train_loss: 0.015460\n",
      "[065/01059] train_loss: 0.016038\n",
      "[065/01109] train_loss: 0.015317\n",
      "[065/01159] train_loss: 0.015012\n",
      "[065/01209] train_loss: 0.014914\n",
      "[066/00033] train_loss: 0.017485\n",
      "[066/00083] train_loss: 0.018091\n",
      "[066/00133] train_loss: 0.016741\n",
      "[066/00183] train_loss: 0.016518\n",
      "[066/00233] train_loss: 0.016523\n",
      "[066/00283] train_loss: 0.016195\n",
      "[066/00333] train_loss: 0.016254\n",
      "[066/00383] train_loss: 0.016209\n",
      "[066/00433] train_loss: 0.016268\n",
      "[066/00483] train_loss: 0.014373\n",
      "[066/00533] train_loss: 0.015011\n",
      "[066/00583] train_loss: 0.015137\n",
      "[066/00633] train_loss: 0.015851\n",
      "[066/00683] train_loss: 0.015485\n",
      "[066/00733] train_loss: 0.015610\n",
      "[066/00783] train_loss: 0.016544\n",
      "[066/00833] train_loss: 0.015523\n",
      "[066/00883] train_loss: 0.015425\n",
      "[066/00933] train_loss: 0.015435\n",
      "[066/00983] train_loss: 0.015752\n",
      "[066/01033] train_loss: 0.016034\n",
      "[066/01083] train_loss: 0.016231\n",
      "[066/01133] train_loss: 0.016332\n",
      "[066/01183] train_loss: 0.016224\n",
      "[067/00007] train_loss: 0.015358\n",
      "[067/00057] train_loss: 0.018243\n",
      "[067/00107] train_loss: 0.016427\n",
      "[067/00157] train_loss: 0.016802\n",
      "[067/00207] train_loss: 0.016699\n",
      "[067/00257] train_loss: 0.016176\n",
      "[067/00307] train_loss: 0.016769\n",
      "[067/00357] train_loss: 0.016625\n",
      "[067/00407] train_loss: 0.015489\n",
      "[067/00457] train_loss: 0.016087\n",
      "[067/00507] train_loss: 0.015435\n",
      "[067/00557] train_loss: 0.016029\n",
      "[067/00607] train_loss: 0.015941\n",
      "[067/00657] train_loss: 0.015393\n",
      "[067/00707] train_loss: 0.015901\n",
      "[067/00757] train_loss: 0.015434\n",
      "[067/00807] train_loss: 0.015738\n",
      "[067/00857] train_loss: 0.015138\n",
      "[067/00907] train_loss: 0.015008\n",
      "[067/00957] train_loss: 0.016010\n",
      "[067/01007] train_loss: 0.015820\n",
      "[067/01057] train_loss: 0.015589\n",
      "[067/01107] train_loss: 0.015887\n",
      "[067/01157] train_loss: 0.015247\n",
      "[067/01207] train_loss: 0.015797\n",
      "[068/00031] train_loss: 0.017382\n",
      "[068/00081] train_loss: 0.017577\n",
      "[068/00131] train_loss: 0.017634\n",
      "[068/00181] train_loss: 0.016560\n",
      "[068/00231] train_loss: 0.016198\n",
      "[068/00281] train_loss: 0.016516\n",
      "[068/00331] train_loss: 0.015812\n",
      "[068/00381] train_loss: 0.015957\n",
      "[068/00431] train_loss: 0.015715\n",
      "[068/00481] train_loss: 0.015605\n",
      "[068/00531] train_loss: 0.015872\n",
      "[068/00581] train_loss: 0.015696\n",
      "[068/00631] train_loss: 0.015404\n",
      "[068/00681] train_loss: 0.015377\n",
      "[068/00731] train_loss: 0.016229\n",
      "[068/00781] train_loss: 0.015691\n",
      "[068/00831] train_loss: 0.015676\n",
      "[068/00881] train_loss: 0.015424\n",
      "[068/00931] train_loss: 0.015926\n",
      "[068/00981] train_loss: 0.015321\n",
      "[068/01031] train_loss: 0.015582\n",
      "[068/01081] train_loss: 0.014698\n",
      "[068/01131] train_loss: 0.016462\n",
      "[068/01181] train_loss: 0.015418\n",
      "[069/00005] train_loss: 0.016231\n",
      "[069/00055] train_loss: 0.017658\n",
      "[069/00105] train_loss: 0.017737\n",
      "[069/00155] train_loss: 0.017084\n",
      "[069/00205] train_loss: 0.016910\n",
      "[069/00255] train_loss: 0.015874\n",
      "[069/00305] train_loss: 0.015630\n",
      "[069/00355] train_loss: 0.015308\n",
      "[069/00405] train_loss: 0.015822\n",
      "[069/00455] train_loss: 0.015729\n",
      "[069/00505] train_loss: 0.016437\n",
      "[069/00555] train_loss: 0.015694\n",
      "[069/00605] train_loss: 0.015853\n",
      "[069/00655] train_loss: 0.015842\n",
      "[069/00705] train_loss: 0.015348\n",
      "[069/00755] train_loss: 0.016673\n",
      "[069/00805] train_loss: 0.015793\n",
      "[069/00855] train_loss: 0.014996\n",
      "[069/00905] train_loss: 0.015681\n",
      "[069/00955] train_loss: 0.014926\n",
      "[069/01005] train_loss: 0.015067\n",
      "[069/01055] train_loss: 0.015113\n",
      "[069/01105] train_loss: 0.015036\n",
      "[069/01155] train_loss: 0.015416\n",
      "[069/01205] train_loss: 0.015343\n",
      "[070/00029] train_loss: 0.016305\n",
      "[070/00079] train_loss: 0.016729\n",
      "[070/00129] train_loss: 0.017163\n",
      "[070/00179] train_loss: 0.016083\n",
      "[070/00229] train_loss: 0.016656\n",
      "[070/00279] train_loss: 0.016957\n",
      "[070/00329] train_loss: 0.016373\n",
      "[070/00379] train_loss: 0.016217\n",
      "[070/00429] train_loss: 0.016026\n",
      "[070/00479] train_loss: 0.015567\n",
      "[070/00529] train_loss: 0.015099\n",
      "[070/00579] train_loss: 0.015405\n",
      "[070/00629] train_loss: 0.015689\n",
      "[070/00679] train_loss: 0.015554\n",
      "[070/00729] train_loss: 0.015587\n",
      "[070/00779] train_loss: 0.016772\n",
      "[070/00829] train_loss: 0.015303\n",
      "[070/00879] train_loss: 0.015585\n",
      "[070/00929] train_loss: 0.015843\n",
      "[070/00979] train_loss: 0.015018\n",
      "[070/01029] train_loss: 0.015138\n",
      "[070/01079] train_loss: 0.015246\n",
      "[070/01129] train_loss: 0.015520\n",
      "[070/01179] train_loss: 0.015919\n",
      "[071/00003] train_loss: 0.015104\n",
      "[071/00053] train_loss: 0.017989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[071/00103] train_loss: 0.016618\n",
      "[071/00153] train_loss: 0.016242\n",
      "[071/00203] train_loss: 0.016098\n",
      "[071/00253] train_loss: 0.015262\n",
      "[071/00303] train_loss: 0.016330\n",
      "[071/00353] train_loss: 0.016559\n",
      "[071/00403] train_loss: 0.016059\n",
      "[071/00453] train_loss: 0.015913\n",
      "[071/00503] train_loss: 0.015064\n",
      "[071/00553] train_loss: 0.014675\n",
      "[071/00603] train_loss: 0.015684\n",
      "[071/00653] train_loss: 0.015566\n",
      "[071/00703] train_loss: 0.015131\n",
      "[071/00753] train_loss: 0.015582\n",
      "[071/00803] train_loss: 0.015401\n",
      "[071/00853] train_loss: 0.015395\n",
      "[071/00903] train_loss: 0.015698\n",
      "[071/00953] train_loss: 0.015165\n",
      "[071/01003] train_loss: 0.015043\n",
      "[071/01053] train_loss: 0.015545\n",
      "[071/01103] train_loss: 0.015314\n",
      "[071/01153] train_loss: 0.015399\n",
      "[071/01203] train_loss: 0.016014\n",
      "[072/00027] train_loss: 0.016195\n",
      "[072/00077] train_loss: 0.017429\n",
      "[072/00127] train_loss: 0.016760\n",
      "[072/00177] train_loss: 0.016758\n",
      "[072/00227] train_loss: 0.016349\n",
      "[072/00277] train_loss: 0.015997\n",
      "[072/00327] train_loss: 0.015742\n",
      "[072/00377] train_loss: 0.016055\n",
      "[072/00427] train_loss: 0.015273\n",
      "[072/00477] train_loss: 0.015393\n",
      "[072/00527] train_loss: 0.015725\n",
      "[072/00577] train_loss: 0.015927\n",
      "[072/00627] train_loss: 0.015593\n",
      "[072/00677] train_loss: 0.016012\n",
      "[072/00727] train_loss: 0.014917\n",
      "[072/00777] train_loss: 0.015129\n",
      "[072/00827] train_loss: 0.014754\n",
      "[072/00877] train_loss: 0.015103\n",
      "[072/00927] train_loss: 0.015528\n",
      "[072/00977] train_loss: 0.016075\n",
      "[072/01027] train_loss: 0.015273\n",
      "[072/01077] train_loss: 0.016187\n",
      "[072/01127] train_loss: 0.016531\n",
      "[072/01177] train_loss: 0.014901\n",
      "[073/00001] train_loss: 0.015329\n",
      "[073/00051] train_loss: 0.017856\n",
      "[073/00101] train_loss: 0.017925\n",
      "[073/00151] train_loss: 0.016352\n",
      "[073/00201] train_loss: 0.016326\n",
      "[073/00251] train_loss: 0.015984\n",
      "[073/00301] train_loss: 0.015418\n",
      "[073/00351] train_loss: 0.015974\n",
      "[073/00401] train_loss: 0.015396\n",
      "[073/00451] train_loss: 0.015815\n",
      "[073/00501] train_loss: 0.015599\n",
      "[073/00551] train_loss: 0.016051\n",
      "[073/00601] train_loss: 0.016075\n",
      "[073/00651] train_loss: 0.015795\n",
      "[073/00701] train_loss: 0.015792\n",
      "[073/00751] train_loss: 0.015775\n",
      "[073/00801] train_loss: 0.015116\n",
      "[073/00851] train_loss: 0.015256\n",
      "[073/00901] train_loss: 0.015537\n",
      "[073/00951] train_loss: 0.014944\n",
      "[073/01001] train_loss: 0.015545\n",
      "[073/01051] train_loss: 0.015003\n",
      "[073/01101] train_loss: 0.014603\n",
      "[073/01151] train_loss: 0.015017\n",
      "[073/01201] train_loss: 0.014989\n",
      "[074/00025] train_loss: 0.016228\n",
      "[074/00075] train_loss: 0.017291\n",
      "[074/00125] train_loss: 0.016808\n",
      "[074/00175] train_loss: 0.017636\n",
      "[074/00225] train_loss: 0.016277\n",
      "[074/00275] train_loss: 0.015864\n",
      "[074/00325] train_loss: 0.016000\n",
      "[074/00375] train_loss: 0.015939\n",
      "[074/00425] train_loss: 0.015693\n",
      "[074/00475] train_loss: 0.015822\n",
      "[074/00525] train_loss: 0.015341\n",
      "[074/00575] train_loss: 0.015890\n",
      "[074/00625] train_loss: 0.015733\n",
      "[074/00675] train_loss: 0.015553\n",
      "[074/00725] train_loss: 0.015567\n",
      "[074/00775] train_loss: 0.015611\n",
      "[074/00825] train_loss: 0.015262\n",
      "[074/00875] train_loss: 0.015745\n",
      "[074/00925] train_loss: 0.015100\n",
      "[074/00975] train_loss: 0.014899\n",
      "[074/01025] train_loss: 0.014797\n",
      "[074/01075] train_loss: 0.015091\n",
      "[074/01125] train_loss: 0.014791\n",
      "[074/01175] train_loss: 0.015687\n",
      "[074/01225] train_loss: 0.014722\n",
      "[075/00049] train_loss: 0.017290\n",
      "[075/00099] train_loss: 0.017218\n",
      "[075/00149] train_loss: 0.016641\n",
      "[075/00199] train_loss: 0.017184\n",
      "[075/00249] train_loss: 0.016323\n",
      "[075/00299] train_loss: 0.015557\n",
      "[075/00349] train_loss: 0.015396\n",
      "[075/00399] train_loss: 0.015769\n",
      "[075/00449] train_loss: 0.016582\n",
      "[075/00499] train_loss: 0.015959\n",
      "[075/00549] train_loss: 0.014932\n",
      "[075/00599] train_loss: 0.014830\n",
      "[075/00649] train_loss: 0.015301\n",
      "[075/00699] train_loss: 0.015657\n",
      "[075/00749] train_loss: 0.016463\n",
      "[075/00799] train_loss: 0.015387\n",
      "[075/00849] train_loss: 0.015045\n",
      "[075/00899] train_loss: 0.015762\n",
      "[075/00949] train_loss: 0.015052\n",
      "[075/00999] train_loss: 0.014907\n",
      "[075/01049] train_loss: 0.015611\n",
      "[075/01099] train_loss: 0.014575\n",
      "[075/01149] train_loss: 0.015066\n",
      "[075/01199] train_loss: 0.014748\n",
      "[076/00023] train_loss: 0.016625\n",
      "[076/00073] train_loss: 0.017469\n",
      "[076/00123] train_loss: 0.016332\n",
      "[076/00173] train_loss: 0.015458\n",
      "[076/00223] train_loss: 0.016295\n",
      "[076/00273] train_loss: 0.016290\n",
      "[076/00323] train_loss: 0.015740\n",
      "[076/00373] train_loss: 0.016110\n",
      "[076/00423] train_loss: 0.014751\n",
      "[076/00473] train_loss: 0.014370\n",
      "[076/00523] train_loss: 0.015313\n",
      "[076/00573] train_loss: 0.015488\n",
      "[076/00623] train_loss: 0.015119\n",
      "[076/00673] train_loss: 0.015272\n",
      "[076/00723] train_loss: 0.014991\n",
      "[076/00773] train_loss: 0.015525\n",
      "[076/00823] train_loss: 0.014594\n",
      "[076/00873] train_loss: 0.015495\n",
      "[076/00923] train_loss: 0.014824\n",
      "[076/00973] train_loss: 0.014534\n",
      "[076/01023] train_loss: 0.015828\n",
      "[076/01073] train_loss: 0.014816\n",
      "[076/01123] train_loss: 0.015439\n",
      "[076/01173] train_loss: 0.015122\n",
      "[076/01223] train_loss: 0.015367\n",
      "[077/00047] train_loss: 0.017972\n",
      "[077/00097] train_loss: 0.016585\n",
      "[077/00147] train_loss: 0.016928\n",
      "[077/00197] train_loss: 0.016634\n",
      "[077/00247] train_loss: 0.015829\n",
      "[077/00297] train_loss: 0.016274\n",
      "[077/00347] train_loss: 0.014722\n",
      "[077/00397] train_loss: 0.015443\n",
      "[077/00447] train_loss: 0.015249\n",
      "[077/00497] train_loss: 0.015259\n",
      "[077/00547] train_loss: 0.015781\n",
      "[077/00597] train_loss: 0.015851\n",
      "[077/00647] train_loss: 0.015048\n",
      "[077/00697] train_loss: 0.014896\n",
      "[077/00747] train_loss: 0.015832\n",
      "[077/00797] train_loss: 0.015057\n",
      "[077/00847] train_loss: 0.016021\n",
      "[077/00897] train_loss: 0.014844\n",
      "[077/00947] train_loss: 0.015092\n",
      "[077/00997] train_loss: 0.015079\n",
      "[077/01047] train_loss: 0.016181\n",
      "[077/01097] train_loss: 0.014758\n",
      "[077/01147] train_loss: 0.015234\n",
      "[077/01197] train_loss: 0.015227\n",
      "[078/00021] train_loss: 0.015582\n",
      "[078/00071] train_loss: 0.017381\n",
      "[078/00121] train_loss: 0.017492\n",
      "[078/00171] train_loss: 0.016331\n",
      "[078/00221] train_loss: 0.015027\n",
      "[078/00271] train_loss: 0.016133\n",
      "[078/00321] train_loss: 0.015416\n",
      "[078/00371] train_loss: 0.016060\n",
      "[078/00421] train_loss: 0.015210\n",
      "[078/00471] train_loss: 0.016119\n",
      "[078/00521] train_loss: 0.016256\n",
      "[078/00571] train_loss: 0.015582\n",
      "[078/00621] train_loss: 0.014577\n",
      "[078/00671] train_loss: 0.015837\n",
      "[078/00721] train_loss: 0.015622\n",
      "[078/00771] train_loss: 0.014764\n",
      "[078/00821] train_loss: 0.016048\n",
      "[078/00871] train_loss: 0.015250\n",
      "[078/00921] train_loss: 0.015141\n",
      "[078/00971] train_loss: 0.014675\n",
      "[078/01021] train_loss: 0.015832\n",
      "[078/01071] train_loss: 0.014773\n",
      "[078/01121] train_loss: 0.014592\n",
      "[078/01171] train_loss: 0.014527\n",
      "[078/01221] train_loss: 0.014678\n",
      "[079/00045] train_loss: 0.016967\n",
      "[079/00095] train_loss: 0.017994\n",
      "[079/00145] train_loss: 0.016090\n",
      "[079/00195] train_loss: 0.015652\n",
      "[079/00245] train_loss: 0.015945\n",
      "[079/00295] train_loss: 0.015341\n",
      "[079/00345] train_loss: 0.015930\n",
      "[079/00395] train_loss: 0.015477\n",
      "[079/00445] train_loss: 0.014859\n",
      "[079/00495] train_loss: 0.014905\n",
      "[079/00545] train_loss: 0.014778\n",
      "[079/00595] train_loss: 0.015357\n",
      "[079/00645] train_loss: 0.015205\n",
      "[079/00695] train_loss: 0.014619\n",
      "[079/00745] train_loss: 0.014551\n",
      "[079/00795] train_loss: 0.015335\n",
      "[079/00845] train_loss: 0.014882\n",
      "[079/00895] train_loss: 0.015167\n",
      "[079/00945] train_loss: 0.014871\n",
      "[079/00995] train_loss: 0.014807\n",
      "[079/01045] train_loss: 0.015157\n",
      "[079/01095] train_loss: 0.016235\n",
      "[079/01145] train_loss: 0.015872\n",
      "[079/01195] train_loss: 0.015270\n",
      "[080/00019] train_loss: 0.015783\n",
      "[080/00069] train_loss: 0.017619\n",
      "[080/00119] train_loss: 0.016249\n",
      "[080/00169] train_loss: 0.016454\n",
      "[080/00219] train_loss: 0.016391\n",
      "[080/00269] train_loss: 0.015468\n",
      "[080/00319] train_loss: 0.015626\n",
      "[080/00369] train_loss: 0.015475\n",
      "[080/00419] train_loss: 0.016260\n",
      "[080/00469] train_loss: 0.014968\n",
      "[080/00519] train_loss: 0.015578\n",
      "[080/00569] train_loss: 0.015862\n",
      "[080/00619] train_loss: 0.015444\n",
      "[080/00669] train_loss: 0.014140\n",
      "[080/00719] train_loss: 0.015763\n",
      "[080/00769] train_loss: 0.014884\n",
      "[080/00819] train_loss: 0.014875\n",
      "[080/00869] train_loss: 0.014953\n",
      "[080/00919] train_loss: 0.015236\n",
      "[080/00969] train_loss: 0.015776\n",
      "[080/01019] train_loss: 0.015115\n",
      "[080/01069] train_loss: 0.015017\n",
      "[080/01119] train_loss: 0.015399\n",
      "[080/01169] train_loss: 0.014890\n",
      "[080/01219] train_loss: 0.015606\n",
      "[081/00043] train_loss: 0.017361\n",
      "[081/00093] train_loss: 0.016444\n",
      "[081/00143] train_loss: 0.015671\n",
      "[081/00193] train_loss: 0.017482\n",
      "[081/00243] train_loss: 0.016359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[081/00293] train_loss: 0.015671\n",
      "[081/00343] train_loss: 0.014934\n",
      "[081/00393] train_loss: 0.015689\n",
      "[081/00443] train_loss: 0.015853\n",
      "[081/00493] train_loss: 0.014931\n",
      "[081/00543] train_loss: 0.014629\n",
      "[081/00593] train_loss: 0.015116\n",
      "[081/00643] train_loss: 0.015077\n",
      "[081/00693] train_loss: 0.015140\n",
      "[081/00743] train_loss: 0.014415\n",
      "[081/00793] train_loss: 0.015685\n",
      "[081/00843] train_loss: 0.014882\n",
      "[081/00893] train_loss: 0.015003\n",
      "[081/00943] train_loss: 0.015804\n",
      "[081/00993] train_loss: 0.015711\n",
      "[081/01043] train_loss: 0.015569\n",
      "[081/01093] train_loss: 0.014814\n",
      "[081/01143] train_loss: 0.014039\n",
      "[081/01193] train_loss: 0.014639\n",
      "[082/00017] train_loss: 0.015240\n",
      "[082/00067] train_loss: 0.016771\n",
      "[082/00117] train_loss: 0.016489\n",
      "[082/00167] train_loss: 0.016040\n",
      "[082/00217] train_loss: 0.016614\n",
      "[082/00267] train_loss: 0.015183\n",
      "[082/00317] train_loss: 0.017152\n",
      "[082/00367] train_loss: 0.015243\n",
      "[082/00417] train_loss: 0.015252\n",
      "[082/00467] train_loss: 0.014663\n",
      "[082/00517] train_loss: 0.014828\n",
      "[082/00567] train_loss: 0.015583\n",
      "[082/00617] train_loss: 0.014571\n",
      "[082/00667] train_loss: 0.015703\n",
      "[082/00717] train_loss: 0.015294\n",
      "[082/00767] train_loss: 0.015180\n",
      "[082/00817] train_loss: 0.014844\n",
      "[082/00867] train_loss: 0.014536\n",
      "[082/00917] train_loss: 0.015261\n",
      "[082/00967] train_loss: 0.014299\n",
      "[082/01017] train_loss: 0.015055\n",
      "[082/01067] train_loss: 0.016323\n",
      "[082/01117] train_loss: 0.014521\n",
      "[082/01167] train_loss: 0.015529\n",
      "[082/01217] train_loss: 0.015475\n",
      "[083/00041] train_loss: 0.017891\n",
      "[083/00091] train_loss: 0.016443\n",
      "[083/00141] train_loss: 0.017594\n",
      "[083/00191] train_loss: 0.016260\n",
      "[083/00241] train_loss: 0.015889\n",
      "[083/00291] train_loss: 0.015583\n",
      "[083/00341] train_loss: 0.015891\n",
      "[083/00391] train_loss: 0.015206\n",
      "[083/00441] train_loss: 0.014536\n",
      "[083/00491] train_loss: 0.015821\n",
      "[083/00541] train_loss: 0.015264\n",
      "[083/00591] train_loss: 0.015269\n",
      "[083/00641] train_loss: 0.014822\n",
      "[083/00691] train_loss: 0.014555\n",
      "[083/00741] train_loss: 0.015423\n",
      "[083/00791] train_loss: 0.014404\n",
      "[083/00841] train_loss: 0.015232\n",
      "[083/00891] train_loss: 0.015521\n",
      "[083/00941] train_loss: 0.014748\n",
      "[083/00991] train_loss: 0.015265\n",
      "[083/01041] train_loss: 0.014546\n",
      "[083/01091] train_loss: 0.014723\n",
      "[083/01141] train_loss: 0.014995\n",
      "[083/01191] train_loss: 0.015046\n",
      "[084/00015] train_loss: 0.015369\n",
      "[084/00065] train_loss: 0.016832\n",
      "[084/00115] train_loss: 0.015734\n",
      "[084/00165] train_loss: 0.016389\n",
      "[084/00215] train_loss: 0.015640\n",
      "[084/00265] train_loss: 0.015162\n",
      "[084/00315] train_loss: 0.015358\n",
      "[084/00365] train_loss: 0.016279\n",
      "[084/00415] train_loss: 0.015073\n",
      "[084/00465] train_loss: 0.015080\n",
      "[084/00515] train_loss: 0.016026\n",
      "[084/00565] train_loss: 0.015521\n",
      "[084/00615] train_loss: 0.015238\n",
      "[084/00665] train_loss: 0.015844\n",
      "[084/00715] train_loss: 0.014680\n",
      "[084/00765] train_loss: 0.015665\n",
      "[084/00815] train_loss: 0.015276\n",
      "[084/00865] train_loss: 0.014949\n",
      "[084/00915] train_loss: 0.015113\n",
      "[084/00965] train_loss: 0.014911\n",
      "[084/01015] train_loss: 0.015574\n",
      "[084/01065] train_loss: 0.014773\n",
      "[084/01115] train_loss: 0.014538\n",
      "[084/01165] train_loss: 0.014331\n",
      "[084/01215] train_loss: 0.014535\n",
      "[085/00039] train_loss: 0.016573\n",
      "[085/00089] train_loss: 0.016556\n",
      "[085/00139] train_loss: 0.016334\n",
      "[085/00189] train_loss: 0.016677\n",
      "[085/00239] train_loss: 0.015132\n",
      "[085/00289] train_loss: 0.015758\n",
      "[085/00339] train_loss: 0.015445\n",
      "[085/00389] train_loss: 0.014542\n",
      "[085/00439] train_loss: 0.015955\n",
      "[085/00489] train_loss: 0.015572\n",
      "[085/00539] train_loss: 0.014864\n",
      "[085/00589] train_loss: 0.014811\n",
      "[085/00639] train_loss: 0.015531\n",
      "[085/00689] train_loss: 0.014464\n",
      "[085/00739] train_loss: 0.015092\n",
      "[085/00789] train_loss: 0.015684\n",
      "[085/00839] train_loss: 0.014975\n",
      "[085/00889] train_loss: 0.014045\n",
      "[085/00939] train_loss: 0.015202\n",
      "[085/00989] train_loss: 0.015510\n",
      "[085/01039] train_loss: 0.014932\n",
      "[085/01089] train_loss: 0.015366\n",
      "[085/01139] train_loss: 0.014717\n",
      "[085/01189] train_loss: 0.015680\n",
      "[086/00013] train_loss: 0.015577\n",
      "[086/00063] train_loss: 0.016616\n",
      "[086/00113] train_loss: 0.015528\n",
      "[086/00163] train_loss: 0.015908\n",
      "[086/00213] train_loss: 0.016001\n",
      "[086/00263] train_loss: 0.015464\n",
      "[086/00313] train_loss: 0.014908\n",
      "[086/00363] train_loss: 0.015472\n",
      "[086/00413] train_loss: 0.015120\n",
      "[086/00463] train_loss: 0.015236\n",
      "[086/00513] train_loss: 0.016136\n",
      "[086/00563] train_loss: 0.015123\n",
      "[086/00613] train_loss: 0.015353\n",
      "[086/00663] train_loss: 0.014949\n",
      "[086/00713] train_loss: 0.015720\n",
      "[086/00763] train_loss: 0.015313\n",
      "[086/00813] train_loss: 0.014919\n",
      "[086/00863] train_loss: 0.014486\n",
      "[086/00913] train_loss: 0.014869\n",
      "[086/00963] train_loss: 0.015377\n",
      "[086/01013] train_loss: 0.015007\n",
      "[086/01063] train_loss: 0.015139\n",
      "[086/01113] train_loss: 0.014431\n",
      "[086/01163] train_loss: 0.014590\n",
      "[086/01213] train_loss: 0.015396\n",
      "[087/00037] train_loss: 0.016427\n",
      "[087/00087] train_loss: 0.015903\n",
      "[087/00137] train_loss: 0.016871\n",
      "[087/00187] train_loss: 0.015705\n",
      "[087/00237] train_loss: 0.015682\n",
      "[087/00287] train_loss: 0.015484\n",
      "[087/00337] train_loss: 0.015592\n",
      "[087/00387] train_loss: 0.016273\n",
      "[087/00437] train_loss: 0.016335\n",
      "[087/00487] train_loss: 0.015369\n",
      "[087/00537] train_loss: 0.015631\n",
      "[087/00587] train_loss: 0.015658\n",
      "[087/00637] train_loss: 0.015075\n",
      "[087/00687] train_loss: 0.014850\n",
      "[087/00737] train_loss: 0.015452\n",
      "[087/00787] train_loss: 0.014555\n",
      "[087/00837] train_loss: 0.014509\n",
      "[087/00887] train_loss: 0.014759\n",
      "[087/00937] train_loss: 0.015006\n",
      "[087/00987] train_loss: 0.015242\n",
      "[087/01037] train_loss: 0.015812\n",
      "[087/01087] train_loss: 0.014788\n",
      "[087/01137] train_loss: 0.014562\n",
      "[087/01187] train_loss: 0.014650\n",
      "[088/00011] train_loss: 0.015122\n",
      "[088/00061] train_loss: 0.016285\n",
      "[088/00111] train_loss: 0.015966\n",
      "[088/00161] train_loss: 0.015133\n",
      "[088/00211] train_loss: 0.016121\n",
      "[088/00261] train_loss: 0.016665\n",
      "[088/00311] train_loss: 0.014603\n",
      "[088/00361] train_loss: 0.015691\n",
      "[088/00411] train_loss: 0.016549\n",
      "[088/00461] train_loss: 0.015654\n",
      "[088/00511] train_loss: 0.015042\n",
      "[088/00561] train_loss: 0.014876\n",
      "[088/00611] train_loss: 0.015024\n",
      "[088/00661] train_loss: 0.014533\n",
      "[088/00711] train_loss: 0.014621\n",
      "[088/00761] train_loss: 0.015024\n",
      "[088/00811] train_loss: 0.015191\n",
      "[088/00861] train_loss: 0.014819\n",
      "[088/00911] train_loss: 0.015570\n",
      "[088/00961] train_loss: 0.015253\n",
      "[088/01011] train_loss: 0.014543\n",
      "[088/01061] train_loss: 0.014447\n",
      "[088/01111] train_loss: 0.014259\n",
      "[088/01161] train_loss: 0.015236\n",
      "[088/01211] train_loss: 0.015194\n",
      "[089/00035] train_loss: 0.016011\n",
      "[089/00085] train_loss: 0.016574\n",
      "[089/00135] train_loss: 0.015964\n",
      "[089/00185] train_loss: 0.015548\n",
      "[089/00235] train_loss: 0.015318\n",
      "[089/00285] train_loss: 0.015399\n",
      "[089/00335] train_loss: 0.015064\n",
      "[089/00385] train_loss: 0.015503\n",
      "[089/00435] train_loss: 0.014710\n",
      "[089/00485] train_loss: 0.015647\n",
      "[089/00535] train_loss: 0.014634\n",
      "[089/00585] train_loss: 0.014997\n",
      "[089/00635] train_loss: 0.016208\n",
      "[089/00685] train_loss: 0.015136\n",
      "[089/00735] train_loss: 0.014317\n",
      "[089/00785] train_loss: 0.015124\n",
      "[089/00835] train_loss: 0.015056\n",
      "[089/00885] train_loss: 0.014469\n",
      "[089/00935] train_loss: 0.015343\n",
      "[089/00985] train_loss: 0.015364\n",
      "[089/01035] train_loss: 0.014756\n",
      "[089/01085] train_loss: 0.013875\n",
      "[089/01135] train_loss: 0.015280\n",
      "[089/01185] train_loss: 0.014929\n",
      "[090/00009] train_loss: 0.015195\n",
      "[090/00059] train_loss: 0.016499\n",
      "[090/00109] train_loss: 0.016049\n",
      "[090/00159] train_loss: 0.015295\n",
      "[090/00209] train_loss: 0.015357\n",
      "[090/00259] train_loss: 0.015270\n",
      "[090/00309] train_loss: 0.015591\n",
      "[090/00359] train_loss: 0.015389\n",
      "[090/00409] train_loss: 0.014730\n",
      "[090/00459] train_loss: 0.015503\n",
      "[090/00509] train_loss: 0.015554\n",
      "[090/00559] train_loss: 0.014605\n",
      "[090/00609] train_loss: 0.015396\n",
      "[090/00659] train_loss: 0.015063\n",
      "[090/00709] train_loss: 0.014440\n",
      "[090/00759] train_loss: 0.015399\n",
      "[090/00809] train_loss: 0.015456\n",
      "[090/00859] train_loss: 0.014434\n",
      "[090/00909] train_loss: 0.014435\n",
      "[090/00959] train_loss: 0.015328\n",
      "[090/01009] train_loss: 0.015313\n",
      "[090/01059] train_loss: 0.015806\n",
      "[090/01109] train_loss: 0.014616\n",
      "[090/01159] train_loss: 0.014451\n",
      "[090/01209] train_loss: 0.014489\n",
      "[091/00033] train_loss: 0.017010\n",
      "[091/00083] train_loss: 0.017158\n",
      "[091/00133] train_loss: 0.015970\n",
      "[091/00183] train_loss: 0.015618\n",
      "[091/00233] train_loss: 0.016327\n",
      "[091/00283] train_loss: 0.015605\n",
      "[091/00333] train_loss: 0.015426\n",
      "[091/00383] train_loss: 0.015698\n",
      "[091/00433] train_loss: 0.015080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[091/00483] train_loss: 0.015784\n",
      "[091/00533] train_loss: 0.014937\n",
      "[091/00583] train_loss: 0.015056\n",
      "[091/00633] train_loss: 0.014983\n",
      "[091/00683] train_loss: 0.014651\n",
      "[091/00733] train_loss: 0.015079\n",
      "[091/00783] train_loss: 0.015198\n",
      "[091/00833] train_loss: 0.014835\n",
      "[091/00883] train_loss: 0.014557\n",
      "[091/00933] train_loss: 0.015335\n",
      "[091/00983] train_loss: 0.014353\n",
      "[091/01033] train_loss: 0.014828\n",
      "[091/01083] train_loss: 0.014282\n",
      "[091/01133] train_loss: 0.014166\n",
      "[091/01183] train_loss: 0.015147\n",
      "[092/00007] train_loss: 0.015465\n",
      "[092/00057] train_loss: 0.016441\n",
      "[092/00107] train_loss: 0.015608\n",
      "[092/00157] train_loss: 0.016393\n",
      "[092/00207] train_loss: 0.015459\n",
      "[092/00257] train_loss: 0.014917\n",
      "[092/00307] train_loss: 0.015260\n",
      "[092/00357] train_loss: 0.015010\n",
      "[092/00407] train_loss: 0.015053\n",
      "[092/00457] train_loss: 0.015356\n",
      "[092/00507] train_loss: 0.014916\n",
      "[092/00557] train_loss: 0.015264\n",
      "[092/00607] train_loss: 0.015271\n",
      "[092/00657] train_loss: 0.014819\n",
      "[092/00707] train_loss: 0.015119\n",
      "[092/00757] train_loss: 0.014713\n",
      "[092/00807] train_loss: 0.014125\n",
      "[092/00857] train_loss: 0.014835\n",
      "[092/00907] train_loss: 0.015314\n",
      "[092/00957] train_loss: 0.014745\n",
      "[092/01007] train_loss: 0.014813\n",
      "[092/01057] train_loss: 0.014181\n",
      "[092/01107] train_loss: 0.015372\n",
      "[092/01157] train_loss: 0.014468\n",
      "[092/01207] train_loss: 0.015463\n",
      "[093/00031] train_loss: 0.016857\n",
      "[093/00081] train_loss: 0.016704\n",
      "[093/00131] train_loss: 0.015274\n",
      "[093/00181] train_loss: 0.015451\n",
      "[093/00231] train_loss: 0.015348\n",
      "[093/00281] train_loss: 0.016208\n",
      "[093/00331] train_loss: 0.014924\n",
      "[093/00381] train_loss: 0.015666\n",
      "[093/00431] train_loss: 0.015160\n",
      "[093/00481] train_loss: 0.014914\n",
      "[093/00531] train_loss: 0.015291\n",
      "[093/00581] train_loss: 0.015004\n",
      "[093/00631] train_loss: 0.015302\n",
      "[093/00681] train_loss: 0.014940\n",
      "[093/00731] train_loss: 0.014606\n",
      "[093/00781] train_loss: 0.014930\n",
      "[093/00831] train_loss: 0.014582\n",
      "[093/00881] train_loss: 0.014452\n",
      "[093/00931] train_loss: 0.014812\n",
      "[093/00981] train_loss: 0.013833\n",
      "[093/01031] train_loss: 0.014831\n",
      "[093/01081] train_loss: 0.014931\n",
      "[093/01131] train_loss: 0.013936\n",
      "[093/01181] train_loss: 0.015400\n",
      "[094/00005] train_loss: 0.015073\n",
      "[094/00055] train_loss: 0.016375\n",
      "[094/00105] train_loss: 0.016596\n",
      "[094/00155] train_loss: 0.015902\n",
      "[094/00205] train_loss: 0.016305\n",
      "[094/00255] train_loss: 0.014648\n",
      "[094/00305] train_loss: 0.015624\n",
      "[094/00355] train_loss: 0.015177\n",
      "[094/00405] train_loss: 0.014092\n",
      "[094/00455] train_loss: 0.015431\n",
      "[094/00505] train_loss: 0.014963\n",
      "[094/00555] train_loss: 0.014607\n",
      "[094/00605] train_loss: 0.015114\n",
      "[094/00655] train_loss: 0.015696\n",
      "[094/00705] train_loss: 0.014855\n",
      "[094/00755] train_loss: 0.014551\n",
      "[094/00805] train_loss: 0.014770\n",
      "[094/00855] train_loss: 0.014429\n",
      "[094/00905] train_loss: 0.014221\n",
      "[094/00955] train_loss: 0.014164\n",
      "[094/01005] train_loss: 0.014944\n",
      "[094/01055] train_loss: 0.015886\n",
      "[094/01105] train_loss: 0.014579\n",
      "[094/01155] train_loss: 0.015105\n",
      "[094/01205] train_loss: 0.014435\n",
      "[095/00029] train_loss: 0.016023\n",
      "[095/00079] train_loss: 0.015746\n",
      "[095/00129] train_loss: 0.015583\n",
      "[095/00179] train_loss: 0.016819\n",
      "[095/00229] train_loss: 0.015804\n",
      "[095/00279] train_loss: 0.016059\n",
      "[095/00329] train_loss: 0.014349\n",
      "[095/00379] train_loss: 0.015734\n",
      "[095/00429] train_loss: 0.015986\n",
      "[095/00479] train_loss: 0.014043\n",
      "[095/00529] train_loss: 0.015863\n",
      "[095/00579] train_loss: 0.014739\n",
      "[095/00629] train_loss: 0.015551\n",
      "[095/00679] train_loss: 0.014388\n",
      "[095/00729] train_loss: 0.014731\n",
      "[095/00779] train_loss: 0.013515\n",
      "[095/00829] train_loss: 0.014857\n",
      "[095/00879] train_loss: 0.014847\n",
      "[095/00929] train_loss: 0.014755\n",
      "[095/00979] train_loss: 0.014910\n",
      "[095/01029] train_loss: 0.014607\n",
      "[095/01079] train_loss: 0.014664\n",
      "[095/01129] train_loss: 0.015033\n",
      "[095/01179] train_loss: 0.014904\n",
      "[096/00003] train_loss: 0.014114\n",
      "[096/00053] train_loss: 0.016617\n",
      "[096/00103] train_loss: 0.016440\n",
      "[096/00153] train_loss: 0.015430\n",
      "[096/00203] train_loss: 0.015256\n",
      "[096/00253] train_loss: 0.015257\n",
      "[096/00303] train_loss: 0.015395\n",
      "[096/00353] train_loss: 0.014401\n",
      "[096/00403] train_loss: 0.014407\n",
      "[096/00453] train_loss: 0.014641\n",
      "[096/00503] train_loss: 0.015233\n",
      "[096/00553] train_loss: 0.014195\n",
      "[096/00603] train_loss: 0.014538\n",
      "[096/00653] train_loss: 0.015613\n",
      "[096/00703] train_loss: 0.014727\n",
      "[096/00753] train_loss: 0.015315\n",
      "[096/00803] train_loss: 0.014791\n",
      "[096/00853] train_loss: 0.013890\n",
      "[096/00903] train_loss: 0.015599\n",
      "[096/00953] train_loss: 0.014765\n",
      "[096/01003] train_loss: 0.014273\n",
      "[096/01053] train_loss: 0.014942\n",
      "[096/01103] train_loss: 0.014419\n",
      "[096/01153] train_loss: 0.015277\n",
      "[096/01203] train_loss: 0.014228\n",
      "[097/00027] train_loss: 0.015714\n",
      "[097/00077] train_loss: 0.017289\n",
      "[097/00127] train_loss: 0.016073\n",
      "[097/00177] train_loss: 0.015915\n",
      "[097/00227] train_loss: 0.016449\n",
      "[097/00277] train_loss: 0.015689\n",
      "[097/00327] train_loss: 0.014943\n",
      "[097/00377] train_loss: 0.015970\n",
      "[097/00427] train_loss: 0.015010\n",
      "[097/00477] train_loss: 0.015135\n",
      "[097/00527] train_loss: 0.014471\n",
      "[097/00577] train_loss: 0.014859\n",
      "[097/00627] train_loss: 0.014244\n",
      "[097/00677] train_loss: 0.015232\n",
      "[097/00727] train_loss: 0.015593\n",
      "[097/00777] train_loss: 0.015214\n",
      "[097/00827] train_loss: 0.015530\n",
      "[097/00877] train_loss: 0.014677\n",
      "[097/00927] train_loss: 0.014407\n",
      "[097/00977] train_loss: 0.014573\n",
      "[097/01027] train_loss: 0.014007\n",
      "[097/01077] train_loss: 0.014119\n",
      "[097/01127] train_loss: 0.015321\n",
      "[097/01177] train_loss: 0.014444\n",
      "[098/00001] train_loss: 0.015159\n",
      "[098/00051] train_loss: 0.015951\n",
      "[098/00101] train_loss: 0.014688\n",
      "[098/00151] train_loss: 0.015923\n",
      "[098/00201] train_loss: 0.015869\n",
      "[098/00251] train_loss: 0.014743\n",
      "[098/00301] train_loss: 0.015049\n",
      "[098/00351] train_loss: 0.013839\n",
      "[098/00401] train_loss: 0.014272\n",
      "[098/00451] train_loss: 0.014103\n",
      "[098/00501] train_loss: 0.014748\n",
      "[098/00551] train_loss: 0.014653\n",
      "[098/00601] train_loss: 0.014173\n",
      "[098/00651] train_loss: 0.015271\n",
      "[098/00701] train_loss: 0.014866\n",
      "[098/00751] train_loss: 0.015184\n",
      "[098/00801] train_loss: 0.014342\n",
      "[098/00851] train_loss: 0.014853\n",
      "[098/00901] train_loss: 0.014125\n",
      "[098/00951] train_loss: 0.014053\n",
      "[098/01001] train_loss: 0.014841\n",
      "[098/01051] train_loss: 0.014941\n",
      "[098/01101] train_loss: 0.014908\n",
      "[098/01151] train_loss: 0.014793\n",
      "[098/01201] train_loss: 0.014279\n",
      "[099/00025] train_loss: 0.015013\n",
      "[099/00075] train_loss: 0.015373\n",
      "[099/00125] train_loss: 0.017012\n",
      "[099/00175] train_loss: 0.015166\n",
      "[099/00225] train_loss: 0.015355\n",
      "[099/00275] train_loss: 0.015307\n",
      "[099/00325] train_loss: 0.015794\n",
      "[099/00375] train_loss: 0.015014\n",
      "[099/00425] train_loss: 0.015293\n",
      "[099/00475] train_loss: 0.015697\n",
      "[099/00525] train_loss: 0.015082\n",
      "[099/00575] train_loss: 0.014747\n",
      "[099/00625] train_loss: 0.014043\n",
      "[099/00675] train_loss: 0.015232\n",
      "[099/00725] train_loss: 0.013782\n",
      "[099/00775] train_loss: 0.015223\n",
      "[099/00825] train_loss: 0.015598\n",
      "[099/00875] train_loss: 0.015062\n",
      "[099/00925] train_loss: 0.014251\n",
      "[099/00975] train_loss: 0.016149\n",
      "[099/01025] train_loss: 0.014679\n",
      "[099/01075] train_loss: 0.014434\n",
      "[099/01125] train_loss: 0.014666\n",
      "[099/01175] train_loss: 0.014626\n",
      "[099/01225] train_loss: 0.014868\n",
      "[100/00049] train_loss: 0.016738\n",
      "[100/00099] train_loss: 0.015859\n",
      "[100/00149] train_loss: 0.015531\n",
      "[100/00199] train_loss: 0.016046\n",
      "[100/00249] train_loss: 0.015173\n",
      "[100/00299] train_loss: 0.015186\n",
      "[100/00349] train_loss: 0.014961\n",
      "[100/00399] train_loss: 0.014895\n",
      "[100/00449] train_loss: 0.014978\n",
      "[100/00499] train_loss: 0.014811\n",
      "[100/00549] train_loss: 0.014999\n",
      "[100/00599] train_loss: 0.014942\n",
      "[100/00649] train_loss: 0.015296\n",
      "[100/00699] train_loss: 0.013645\n",
      "[100/00749] train_loss: 0.014773\n",
      "[100/00799] train_loss: 0.014412\n",
      "[100/00849] train_loss: 0.015406\n",
      "[100/00899] train_loss: 0.015275\n",
      "[100/00949] train_loss: 0.015014\n",
      "[100/00999] train_loss: 0.014514\n",
      "[100/01049] train_loss: 0.015409\n",
      "[100/01099] train_loss: 0.014592\n",
      "[100/01149] train_loss: 0.014329\n",
      "[100/01199] train_loss: 0.015390\n",
      "[101/00023] train_loss: 0.015523\n",
      "[101/00073] train_loss: 0.015976\n",
      "[101/00123] train_loss: 0.015921\n",
      "[101/00173] train_loss: 0.015745\n",
      "[101/00223] train_loss: 0.015957\n",
      "[101/00273] train_loss: 0.015860\n",
      "[101/00323] train_loss: 0.015032\n",
      "[101/00373] train_loss: 0.014727\n",
      "[101/00423] train_loss: 0.014775\n",
      "[101/00473] train_loss: 0.015254\n",
      "[101/00523] train_loss: 0.014305\n",
      "[101/00573] train_loss: 0.014689\n",
      "[101/00623] train_loss: 0.014231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101/00673] train_loss: 0.014550\n",
      "[101/00723] train_loss: 0.014690\n",
      "[101/00773] train_loss: 0.015010\n",
      "[101/00823] train_loss: 0.015278\n",
      "[101/00873] train_loss: 0.014736\n",
      "[101/00923] train_loss: 0.014717\n",
      "[101/00973] train_loss: 0.014430\n",
      "[101/01023] train_loss: 0.014822\n",
      "[101/01073] train_loss: 0.014390\n",
      "[101/01123] train_loss: 0.013382\n",
      "[101/01173] train_loss: 0.013649\n",
      "[101/01223] train_loss: 0.014289\n",
      "[102/00047] train_loss: 0.016304\n",
      "[102/00097] train_loss: 0.016415\n",
      "[102/00147] train_loss: 0.016147\n",
      "[102/00197] train_loss: 0.015924\n",
      "[102/00247] train_loss: 0.014670\n",
      "[102/00297] train_loss: 0.016058\n",
      "[102/00347] train_loss: 0.015381\n",
      "[102/00397] train_loss: 0.015033\n",
      "[102/00447] train_loss: 0.015122\n",
      "[102/00497] train_loss: 0.015154\n",
      "[102/00547] train_loss: 0.014954\n",
      "[102/00597] train_loss: 0.014803\n",
      "[102/00647] train_loss: 0.015017\n",
      "[102/00697] train_loss: 0.014556\n",
      "[102/00747] train_loss: 0.014886\n",
      "[102/00797] train_loss: 0.014379\n",
      "[102/00847] train_loss: 0.014481\n",
      "[102/00897] train_loss: 0.015728\n",
      "[102/00947] train_loss: 0.014939\n",
      "[102/00997] train_loss: 0.014931\n",
      "[102/01047] train_loss: 0.014570\n",
      "[102/01097] train_loss: 0.014403\n",
      "[102/01147] train_loss: 0.014647\n",
      "[102/01197] train_loss: 0.014720\n",
      "[103/00021] train_loss: 0.015815\n",
      "[103/00071] train_loss: 0.015666\n",
      "[103/00121] train_loss: 0.015908\n",
      "[103/00171] train_loss: 0.016597\n",
      "[103/00221] train_loss: 0.015082\n",
      "[103/00271] train_loss: 0.015275\n",
      "[103/00321] train_loss: 0.014441\n",
      "[103/00371] train_loss: 0.014607\n",
      "[103/00421] train_loss: 0.014759\n",
      "[103/00471] train_loss: 0.015295\n",
      "[103/00521] train_loss: 0.015229\n",
      "[103/00571] train_loss: 0.014331\n",
      "[103/00621] train_loss: 0.015226\n",
      "[103/00671] train_loss: 0.014931\n",
      "[103/00721] train_loss: 0.014983\n",
      "[103/00771] train_loss: 0.014593\n",
      "[103/00821] train_loss: 0.014890\n",
      "[103/00871] train_loss: 0.014752\n",
      "[103/00921] train_loss: 0.014004\n",
      "[103/00971] train_loss: 0.013993\n",
      "[103/01021] train_loss: 0.014603\n",
      "[103/01071] train_loss: 0.014131\n",
      "[103/01121] train_loss: 0.014187\n",
      "[103/01171] train_loss: 0.015610\n",
      "[103/01221] train_loss: 0.015152\n",
      "[104/00045] train_loss: 0.016286\n",
      "[104/00095] train_loss: 0.015325\n",
      "[104/00145] train_loss: 0.015446\n",
      "[104/00195] train_loss: 0.014166\n",
      "[104/00245] train_loss: 0.014565\n",
      "[104/00295] train_loss: 0.016408\n",
      "[104/00345] train_loss: 0.015225\n",
      "[104/00395] train_loss: 0.015429\n",
      "[104/00445] train_loss: 0.014769\n",
      "[104/00495] train_loss: 0.014759\n",
      "[104/00545] train_loss: 0.015493\n",
      "[104/00595] train_loss: 0.014516\n",
      "[104/00645] train_loss: 0.015031\n",
      "[104/00695] train_loss: 0.015007\n",
      "[104/00745] train_loss: 0.014353\n",
      "[104/00795] train_loss: 0.014791\n",
      "[104/00845] train_loss: 0.014392\n",
      "[104/00895] train_loss: 0.014783\n",
      "[104/00945] train_loss: 0.014729\n",
      "[104/00995] train_loss: 0.014122\n",
      "[104/01045] train_loss: 0.014243\n",
      "[104/01095] train_loss: 0.014224\n",
      "[104/01145] train_loss: 0.015133\n",
      "[104/01195] train_loss: 0.014913\n",
      "[105/00019] train_loss: 0.014792\n",
      "[105/00069] train_loss: 0.016209\n",
      "[105/00119] train_loss: 0.015243\n",
      "[105/00169] train_loss: 0.016129\n",
      "[105/00219] train_loss: 0.015729\n",
      "[105/00269] train_loss: 0.015016\n",
      "[105/00319] train_loss: 0.014238\n",
      "[105/00369] train_loss: 0.015103\n",
      "[105/00419] train_loss: 0.015072\n",
      "[105/00469] train_loss: 0.014536\n",
      "[105/00519] train_loss: 0.015123\n",
      "[105/00569] train_loss: 0.014702\n",
      "[105/00619] train_loss: 0.015010\n",
      "[105/00669] train_loss: 0.014245\n",
      "[105/00719] train_loss: 0.014702\n",
      "[105/00769] train_loss: 0.014562\n",
      "[105/00819] train_loss: 0.014744\n",
      "[105/00869] train_loss: 0.014078\n",
      "[105/00919] train_loss: 0.015222\n",
      "[105/00969] train_loss: 0.014778\n",
      "[105/01019] train_loss: 0.015204\n",
      "[105/01069] train_loss: 0.014137\n",
      "[105/01119] train_loss: 0.014888\n",
      "[105/01169] train_loss: 0.014194\n",
      "[105/01219] train_loss: 0.014265\n",
      "[106/00043] train_loss: 0.016327\n",
      "[106/00093] train_loss: 0.015636\n",
      "[106/00143] train_loss: 0.015843\n",
      "[106/00193] train_loss: 0.015282\n",
      "[106/00243] train_loss: 0.014905\n",
      "[106/00293] train_loss: 0.014196\n",
      "[106/00343] train_loss: 0.015527\n",
      "[106/00393] train_loss: 0.014735\n",
      "[106/00443] train_loss: 0.015591\n",
      "[106/00493] train_loss: 0.014978\n",
      "[106/00543] train_loss: 0.014181\n",
      "[106/00593] train_loss: 0.014597\n",
      "[106/00643] train_loss: 0.015147\n",
      "[106/00693] train_loss: 0.015583\n",
      "[106/00743] train_loss: 0.014630\n",
      "[106/00793] train_loss: 0.014792\n",
      "[106/00843] train_loss: 0.013982\n",
      "[106/00893] train_loss: 0.014521\n",
      "[106/00943] train_loss: 0.013919\n",
      "[106/00993] train_loss: 0.015180\n",
      "[106/01043] train_loss: 0.014833\n",
      "[106/01093] train_loss: 0.013764\n",
      "[106/01143] train_loss: 0.014407\n",
      "[106/01193] train_loss: 0.014147\n",
      "[107/00017] train_loss: 0.015565\n",
      "[107/00067] train_loss: 0.016214\n",
      "[107/00117] train_loss: 0.016479\n",
      "[107/00167] train_loss: 0.015717\n",
      "[107/00217] train_loss: 0.014534\n",
      "[107/00267] train_loss: 0.014744\n",
      "[107/00317] train_loss: 0.015088\n",
      "[107/00367] train_loss: 0.014643\n",
      "[107/00417] train_loss: 0.014659\n",
      "[107/00467] train_loss: 0.014059\n",
      "[107/00517] train_loss: 0.014851\n",
      "[107/00567] train_loss: 0.014360\n",
      "[107/00617] train_loss: 0.015077\n",
      "[107/00667] train_loss: 0.014707\n",
      "[107/00717] train_loss: 0.014308\n",
      "[107/00767] train_loss: 0.014047\n",
      "[107/00817] train_loss: 0.015114\n",
      "[107/00867] train_loss: 0.014427\n",
      "[107/00917] train_loss: 0.014587\n",
      "[107/00967] train_loss: 0.014755\n",
      "[107/01017] train_loss: 0.015022\n",
      "[107/01067] train_loss: 0.014456\n",
      "[107/01117] train_loss: 0.015797\n",
      "[107/01167] train_loss: 0.014670\n",
      "[107/01217] train_loss: 0.013648\n",
      "[108/00041] train_loss: 0.015023\n",
      "[108/00091] train_loss: 0.015962\n",
      "[108/00141] train_loss: 0.015287\n",
      "[108/00191] train_loss: 0.014962\n",
      "[108/00241] train_loss: 0.014629\n",
      "[108/00291] train_loss: 0.014942\n",
      "[108/00341] train_loss: 0.016087\n",
      "[108/00391] train_loss: 0.015499\n",
      "[108/00441] train_loss: 0.015134\n",
      "[108/00491] train_loss: 0.015117\n",
      "[108/00541] train_loss: 0.014518\n",
      "[108/00591] train_loss: 0.014970\n",
      "[108/00641] train_loss: 0.015390\n",
      "[108/00691] train_loss: 0.014313\n",
      "[108/00741] train_loss: 0.014465\n",
      "[108/00791] train_loss: 0.014928\n",
      "[108/00841] train_loss: 0.015069\n",
      "[108/00891] train_loss: 0.013949\n",
      "[108/00941] train_loss: 0.014151\n",
      "[108/00991] train_loss: 0.014396\n",
      "[108/01041] train_loss: 0.015028\n",
      "[108/01091] train_loss: 0.015351\n",
      "[108/01141] train_loss: 0.014320\n",
      "[108/01191] train_loss: 0.014711\n",
      "[109/00015] train_loss: 0.015798\n",
      "[109/00065] train_loss: 0.015910\n",
      "[109/00115] train_loss: 0.015872\n",
      "[109/00165] train_loss: 0.015548\n",
      "[109/00215] train_loss: 0.015039\n",
      "[109/00265] train_loss: 0.014978\n",
      "[109/00315] train_loss: 0.015086\n",
      "[109/00365] train_loss: 0.014720\n",
      "[109/00415] train_loss: 0.015530\n",
      "[109/00465] train_loss: 0.014420\n",
      "[109/00515] train_loss: 0.014259\n",
      "[109/00565] train_loss: 0.014041\n",
      "[109/00615] train_loss: 0.016066\n",
      "[109/00665] train_loss: 0.014357\n",
      "[109/00715] train_loss: 0.014540\n",
      "[109/00765] train_loss: 0.013807\n",
      "[109/00815] train_loss: 0.014353\n",
      "[109/00865] train_loss: 0.014406\n",
      "[109/00915] train_loss: 0.015234\n",
      "[109/00965] train_loss: 0.014958\n",
      "[109/01015] train_loss: 0.014182\n",
      "[109/01065] train_loss: 0.014738\n",
      "[109/01115] train_loss: 0.013715\n",
      "[109/01165] train_loss: 0.014359\n",
      "[109/01215] train_loss: 0.013996\n",
      "[110/00039] train_loss: 0.015915\n",
      "[110/00089] train_loss: 0.016031\n",
      "[110/00139] train_loss: 0.016879\n",
      "[110/00189] train_loss: 0.015793\n",
      "[110/00239] train_loss: 0.015282\n",
      "[110/00289] train_loss: 0.015796\n",
      "[110/00339] train_loss: 0.014896\n",
      "[110/00389] train_loss: 0.014922\n",
      "[110/00439] train_loss: 0.015106\n",
      "[110/00489] train_loss: 0.014013\n",
      "[110/00539] train_loss: 0.015022\n",
      "[110/00589] train_loss: 0.014514\n",
      "[110/00639] train_loss: 0.014337\n",
      "[110/00689] train_loss: 0.014723\n",
      "[110/00739] train_loss: 0.014917\n",
      "[110/00789] train_loss: 0.013925\n",
      "[110/00839] train_loss: 0.014382\n",
      "[110/00889] train_loss: 0.014409\n",
      "[110/00939] train_loss: 0.014582\n",
      "[110/00989] train_loss: 0.014797\n",
      "[110/01039] train_loss: 0.014000\n",
      "[110/01089] train_loss: 0.014515\n",
      "[110/01139] train_loss: 0.014161\n",
      "[110/01189] train_loss: 0.014346\n",
      "[111/00013] train_loss: 0.014562\n",
      "[111/00063] train_loss: 0.015614\n",
      "[111/00113] train_loss: 0.015681\n",
      "[111/00163] train_loss: 0.015497\n",
      "[111/00213] train_loss: 0.015742\n",
      "[111/00263] train_loss: 0.014854\n",
      "[111/00313] train_loss: 0.014760\n",
      "[111/00363] train_loss: 0.014796\n",
      "[111/00413] train_loss: 0.014687\n",
      "[111/00463] train_loss: 0.014196\n",
      "[111/00513] train_loss: 0.015264\n",
      "[111/00563] train_loss: 0.015037\n",
      "[111/00613] train_loss: 0.014022\n",
      "[111/00663] train_loss: 0.014992\n",
      "[111/00713] train_loss: 0.015052\n",
      "[111/00763] train_loss: 0.014030\n",
      "[111/00813] train_loss: 0.015274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[111/00863] train_loss: 0.014447\n",
      "[111/00913] train_loss: 0.014268\n",
      "[111/00963] train_loss: 0.014376\n",
      "[111/01013] train_loss: 0.014474\n",
      "[111/01063] train_loss: 0.014079\n",
      "[111/01113] train_loss: 0.014536\n",
      "[111/01163] train_loss: 0.013866\n",
      "[111/01213] train_loss: 0.014442\n",
      "[112/00037] train_loss: 0.015465\n",
      "[112/00087] train_loss: 0.016020\n",
      "[112/00137] train_loss: 0.016325\n",
      "[112/00187] train_loss: 0.015150\n",
      "[112/00237] train_loss: 0.015054\n",
      "[112/00287] train_loss: 0.014662\n",
      "[112/00337] train_loss: 0.015399\n",
      "[112/00387] train_loss: 0.014764\n",
      "[112/00437] train_loss: 0.014568\n",
      "[112/00487] train_loss: 0.015560\n",
      "[112/00537] train_loss: 0.014049\n",
      "[112/00587] train_loss: 0.015400\n",
      "[112/00637] train_loss: 0.015639\n",
      "[112/00687] train_loss: 0.013662\n",
      "[112/00737] train_loss: 0.015163\n",
      "[112/00787] train_loss: 0.015017\n",
      "[112/00837] train_loss: 0.014545\n",
      "[112/00887] train_loss: 0.015222\n",
      "[112/00937] train_loss: 0.013857\n",
      "[112/00987] train_loss: 0.014074\n",
      "[112/01037] train_loss: 0.014522\n",
      "[112/01087] train_loss: 0.014672\n",
      "[112/01137] train_loss: 0.014094\n",
      "[112/01187] train_loss: 0.013866\n",
      "[113/00011] train_loss: 0.014647\n",
      "[113/00061] train_loss: 0.015086\n",
      "[113/00111] train_loss: 0.016133\n",
      "[113/00161] train_loss: 0.015599\n",
      "[113/00211] train_loss: 0.015209\n",
      "[113/00261] train_loss: 0.015605\n",
      "[113/00311] train_loss: 0.014728\n",
      "[113/00361] train_loss: 0.014756\n",
      "[113/00411] train_loss: 0.014230\n",
      "[113/00461] train_loss: 0.014987\n",
      "[113/00511] train_loss: 0.014374\n",
      "[113/00561] train_loss: 0.015204\n",
      "[113/00611] train_loss: 0.013543\n",
      "[113/00661] train_loss: 0.014546\n",
      "[113/00711] train_loss: 0.014653\n",
      "[113/00761] train_loss: 0.014113\n",
      "[113/00811] train_loss: 0.014030\n",
      "[113/00861] train_loss: 0.014391\n",
      "[113/00911] train_loss: 0.014406\n",
      "[113/00961] train_loss: 0.014551\n",
      "[113/01011] train_loss: 0.014523\n",
      "[113/01061] train_loss: 0.014454\n",
      "[113/01111] train_loss: 0.015181\n",
      "[113/01161] train_loss: 0.014262\n",
      "[113/01211] train_loss: 0.013760\n",
      "[114/00035] train_loss: 0.015637\n",
      "[114/00085] train_loss: 0.014327\n",
      "[114/00135] train_loss: 0.015592\n",
      "[114/00185] train_loss: 0.015708\n",
      "[114/00235] train_loss: 0.015556\n",
      "[114/00285] train_loss: 0.014619\n",
      "[114/00335] train_loss: 0.015102\n",
      "[114/00385] train_loss: 0.014274\n",
      "[114/00435] train_loss: 0.014945\n",
      "[114/00485] train_loss: 0.013264\n",
      "[114/00535] train_loss: 0.014589\n",
      "[114/00585] train_loss: 0.014507\n",
      "[114/00635] train_loss: 0.015133\n",
      "[114/00685] train_loss: 0.015419\n",
      "[114/00735] train_loss: 0.015187\n",
      "[114/00785] train_loss: 0.014368\n",
      "[114/00835] train_loss: 0.015260\n",
      "[114/00885] train_loss: 0.013929\n",
      "[114/00935] train_loss: 0.013973\n",
      "[114/00985] train_loss: 0.014983\n",
      "[114/01035] train_loss: 0.014269\n",
      "[114/01085] train_loss: 0.014797\n",
      "[114/01135] train_loss: 0.013900\n",
      "[114/01185] train_loss: 0.014108\n",
      "[115/00009] train_loss: 0.014183\n",
      "[115/00059] train_loss: 0.016113\n",
      "[115/00109] train_loss: 0.015186\n",
      "[115/00159] train_loss: 0.015575\n",
      "[115/00209] train_loss: 0.014944\n",
      "[115/00259] train_loss: 0.015371\n",
      "[115/00309] train_loss: 0.014807\n",
      "[115/00359] train_loss: 0.015164\n",
      "[115/00409] train_loss: 0.014304\n",
      "[115/00459] train_loss: 0.014964\n",
      "[115/00509] train_loss: 0.013869\n",
      "[115/00559] train_loss: 0.014130\n",
      "[115/00609] train_loss: 0.014932\n",
      "[115/00659] train_loss: 0.014428\n",
      "[115/00709] train_loss: 0.014491\n",
      "[115/00759] train_loss: 0.014085\n",
      "[115/00809] train_loss: 0.014040\n",
      "[115/00859] train_loss: 0.014121\n",
      "[115/00909] train_loss: 0.014220\n",
      "[115/00959] train_loss: 0.014038\n",
      "[115/01009] train_loss: 0.014528\n",
      "[115/01059] train_loss: 0.014167\n",
      "[115/01109] train_loss: 0.014121\n",
      "[115/01159] train_loss: 0.014006\n",
      "[115/01209] train_loss: 0.014385\n",
      "[116/00033] train_loss: 0.014493\n",
      "[116/00083] train_loss: 0.015324\n",
      "[116/00133] train_loss: 0.015823\n",
      "[116/00183] train_loss: 0.015711\n",
      "[116/00233] train_loss: 0.016231\n",
      "[116/00283] train_loss: 0.015520\n",
      "[116/00333] train_loss: 0.015299\n",
      "[116/00383] train_loss: 0.014075\n",
      "[116/00433] train_loss: 0.014657\n",
      "[116/00483] train_loss: 0.014813\n",
      "[116/00533] train_loss: 0.014517\n",
      "[116/00583] train_loss: 0.013733\n",
      "[116/00633] train_loss: 0.015385\n",
      "[116/00683] train_loss: 0.013734\n",
      "[116/00733] train_loss: 0.015108\n",
      "[116/00783] train_loss: 0.015223\n",
      "[116/00833] train_loss: 0.014676\n",
      "[116/00883] train_loss: 0.014111\n",
      "[116/00933] train_loss: 0.014321\n",
      "[116/00983] train_loss: 0.014726\n",
      "[116/01033] train_loss: 0.014197\n",
      "[116/01083] train_loss: 0.014816\n",
      "[116/01133] train_loss: 0.013726\n",
      "[116/01183] train_loss: 0.014001\n",
      "[117/00007] train_loss: 0.013760\n",
      "[117/00057] train_loss: 0.016255\n",
      "[117/00107] train_loss: 0.014988\n",
      "[117/00157] train_loss: 0.015268\n",
      "[117/00207] train_loss: 0.015380\n",
      "[117/00257] train_loss: 0.014217\n",
      "[117/00307] train_loss: 0.014722\n",
      "[117/00357] train_loss: 0.015238\n",
      "[117/00407] train_loss: 0.014841\n",
      "[117/00457] train_loss: 0.013647\n",
      "[117/00507] train_loss: 0.014569\n",
      "[117/00557] train_loss: 0.014830\n",
      "[117/00607] train_loss: 0.015149\n",
      "[117/00657] train_loss: 0.014898\n",
      "[117/00707] train_loss: 0.013979\n",
      "[117/00757] train_loss: 0.013948\n",
      "[117/00807] train_loss: 0.014466\n",
      "[117/00857] train_loss: 0.013759\n",
      "[117/00907] train_loss: 0.013883\n",
      "[117/00957] train_loss: 0.014195\n",
      "[117/01007] train_loss: 0.014777\n",
      "[117/01057] train_loss: 0.014570\n",
      "[117/01107] train_loss: 0.014750\n",
      "[117/01157] train_loss: 0.014594\n",
      "[117/01207] train_loss: 0.014463\n",
      "[118/00031] train_loss: 0.014814\n",
      "[118/00081] train_loss: 0.015730\n",
      "[118/00131] train_loss: 0.015692\n",
      "[118/00181] train_loss: 0.016165\n",
      "[118/00231] train_loss: 0.015631\n",
      "[118/00281] train_loss: 0.014698\n",
      "[118/00331] train_loss: 0.014942\n",
      "[118/00381] train_loss: 0.014700\n",
      "[118/00431] train_loss: 0.014449\n",
      "[118/00481] train_loss: 0.014786\n",
      "[118/00531] train_loss: 0.014819\n",
      "[118/00581] train_loss: 0.014225\n",
      "[118/00631] train_loss: 0.014501\n",
      "[118/00681] train_loss: 0.014267\n",
      "[118/00731] train_loss: 0.014696\n",
      "[118/00781] train_loss: 0.014933\n",
      "[118/00831] train_loss: 0.014003\n",
      "[118/00881] train_loss: 0.014036\n",
      "[118/00931] train_loss: 0.014295\n",
      "[118/00981] train_loss: 0.013756\n",
      "[118/01031] train_loss: 0.014192\n",
      "[118/01081] train_loss: 0.014063\n",
      "[118/01131] train_loss: 0.013549\n",
      "[118/01181] train_loss: 0.014734\n",
      "[119/00005] train_loss: 0.014683\n",
      "[119/00055] train_loss: 0.015262\n",
      "[119/00105] train_loss: 0.016132\n",
      "[119/00155] train_loss: 0.015945\n",
      "[119/00205] train_loss: 0.015006\n",
      "[119/00255] train_loss: 0.015668\n",
      "[119/00305] train_loss: 0.014140\n",
      "[119/00355] train_loss: 0.014762\n",
      "[119/00405] train_loss: 0.015159\n",
      "[119/00455] train_loss: 0.014110\n",
      "[119/00505] train_loss: 0.013747\n",
      "[119/00555] train_loss: 0.014442\n",
      "[119/00605] train_loss: 0.013812\n",
      "[119/00655] train_loss: 0.013763\n",
      "[119/00705] train_loss: 0.013936\n",
      "[119/00755] train_loss: 0.014516\n",
      "[119/00805] train_loss: 0.014643\n",
      "[119/00855] train_loss: 0.014675\n",
      "[119/00905] train_loss: 0.014373\n",
      "[119/00955] train_loss: 0.013664\n",
      "[119/01005] train_loss: 0.014391\n",
      "[119/01055] train_loss: 0.014515\n",
      "[119/01105] train_loss: 0.014061\n",
      "[119/01155] train_loss: 0.014626\n",
      "[119/01205] train_loss: 0.013801\n",
      "[120/00029] train_loss: 0.015375\n",
      "[120/00079] train_loss: 0.015706\n",
      "[120/00129] train_loss: 0.015599\n",
      "[120/00179] train_loss: 0.015150\n",
      "[120/00229] train_loss: 0.014138\n",
      "[120/00279] train_loss: 0.014888\n",
      "[120/00329] train_loss: 0.015163\n",
      "[120/00379] train_loss: 0.014279\n",
      "[120/00429] train_loss: 0.014062\n",
      "[120/00479] train_loss: 0.014406\n",
      "[120/00529] train_loss: 0.013795\n",
      "[120/00579] train_loss: 0.014565\n",
      "[120/00629] train_loss: 0.014335\n",
      "[120/00679] train_loss: 0.014238\n",
      "[120/00729] train_loss: 0.014290\n",
      "[120/00779] train_loss: 0.014079\n",
      "[120/00829] train_loss: 0.014351\n",
      "[120/00879] train_loss: 0.014091\n",
      "[120/00929] train_loss: 0.014455\n",
      "[120/00979] train_loss: 0.013712\n",
      "[120/01029] train_loss: 0.013935\n",
      "[120/01079] train_loss: 0.013905\n",
      "[120/01129] train_loss: 0.014478\n",
      "[120/01179] train_loss: 0.013810\n",
      "[121/00003] train_loss: 0.014429\n",
      "[121/00053] train_loss: 0.016098\n",
      "[121/00103] train_loss: 0.015560\n",
      "[121/00153] train_loss: 0.014837\n",
      "[121/00203] train_loss: 0.014916\n",
      "[121/00253] train_loss: 0.014720\n",
      "[121/00303] train_loss: 0.014806\n",
      "[121/00353] train_loss: 0.013910\n",
      "[121/00403] train_loss: 0.014640\n",
      "[121/00453] train_loss: 0.014468\n",
      "[121/00503] train_loss: 0.014786\n",
      "[121/00553] train_loss: 0.016298\n",
      "[121/00603] train_loss: 0.014371\n",
      "[121/00653] train_loss: 0.014077\n",
      "[121/00703] train_loss: 0.014442\n",
      "[121/00753] train_loss: 0.014697\n",
      "[121/00803] train_loss: 0.014389\n",
      "[121/00853] train_loss: 0.014314\n",
      "[121/00903] train_loss: 0.014002\n",
      "[121/00953] train_loss: 0.014003\n",
      "[121/01003] train_loss: 0.013500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[121/01053] train_loss: 0.014702\n",
      "[121/01103] train_loss: 0.013877\n",
      "[121/01153] train_loss: 0.014903\n",
      "[121/01203] train_loss: 0.013279\n",
      "[122/00027] train_loss: 0.015053\n",
      "[122/00077] train_loss: 0.015565\n",
      "[122/00127] train_loss: 0.015615\n",
      "[122/00177] train_loss: 0.015526\n",
      "[122/00227] train_loss: 0.015182\n",
      "[122/00277] train_loss: 0.015100\n",
      "[122/00327] train_loss: 0.014471\n",
      "[122/00377] train_loss: 0.014921\n",
      "[122/00427] train_loss: 0.014489\n",
      "[122/00477] train_loss: 0.014148\n",
      "[122/00527] train_loss: 0.014880\n",
      "[122/00577] train_loss: 0.013452\n",
      "[122/00627] train_loss: 0.015204\n",
      "[122/00677] train_loss: 0.014481\n",
      "[122/00727] train_loss: 0.014485\n",
      "[122/00777] train_loss: 0.015141\n",
      "[122/00827] train_loss: 0.014786\n",
      "[122/00877] train_loss: 0.013796\n",
      "[122/00927] train_loss: 0.014379\n",
      "[122/00977] train_loss: 0.014338\n",
      "[122/01027] train_loss: 0.014618\n",
      "[122/01077] train_loss: 0.013836\n",
      "[122/01127] train_loss: 0.013605\n",
      "[122/01177] train_loss: 0.014181\n",
      "[123/00001] train_loss: 0.014097\n",
      "[123/00051] train_loss: 0.016001\n",
      "[123/00101] train_loss: 0.015015\n",
      "[123/00151] train_loss: 0.015030\n",
      "[123/00201] train_loss: 0.014650\n",
      "[123/00251] train_loss: 0.014632\n",
      "[123/00301] train_loss: 0.014937\n",
      "[123/00351] train_loss: 0.014306\n",
      "[123/00401] train_loss: 0.015229\n",
      "[123/00451] train_loss: 0.013958\n",
      "[123/00501] train_loss: 0.013789\n",
      "[123/00551] train_loss: 0.015140\n",
      "[123/00601] train_loss: 0.015154\n",
      "[123/00651] train_loss: 0.013387\n",
      "[123/00701] train_loss: 0.013327\n",
      "[123/00751] train_loss: 0.014192\n",
      "[123/00801] train_loss: 0.014070\n",
      "[123/00851] train_loss: 0.013744\n",
      "[123/00901] train_loss: 0.014068\n",
      "[123/00951] train_loss: 0.014586\n",
      "[123/01001] train_loss: 0.014364\n",
      "[123/01051] train_loss: 0.014962\n",
      "[123/01101] train_loss: 0.014301\n",
      "[123/01151] train_loss: 0.013860\n",
      "[123/01201] train_loss: 0.014559\n",
      "[124/00025] train_loss: 0.014893\n",
      "[124/00075] train_loss: 0.015624\n",
      "[124/00125] train_loss: 0.016077\n",
      "[124/00175] train_loss: 0.015088\n",
      "[124/00225] train_loss: 0.014636\n",
      "[124/00275] train_loss: 0.014216\n",
      "[124/00325] train_loss: 0.014536\n",
      "[124/00375] train_loss: 0.013893\n",
      "[124/00425] train_loss: 0.014858\n",
      "[124/00475] train_loss: 0.014908\n",
      "[124/00525] train_loss: 0.014399\n",
      "[124/00575] train_loss: 0.013651\n",
      "[124/00625] train_loss: 0.014751\n",
      "[124/00675] train_loss: 0.014596\n",
      "[124/00725] train_loss: 0.014574\n",
      "[124/00775] train_loss: 0.014801\n",
      "[124/00825] train_loss: 0.014642\n",
      "[124/00875] train_loss: 0.014977\n",
      "[124/00925] train_loss: 0.013970\n",
      "[124/00975] train_loss: 0.014495\n",
      "[124/01025] train_loss: 0.014884\n",
      "[124/01075] train_loss: 0.014200\n",
      "[124/01125] train_loss: 0.013892\n",
      "[124/01175] train_loss: 0.013949\n",
      "[124/01225] train_loss: 0.014949\n",
      "[125/00049] train_loss: 0.015890\n",
      "[125/00099] train_loss: 0.014515\n",
      "[125/00149] train_loss: 0.015363\n",
      "[125/00199] train_loss: 0.015016\n",
      "[125/00249] train_loss: 0.014977\n",
      "[125/00299] train_loss: 0.015429\n",
      "[125/00349] train_loss: 0.014522\n",
      "[125/00399] train_loss: 0.014317\n",
      "[125/00449] train_loss: 0.013831\n",
      "[125/00499] train_loss: 0.014401\n",
      "[125/00549] train_loss: 0.014045\n",
      "[125/00599] train_loss: 0.014036\n",
      "[125/00649] train_loss: 0.013541\n",
      "[125/00699] train_loss: 0.013497\n",
      "[125/00749] train_loss: 0.014687\n",
      "[125/00799] train_loss: 0.014573\n",
      "[125/00849] train_loss: 0.014923\n",
      "[125/00899] train_loss: 0.013977\n",
      "[125/00949] train_loss: 0.014052\n",
      "[125/00999] train_loss: 0.014515\n",
      "[125/01049] train_loss: 0.014326\n",
      "[125/01099] train_loss: 0.013858\n",
      "[125/01149] train_loss: 0.013780\n",
      "[125/01199] train_loss: 0.013318\n",
      "[126/00023] train_loss: 0.015100\n",
      "[126/00073] train_loss: 0.015384\n",
      "[126/00123] train_loss: 0.015353\n",
      "[126/00173] train_loss: 0.014574\n",
      "[126/00223] train_loss: 0.016171\n",
      "[126/00273] train_loss: 0.015647\n",
      "[126/00323] train_loss: 0.015132\n",
      "[126/00373] train_loss: 0.014458\n",
      "[126/00423] train_loss: 0.014621\n",
      "[126/00473] train_loss: 0.015570\n",
      "[126/00523] train_loss: 0.014370\n",
      "[126/00573] train_loss: 0.014437\n",
      "[126/00623] train_loss: 0.013830\n",
      "[126/00673] train_loss: 0.013886\n",
      "[126/00723] train_loss: 0.014511\n",
      "[126/00773] train_loss: 0.013849\n",
      "[126/00823] train_loss: 0.013839\n",
      "[126/00873] train_loss: 0.014794\n",
      "[126/00923] train_loss: 0.013561\n",
      "[126/00973] train_loss: 0.014107\n",
      "[126/01023] train_loss: 0.014335\n",
      "[126/01073] train_loss: 0.014150\n",
      "[126/01123] train_loss: 0.013655\n",
      "[126/01173] train_loss: 0.014148\n",
      "[126/01223] train_loss: 0.013934\n",
      "[127/00047] train_loss: 0.015179\n",
      "[127/00097] train_loss: 0.014804\n",
      "[127/00147] train_loss: 0.016178\n",
      "[127/00197] train_loss: 0.014622\n",
      "[127/00247] train_loss: 0.014249\n",
      "[127/00297] train_loss: 0.014382\n",
      "[127/00347] train_loss: 0.014411\n",
      "[127/00397] train_loss: 0.014093\n",
      "[127/00447] train_loss: 0.014592\n",
      "[127/00497] train_loss: 0.014851\n",
      "[127/00547] train_loss: 0.013301\n",
      "[127/00597] train_loss: 0.014223\n",
      "[127/00647] train_loss: 0.014310\n",
      "[127/00697] train_loss: 0.014560\n",
      "[127/00747] train_loss: 0.013887\n",
      "[127/00797] train_loss: 0.014430\n",
      "[127/00847] train_loss: 0.014347\n",
      "[127/00897] train_loss: 0.014157\n",
      "[127/00947] train_loss: 0.014321\n",
      "[127/00997] train_loss: 0.014463\n",
      "[127/01047] train_loss: 0.014188\n",
      "[127/01097] train_loss: 0.014817\n",
      "[127/01147] train_loss: 0.014318\n",
      "[127/01197] train_loss: 0.013702\n",
      "[128/00021] train_loss: 0.014826\n",
      "[128/00071] train_loss: 0.015775\n",
      "[128/00121] train_loss: 0.015632\n",
      "[128/00171] train_loss: 0.014754\n",
      "[128/00221] train_loss: 0.014971\n",
      "[128/00271] train_loss: 0.014484\n",
      "[128/00321] train_loss: 0.014117\n",
      "[128/00371] train_loss: 0.014965\n",
      "[128/00421] train_loss: 0.013732\n",
      "[128/00471] train_loss: 0.014313\n",
      "[128/00521] train_loss: 0.013718\n",
      "[128/00571] train_loss: 0.014503\n",
      "[128/00621] train_loss: 0.013752\n",
      "[128/00671] train_loss: 0.014088\n",
      "[128/00721] train_loss: 0.013802\n",
      "[128/00771] train_loss: 0.015483\n",
      "[128/00821] train_loss: 0.014190\n",
      "[128/00871] train_loss: 0.014658\n",
      "[128/00921] train_loss: 0.014078\n",
      "[128/00971] train_loss: 0.014564\n",
      "[128/01021] train_loss: 0.014503\n",
      "[128/01071] train_loss: 0.014378\n",
      "[128/01121] train_loss: 0.015009\n",
      "[128/01171] train_loss: 0.014058\n",
      "[128/01221] train_loss: 0.014624\n",
      "[129/00045] train_loss: 0.014982\n",
      "[129/00095] train_loss: 0.015320\n",
      "[129/00145] train_loss: 0.014706\n",
      "[129/00195] train_loss: 0.014604\n",
      "[129/00245] train_loss: 0.014300\n",
      "[129/00295] train_loss: 0.013991\n",
      "[129/00345] train_loss: 0.013508\n",
      "[129/00395] train_loss: 0.014467\n",
      "[129/00445] train_loss: 0.014831\n",
      "[129/00495] train_loss: 0.014408\n",
      "[129/00545] train_loss: 0.014866\n",
      "[129/00595] train_loss: 0.014006\n",
      "[129/00645] train_loss: 0.014384\n",
      "[129/00695] train_loss: 0.014470\n",
      "[129/00745] train_loss: 0.014495\n",
      "[129/00795] train_loss: 0.014062\n",
      "[129/00845] train_loss: 0.014304\n",
      "[129/00895] train_loss: 0.013995\n",
      "[129/00945] train_loss: 0.014136\n",
      "[129/00995] train_loss: 0.014415\n",
      "[129/01045] train_loss: 0.013491\n",
      "[129/01095] train_loss: 0.013834\n",
      "[129/01145] train_loss: 0.014244\n",
      "[129/01195] train_loss: 0.013981\n",
      "[130/00019] train_loss: 0.015145\n",
      "[130/00069] train_loss: 0.015780\n",
      "[130/00119] train_loss: 0.015105\n",
      "[130/00169] train_loss: 0.015706\n",
      "[130/00219] train_loss: 0.014839\n",
      "[130/00269] train_loss: 0.015203\n",
      "[130/00319] train_loss: 0.014133\n",
      "[130/00369] train_loss: 0.015201\n",
      "[130/00419] train_loss: 0.014732\n",
      "[130/00469] train_loss: 0.014359\n",
      "[130/00519] train_loss: 0.014345\n",
      "[130/00569] train_loss: 0.014401\n",
      "[130/00619] train_loss: 0.014387\n",
      "[130/00669] train_loss: 0.014480\n",
      "[130/00719] train_loss: 0.014236\n",
      "[130/00769] train_loss: 0.014025\n",
      "[130/00819] train_loss: 0.013659\n",
      "[130/00869] train_loss: 0.014211\n",
      "[130/00919] train_loss: 0.014918\n",
      "[130/00969] train_loss: 0.013525\n",
      "[130/01019] train_loss: 0.014036\n",
      "[130/01069] train_loss: 0.014429\n",
      "[130/01119] train_loss: 0.014448\n",
      "[130/01169] train_loss: 0.014491\n",
      "[130/01219] train_loss: 0.014426\n",
      "[131/00043] train_loss: 0.014858\n",
      "[131/00093] train_loss: 0.014755\n",
      "[131/00143] train_loss: 0.014458\n",
      "[131/00193] train_loss: 0.014658\n",
      "[131/00243] train_loss: 0.014542\n",
      "[131/00293] train_loss: 0.014670\n",
      "[131/00343] train_loss: 0.014863\n",
      "[131/00393] train_loss: 0.014280\n",
      "[131/00443] train_loss: 0.014359\n",
      "[131/00493] train_loss: 0.014053\n",
      "[131/00543] train_loss: 0.013442\n",
      "[131/00593] train_loss: 0.013841\n",
      "[131/00643] train_loss: 0.013809\n",
      "[131/00693] train_loss: 0.013905\n",
      "[131/00743] train_loss: 0.014074\n",
      "[131/00793] train_loss: 0.013989\n",
      "[131/00843] train_loss: 0.014479\n",
      "[131/00893] train_loss: 0.014138\n",
      "[131/00943] train_loss: 0.013870\n",
      "[131/00993] train_loss: 0.014185\n",
      "[131/01043] train_loss: 0.014059\n",
      "[131/01093] train_loss: 0.013593\n",
      "[131/01143] train_loss: 0.014503\n",
      "[131/01193] train_loss: 0.014189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[132/00017] train_loss: 0.015100\n",
      "[132/00067] train_loss: 0.015260\n",
      "[132/00117] train_loss: 0.015911\n",
      "[132/00167] train_loss: 0.015021\n",
      "[132/00217] train_loss: 0.014595\n",
      "[132/00267] train_loss: 0.015096\n",
      "[132/00317] train_loss: 0.015101\n",
      "[132/00367] train_loss: 0.014196\n",
      "[132/00417] train_loss: 0.015060\n",
      "[132/00467] train_loss: 0.013552\n",
      "[132/00517] train_loss: 0.014559\n",
      "[132/00567] train_loss: 0.013613\n",
      "[132/00617] train_loss: 0.013699\n",
      "[132/00667] train_loss: 0.013778\n",
      "[132/00717] train_loss: 0.014305\n",
      "[132/00767] train_loss: 0.014781\n",
      "[132/00817] train_loss: 0.014166\n",
      "[132/00867] train_loss: 0.015023\n",
      "[132/00917] train_loss: 0.013500\n",
      "[132/00967] train_loss: 0.014803\n",
      "[132/01017] train_loss: 0.014078\n",
      "[132/01067] train_loss: 0.013488\n",
      "[132/01117] train_loss: 0.013617\n",
      "[132/01167] train_loss: 0.014004\n",
      "[132/01217] train_loss: 0.014819\n",
      "[133/00041] train_loss: 0.016063\n",
      "[133/00091] train_loss: 0.015557\n",
      "[133/00141] train_loss: 0.014450\n",
      "[133/00191] train_loss: 0.014173\n",
      "[133/00241] train_loss: 0.013712\n",
      "[133/00291] train_loss: 0.015001\n",
      "[133/00341] train_loss: 0.014082\n",
      "[133/00391] train_loss: 0.014193\n",
      "[133/00441] train_loss: 0.014104\n",
      "[133/00491] train_loss: 0.013616\n",
      "[133/00541] train_loss: 0.014096\n",
      "[133/00591] train_loss: 0.014246\n",
      "[133/00641] train_loss: 0.013362\n",
      "[133/00691] train_loss: 0.014064\n",
      "[133/00741] train_loss: 0.014363\n",
      "[133/00791] train_loss: 0.014209\n",
      "[133/00841] train_loss: 0.013994\n",
      "[133/00891] train_loss: 0.014147\n",
      "[133/00941] train_loss: 0.014873\n",
      "[133/00991] train_loss: 0.014568\n",
      "[133/01041] train_loss: 0.014140\n",
      "[133/01091] train_loss: 0.013675\n",
      "[133/01141] train_loss: 0.014185\n",
      "[133/01191] train_loss: 0.013748\n",
      "[134/00015] train_loss: 0.014279\n",
      "[134/00065] train_loss: 0.015218\n",
      "[134/00115] train_loss: 0.014541\n",
      "[134/00165] train_loss: 0.014999\n",
      "[134/00215] train_loss: 0.014884\n",
      "[134/00265] train_loss: 0.014731\n",
      "[134/00315] train_loss: 0.015124\n",
      "[134/00365] train_loss: 0.013805\n",
      "[134/00415] train_loss: 0.014619\n",
      "[134/00465] train_loss: 0.014296\n",
      "[134/00515] train_loss: 0.014061\n",
      "[134/00565] train_loss: 0.014451\n",
      "[134/00615] train_loss: 0.014416\n",
      "[134/00665] train_loss: 0.014406\n",
      "[134/00715] train_loss: 0.014154\n",
      "[134/00765] train_loss: 0.014469\n",
      "[134/00815] train_loss: 0.014431\n",
      "[134/00865] train_loss: 0.014035\n",
      "[134/00915] train_loss: 0.014015\n",
      "[134/00965] train_loss: 0.014269\n",
      "[134/01015] train_loss: 0.013513\n",
      "[134/01065] train_loss: 0.014042\n",
      "[134/01115] train_loss: 0.014590\n",
      "[134/01165] train_loss: 0.014369\n",
      "[134/01215] train_loss: 0.014604\n",
      "[135/00039] train_loss: 0.015368\n",
      "[135/00089] train_loss: 0.014971\n",
      "[135/00139] train_loss: 0.015560\n",
      "[135/00189] train_loss: 0.014412\n",
      "[135/00239] train_loss: 0.014095\n",
      "[135/00289] train_loss: 0.014060\n",
      "[135/00339] train_loss: 0.014532\n",
      "[135/00389] train_loss: 0.014743\n",
      "[135/00439] train_loss: 0.013881\n",
      "[135/00489] train_loss: 0.014326\n",
      "[135/00539] train_loss: 0.013586\n",
      "[135/00589] train_loss: 0.015337\n",
      "[135/00639] train_loss: 0.014158\n",
      "[135/00689] train_loss: 0.014126\n",
      "[135/00739] train_loss: 0.013840\n",
      "[135/00789] train_loss: 0.013636\n",
      "[135/00839] train_loss: 0.014513\n",
      "[135/00889] train_loss: 0.013812\n",
      "[135/00939] train_loss: 0.013600\n",
      "[135/00989] train_loss: 0.014635\n",
      "[135/01039] train_loss: 0.014148\n",
      "[135/01089] train_loss: 0.013825\n",
      "[135/01139] train_loss: 0.014506\n",
      "[135/01189] train_loss: 0.013537\n",
      "[136/00013] train_loss: 0.013970\n",
      "[136/00063] train_loss: 0.016279\n",
      "[136/00113] train_loss: 0.015991\n",
      "[136/00163] train_loss: 0.013874\n",
      "[136/00213] train_loss: 0.014681\n",
      "[136/00263] train_loss: 0.014015\n",
      "[136/00313] train_loss: 0.014565\n",
      "[136/00363] train_loss: 0.014190\n",
      "[136/00413] train_loss: 0.014670\n",
      "[136/00463] train_loss: 0.014753\n",
      "[136/00513] train_loss: 0.014279\n",
      "[136/00563] train_loss: 0.014082\n",
      "[136/00613] train_loss: 0.015134\n",
      "[136/00663] train_loss: 0.013984\n",
      "[136/00713] train_loss: 0.014518\n",
      "[136/00763] train_loss: 0.014591\n",
      "[136/00813] train_loss: 0.014907\n",
      "[136/00863] train_loss: 0.013469\n",
      "[136/00913] train_loss: 0.014460\n",
      "[136/00963] train_loss: 0.014276\n",
      "[136/01013] train_loss: 0.013979\n",
      "[136/01063] train_loss: 0.014680\n",
      "[136/01113] train_loss: 0.014177\n",
      "[136/01163] train_loss: 0.014827\n",
      "[136/01213] train_loss: 0.013770\n",
      "[137/00037] train_loss: 0.014765\n",
      "[137/00087] train_loss: 0.015502\n",
      "[137/00137] train_loss: 0.014770\n",
      "[137/00187] train_loss: 0.015788\n",
      "[137/00237] train_loss: 0.014242\n",
      "[137/00287] train_loss: 0.014171\n",
      "[137/00337] train_loss: 0.013972\n",
      "[137/00387] train_loss: 0.013877\n",
      "[137/00437] train_loss: 0.014481\n",
      "[137/00487] train_loss: 0.013666\n",
      "[137/00537] train_loss: 0.013403\n",
      "[137/00587] train_loss: 0.013986\n",
      "[137/00637] train_loss: 0.014112\n",
      "[137/00687] train_loss: 0.014267\n",
      "[137/00737] train_loss: 0.013775\n",
      "[137/00787] train_loss: 0.014451\n",
      "[137/00837] train_loss: 0.013551\n",
      "[137/00887] train_loss: 0.014158\n",
      "[137/00937] train_loss: 0.013854\n",
      "[137/00987] train_loss: 0.014102\n",
      "[137/01037] train_loss: 0.013607\n",
      "[137/01087] train_loss: 0.014329\n",
      "[137/01137] train_loss: 0.014388\n",
      "[137/01187] train_loss: 0.014125\n",
      "[138/00011] train_loss: 0.013902\n",
      "[138/00061] train_loss: 0.015773\n",
      "[138/00111] train_loss: 0.014615\n",
      "[138/00161] train_loss: 0.015265\n",
      "[138/00211] train_loss: 0.015037\n",
      "[138/00261] train_loss: 0.014420\n",
      "[138/00311] train_loss: 0.014292\n",
      "[138/00361] train_loss: 0.014977\n",
      "[138/00411] train_loss: 0.014431\n",
      "[138/00461] train_loss: 0.014408\n",
      "[138/00511] train_loss: 0.013728\n",
      "[138/00561] train_loss: 0.014395\n",
      "[138/00611] train_loss: 0.014743\n",
      "[138/00661] train_loss: 0.014216\n",
      "[138/00711] train_loss: 0.014333\n",
      "[138/00761] train_loss: 0.013459\n",
      "[138/00811] train_loss: 0.013779\n",
      "[138/00861] train_loss: 0.013733\n",
      "[138/00911] train_loss: 0.014323\n",
      "[138/00961] train_loss: 0.013528\n",
      "[138/01011] train_loss: 0.014366\n",
      "[138/01061] train_loss: 0.014344\n",
      "[138/01111] train_loss: 0.013917\n",
      "[138/01161] train_loss: 0.014593\n",
      "[138/01211] train_loss: 0.013635\n",
      "[139/00035] train_loss: 0.015149\n",
      "[139/00085] train_loss: 0.015119\n",
      "[139/00135] train_loss: 0.013929\n",
      "[139/00185] train_loss: 0.015189\n",
      "[139/00235] train_loss: 0.013754\n",
      "[139/00285] train_loss: 0.014031\n",
      "[139/00335] train_loss: 0.014542\n",
      "[139/00385] train_loss: 0.014076\n",
      "[139/00435] train_loss: 0.014575\n",
      "[139/00485] train_loss: 0.013067\n",
      "[139/00535] train_loss: 0.013922\n",
      "[139/00585] train_loss: 0.013166\n",
      "[139/00635] train_loss: 0.013950\n",
      "[139/00685] train_loss: 0.014166\n",
      "[139/00735] train_loss: 0.013794\n",
      "[139/00785] train_loss: 0.014652\n",
      "[139/00835] train_loss: 0.014180\n",
      "[139/00885] train_loss: 0.014072\n",
      "[139/00935] train_loss: 0.013611\n",
      "[139/00985] train_loss: 0.013884\n",
      "[139/01035] train_loss: 0.014169\n",
      "[139/01085] train_loss: 0.014450\n",
      "[139/01135] train_loss: 0.014350\n",
      "[139/01185] train_loss: 0.014989\n",
      "[140/00009] train_loss: 0.014531\n",
      "[140/00059] train_loss: 0.015623\n",
      "[140/00109] train_loss: 0.014194\n",
      "[140/00159] train_loss: 0.015077\n",
      "[140/00209] train_loss: 0.014467\n",
      "[140/00259] train_loss: 0.014317\n",
      "[140/00309] train_loss: 0.014286\n",
      "[140/00359] train_loss: 0.014867\n",
      "[140/00409] train_loss: 0.014612\n",
      "[140/00459] train_loss: 0.013552\n",
      "[140/00509] train_loss: 0.013698\n",
      "[140/00559] train_loss: 0.013863\n",
      "[140/00609] train_loss: 0.014299\n",
      "[140/00659] train_loss: 0.014250\n",
      "[140/00709] train_loss: 0.013868\n",
      "[140/00759] train_loss: 0.014651\n",
      "[140/00809] train_loss: 0.013865\n",
      "[140/00859] train_loss: 0.013779\n",
      "[140/00909] train_loss: 0.014769\n",
      "[140/00959] train_loss: 0.014314\n",
      "[140/01009] train_loss: 0.013757\n",
      "[140/01059] train_loss: 0.013480\n",
      "[140/01109] train_loss: 0.014212\n",
      "[140/01159] train_loss: 0.014128\n",
      "[140/01209] train_loss: 0.013529\n",
      "[141/00033] train_loss: 0.015389\n",
      "[141/00083] train_loss: 0.014933\n",
      "[141/00133] train_loss: 0.014424\n",
      "[141/00183] train_loss: 0.014948\n",
      "[141/00233] train_loss: 0.014950\n",
      "[141/00283] train_loss: 0.015160\n",
      "[141/00333] train_loss: 0.014512\n",
      "[141/00383] train_loss: 0.013879\n",
      "[141/00433] train_loss: 0.014244\n",
      "[141/00483] train_loss: 0.014103\n",
      "[141/00533] train_loss: 0.014694\n",
      "[141/00583] train_loss: 0.014348\n",
      "[141/00633] train_loss: 0.013302\n",
      "[141/00683] train_loss: 0.013948\n",
      "[141/00733] train_loss: 0.014553\n",
      "[141/00783] train_loss: 0.013102\n",
      "[141/00833] train_loss: 0.013535\n",
      "[141/00883] train_loss: 0.014316\n",
      "[141/00933] train_loss: 0.013700\n",
      "[141/00983] train_loss: 0.013957\n",
      "[141/01033] train_loss: 0.014284\n",
      "[141/01083] train_loss: 0.013307\n",
      "[141/01133] train_loss: 0.013702\n",
      "[141/01183] train_loss: 0.014260\n",
      "[142/00007] train_loss: 0.013498\n",
      "[142/00057] train_loss: 0.015813\n",
      "[142/00107] train_loss: 0.016270\n",
      "[142/00157] train_loss: 0.014489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[142/00207] train_loss: 0.014539\n",
      "[142/00257] train_loss: 0.014116\n",
      "[142/00307] train_loss: 0.015035\n",
      "[142/00357] train_loss: 0.013557\n",
      "[142/00407] train_loss: 0.013988\n",
      "[142/00457] train_loss: 0.014638\n",
      "[142/00507] train_loss: 0.014151\n",
      "[142/00557] train_loss: 0.013682\n",
      "[142/00607] train_loss: 0.014099\n",
      "[142/00657] train_loss: 0.014978\n",
      "[142/00707] train_loss: 0.013491\n",
      "[142/00757] train_loss: 0.014065\n",
      "[142/00807] train_loss: 0.013610\n",
      "[142/00857] train_loss: 0.013192\n",
      "[142/00907] train_loss: 0.014173\n",
      "[142/00957] train_loss: 0.014821\n",
      "[142/01007] train_loss: 0.014053\n",
      "[142/01057] train_loss: 0.014379\n",
      "[142/01107] train_loss: 0.013685\n",
      "[142/01157] train_loss: 0.014078\n",
      "[142/01207] train_loss: 0.013583\n",
      "[143/00031] train_loss: 0.014928\n",
      "[143/00081] train_loss: 0.015742\n",
      "[143/00131] train_loss: 0.014707\n",
      "[143/00181] train_loss: 0.014013\n",
      "[143/00231] train_loss: 0.014099\n",
      "[143/00281] train_loss: 0.013629\n",
      "[143/00331] train_loss: 0.014093\n",
      "[143/00381] train_loss: 0.014712\n",
      "[143/00431] train_loss: 0.013599\n",
      "[143/00481] train_loss: 0.013883\n",
      "[143/00531] train_loss: 0.013889\n",
      "[143/00581] train_loss: 0.013734\n",
      "[143/00631] train_loss: 0.014453\n",
      "[143/00681] train_loss: 0.014004\n",
      "[143/00731] train_loss: 0.014424\n",
      "[143/00781] train_loss: 0.014183\n",
      "[143/00831] train_loss: 0.013938\n",
      "[143/00881] train_loss: 0.014177\n",
      "[143/00931] train_loss: 0.013019\n",
      "[143/00981] train_loss: 0.014443\n",
      "[143/01031] train_loss: 0.014946\n",
      "[143/01081] train_loss: 0.014645\n",
      "[143/01131] train_loss: 0.013098\n",
      "[143/01181] train_loss: 0.013602\n",
      "[144/00005] train_loss: 0.013451\n",
      "[144/00055] train_loss: 0.015709\n",
      "[144/00105] train_loss: 0.014751\n",
      "[144/00155] train_loss: 0.014795\n",
      "[144/00205] train_loss: 0.014039\n",
      "[144/00255] train_loss: 0.013705\n",
      "[144/00305] train_loss: 0.014278\n",
      "[144/00355] train_loss: 0.015942\n",
      "[144/00405] train_loss: 0.014835\n",
      "[144/00455] train_loss: 0.014452\n",
      "[144/00505] train_loss: 0.014457\n",
      "[144/00555] train_loss: 0.013283\n",
      "[144/00605] train_loss: 0.014121\n",
      "[144/00655] train_loss: 0.014817\n",
      "[144/00705] train_loss: 0.013201\n",
      "[144/00755] train_loss: 0.014710\n",
      "[144/00805] train_loss: 0.014174\n",
      "[144/00855] train_loss: 0.013256\n",
      "[144/00905] train_loss: 0.013425\n",
      "[144/00955] train_loss: 0.013416\n",
      "[144/01005] train_loss: 0.014322\n",
      "[144/01055] train_loss: 0.012690\n",
      "[144/01105] train_loss: 0.014016\n",
      "[144/01155] train_loss: 0.014311\n",
      "[144/01205] train_loss: 0.013188\n",
      "[145/00029] train_loss: 0.014616\n",
      "[145/00079] train_loss: 0.014710\n",
      "[145/00129] train_loss: 0.014560\n",
      "[145/00179] train_loss: 0.014336\n",
      "[145/00229] train_loss: 0.015057\n",
      "[145/00279] train_loss: 0.014752\n",
      "[145/00329] train_loss: 0.014898\n",
      "[145/00379] train_loss: 0.014168\n",
      "[145/00429] train_loss: 0.013790\n",
      "[145/00479] train_loss: 0.013944\n",
      "[145/00529] train_loss: 0.014144\n",
      "[145/00579] train_loss: 0.014096\n",
      "[145/00629] train_loss: 0.013352\n",
      "[145/00679] train_loss: 0.013951\n",
      "[145/00729] train_loss: 0.013969\n",
      "[145/00779] train_loss: 0.013465\n",
      "[145/00829] train_loss: 0.013736\n",
      "[145/00879] train_loss: 0.013557\n",
      "[145/00929] train_loss: 0.014444\n",
      "[145/00979] train_loss: 0.013607\n",
      "[145/01029] train_loss: 0.014630\n",
      "[145/01079] train_loss: 0.014174\n",
      "[145/01129] train_loss: 0.014104\n",
      "[145/01179] train_loss: 0.014148\n",
      "[146/00003] train_loss: 0.014061\n",
      "[146/00053] train_loss: 0.015174\n",
      "[146/00103] train_loss: 0.015753\n",
      "[146/00153] train_loss: 0.015198\n",
      "[146/00203] train_loss: 0.014640\n",
      "[146/00253] train_loss: 0.014123\n",
      "[146/00303] train_loss: 0.014240\n",
      "[146/00353] train_loss: 0.014735\n",
      "[146/00403] train_loss: 0.014856\n",
      "[146/00453] train_loss: 0.013143\n",
      "[146/00503] train_loss: 0.013926\n",
      "[146/00553] train_loss: 0.015320\n",
      "[146/00603] train_loss: 0.013560\n",
      "[146/00653] train_loss: 0.014062\n",
      "[146/00703] train_loss: 0.013341\n",
      "[146/00753] train_loss: 0.013169\n",
      "[146/00803] train_loss: 0.014241\n",
      "[146/00853] train_loss: 0.014112\n",
      "[146/00903] train_loss: 0.013684\n",
      "[146/00953] train_loss: 0.013611\n",
      "[146/01003] train_loss: 0.014695\n",
      "[146/01053] train_loss: 0.013990\n",
      "[146/01103] train_loss: 0.014924\n",
      "[146/01153] train_loss: 0.013093\n",
      "[146/01203] train_loss: 0.013604\n",
      "[147/00027] train_loss: 0.014279\n",
      "[147/00077] train_loss: 0.014733\n",
      "[147/00127] train_loss: 0.015066\n",
      "[147/00177] train_loss: 0.014953\n",
      "[147/00227] train_loss: 0.013782\n",
      "[147/00277] train_loss: 0.014236\n",
      "[147/00327] train_loss: 0.013429\n",
      "[147/00377] train_loss: 0.014810\n",
      "[147/00427] train_loss: 0.014258\n",
      "[147/00477] train_loss: 0.014312\n",
      "[147/00527] train_loss: 0.014254\n",
      "[147/00577] train_loss: 0.014014\n",
      "[147/00627] train_loss: 0.013848\n",
      "[147/00677] train_loss: 0.013826\n",
      "[147/00727] train_loss: 0.013654\n",
      "[147/00777] train_loss: 0.015020\n",
      "[147/00827] train_loss: 0.013758\n",
      "[147/00877] train_loss: 0.014668\n",
      "[147/00927] train_loss: 0.013177\n",
      "[147/00977] train_loss: 0.013201\n",
      "[147/01027] train_loss: 0.014341\n",
      "[147/01077] train_loss: 0.013745\n",
      "[147/01127] train_loss: 0.013560\n",
      "[147/01177] train_loss: 0.013911\n",
      "[148/00001] train_loss: 0.013434\n",
      "[148/00051] train_loss: 0.014755\n",
      "[148/00101] train_loss: 0.014385\n",
      "[148/00151] train_loss: 0.014768\n",
      "[148/00201] train_loss: 0.014505\n",
      "[148/00251] train_loss: 0.015112\n",
      "[148/00301] train_loss: 0.014942\n",
      "[148/00351] train_loss: 0.014760\n",
      "[148/00401] train_loss: 0.013419\n",
      "[148/00451] train_loss: 0.014532\n",
      "[148/00501] train_loss: 0.014656\n",
      "[148/00551] train_loss: 0.014677\n",
      "[148/00601] train_loss: 0.013452\n",
      "[148/00651] train_loss: 0.013462\n",
      "[148/00701] train_loss: 0.014438\n",
      "[148/00751] train_loss: 0.014133\n",
      "[148/00801] train_loss: 0.014121\n",
      "[148/00851] train_loss: 0.013947\n",
      "[148/00901] train_loss: 0.012837\n",
      "[148/00951] train_loss: 0.013914\n",
      "[148/01001] train_loss: 0.013936\n",
      "[148/01051] train_loss: 0.013828\n",
      "[148/01101] train_loss: 0.013662\n",
      "[148/01151] train_loss: 0.014240\n",
      "[148/01201] train_loss: 0.013824\n",
      "[149/00025] train_loss: 0.013958\n",
      "[149/00075] train_loss: 0.015184\n",
      "[149/00125] train_loss: 0.015382\n",
      "[149/00175] train_loss: 0.013895\n",
      "[149/00225] train_loss: 0.014798\n",
      "[149/00275] train_loss: 0.014341\n",
      "[149/00325] train_loss: 0.014501\n",
      "[149/00375] train_loss: 0.014693\n",
      "[149/00425] train_loss: 0.013471\n",
      "[149/00475] train_loss: 0.013828\n",
      "[149/00525] train_loss: 0.013256\n",
      "[149/00575] train_loss: 0.014372\n",
      "[149/00625] train_loss: 0.014179\n",
      "[149/00675] train_loss: 0.014048\n",
      "[149/00725] train_loss: 0.013479\n",
      "[149/00775] train_loss: 0.013520\n",
      "[149/00825] train_loss: 0.014178\n",
      "[149/00875] train_loss: 0.013232\n",
      "[149/00925] train_loss: 0.013606\n",
      "[149/00975] train_loss: 0.014355\n",
      "[149/01025] train_loss: 0.013791\n",
      "[149/01075] train_loss: 0.012659\n",
      "[149/01125] train_loss: 0.014754\n",
      "[149/01175] train_loss: 0.013801\n",
      "[149/01225] train_loss: 0.013359\n",
      "[150/00049] train_loss: 0.014572\n",
      "[150/00099] train_loss: 0.015482\n",
      "[150/00149] train_loss: 0.014629\n",
      "[150/00199] train_loss: 0.015023\n",
      "[150/00249] train_loss: 0.015355\n",
      "[150/00299] train_loss: 0.014301\n",
      "[150/00349] train_loss: 0.014112\n",
      "[150/00399] train_loss: 0.014090\n",
      "[150/00449] train_loss: 0.013386\n",
      "[150/00499] train_loss: 0.013831\n",
      "[150/00549] train_loss: 0.014056\n",
      "[150/00599] train_loss: 0.013950\n",
      "[150/00649] train_loss: 0.014091\n",
      "[150/00699] train_loss: 0.013687\n",
      "[150/00749] train_loss: 0.014763\n",
      "[150/00799] train_loss: 0.013528\n",
      "[150/00849] train_loss: 0.013168\n",
      "[150/00899] train_loss: 0.013785\n",
      "[150/00949] train_loss: 0.012959\n",
      "[150/00999] train_loss: 0.013985\n",
      "[150/01049] train_loss: 0.013710\n",
      "[150/01099] train_loss: 0.013970\n",
      "[150/01149] train_loss: 0.014089\n",
      "[150/01199] train_loss: 0.014091\n",
      "[151/00023] train_loss: 0.014328\n",
      "[151/00073] train_loss: 0.014493\n",
      "[151/00123] train_loss: 0.015458\n",
      "[151/00173] train_loss: 0.014971\n",
      "[151/00223] train_loss: 0.013696\n",
      "[151/00273] train_loss: 0.014512\n",
      "[151/00323] train_loss: 0.013955\n",
      "[151/00373] train_loss: 0.013114\n",
      "[151/00423] train_loss: 0.014202\n",
      "[151/00473] train_loss: 0.014270\n",
      "[151/00523] train_loss: 0.013898\n",
      "[151/00573] train_loss: 0.013420\n",
      "[151/00623] train_loss: 0.013636\n",
      "[151/00673] train_loss: 0.013506\n",
      "[151/00723] train_loss: 0.014285\n",
      "[151/00773] train_loss: 0.014037\n",
      "[151/00823] train_loss: 0.014174\n",
      "[151/00873] train_loss: 0.013685\n",
      "[151/00923] train_loss: 0.013714\n",
      "[151/00973] train_loss: 0.014115\n",
      "[151/01023] train_loss: 0.013950\n",
      "[151/01073] train_loss: 0.013731\n",
      "[151/01123] train_loss: 0.013590\n",
      "[151/01173] train_loss: 0.014048\n",
      "[151/01223] train_loss: 0.014697\n",
      "[152/00047] train_loss: 0.014920\n",
      "[152/00097] train_loss: 0.015104\n",
      "[152/00147] train_loss: 0.014722\n",
      "[152/00197] train_loss: 0.014865\n",
      "[152/00247] train_loss: 0.014311\n",
      "[152/00297] train_loss: 0.014379\n",
      "[152/00347] train_loss: 0.013989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[152/00397] train_loss: 0.014680\n",
      "[152/00447] train_loss: 0.013897\n",
      "[152/00497] train_loss: 0.013685\n",
      "[152/00547] train_loss: 0.012981\n",
      "[152/00597] train_loss: 0.014306\n",
      "[152/00647] train_loss: 0.013589\n",
      "[152/00697] train_loss: 0.014522\n",
      "[152/00747] train_loss: 0.013969\n",
      "[152/00797] train_loss: 0.014076\n",
      "[152/00847] train_loss: 0.013878\n",
      "[152/00897] train_loss: 0.013148\n",
      "[152/00947] train_loss: 0.013788\n",
      "[152/00997] train_loss: 0.013021\n",
      "[152/01047] train_loss: 0.014115\n",
      "[152/01097] train_loss: 0.013861\n",
      "[152/01147] train_loss: 0.014192\n",
      "[152/01197] train_loss: 0.013219\n",
      "[153/00021] train_loss: 0.014178\n",
      "[153/00071] train_loss: 0.014712\n",
      "[153/00121] train_loss: 0.015045\n",
      "[153/00171] train_loss: 0.013778\n",
      "[153/00221] train_loss: 0.014962\n",
      "[153/00271] train_loss: 0.014103\n",
      "[153/00321] train_loss: 0.014075\n",
      "[153/00371] train_loss: 0.013658\n",
      "[153/00421] train_loss: 0.013547\n",
      "[153/00471] train_loss: 0.013631\n",
      "[153/00521] train_loss: 0.012619\n",
      "[153/00571] train_loss: 0.013805\n",
      "[153/00621] train_loss: 0.013573\n",
      "[153/00671] train_loss: 0.013921\n",
      "[153/00721] train_loss: 0.014471\n",
      "[153/00771] train_loss: 0.013557\n",
      "[153/00821] train_loss: 0.013981\n",
      "[153/00871] train_loss: 0.013496\n",
      "[153/00921] train_loss: 0.013902\n",
      "[153/00971] train_loss: 0.013903\n",
      "[153/01021] train_loss: 0.013624\n",
      "[153/01071] train_loss: 0.014025\n",
      "[153/01121] train_loss: 0.013996\n",
      "[153/01171] train_loss: 0.014385\n",
      "[153/01221] train_loss: 0.013612\n",
      "[154/00045] train_loss: 0.015364\n",
      "[154/00095] train_loss: 0.014499\n",
      "[154/00145] train_loss: 0.014656\n",
      "[154/00195] train_loss: 0.015128\n",
      "[154/00245] train_loss: 0.014082\n",
      "[154/00295] train_loss: 0.013701\n",
      "[154/00345] train_loss: 0.014290\n",
      "[154/00395] train_loss: 0.014232\n",
      "[154/00445] train_loss: 0.013625\n",
      "[154/00495] train_loss: 0.014373\n",
      "[154/00545] train_loss: 0.013667\n",
      "[154/00595] train_loss: 0.013618\n",
      "[154/00645] train_loss: 0.014227\n",
      "[154/00695] train_loss: 0.013775\n",
      "[154/00745] train_loss: 0.013584\n",
      "[154/00795] train_loss: 0.013344\n",
      "[154/00845] train_loss: 0.012984\n",
      "[154/00895] train_loss: 0.013412\n",
      "[154/00945] train_loss: 0.014303\n",
      "[154/00995] train_loss: 0.013936\n",
      "[154/01045] train_loss: 0.014178\n",
      "[154/01095] train_loss: 0.013720\n",
      "[154/01145] train_loss: 0.013773\n",
      "[154/01195] train_loss: 0.014234\n",
      "[155/00019] train_loss: 0.013698\n",
      "[155/00069] train_loss: 0.015150\n",
      "[155/00119] train_loss: 0.014586\n",
      "[155/00169] train_loss: 0.014320\n",
      "[155/00219] train_loss: 0.014449\n",
      "[155/00269] train_loss: 0.014125\n",
      "[155/00319] train_loss: 0.014349\n",
      "[155/00369] train_loss: 0.013463\n",
      "[155/00419] train_loss: 0.014493\n",
      "[155/00469] train_loss: 0.013698\n",
      "[155/00519] train_loss: 0.014147\n",
      "[155/00569] train_loss: 0.013942\n",
      "[155/00619] train_loss: 0.013576\n",
      "[155/00669] train_loss: 0.014564\n",
      "[155/00719] train_loss: 0.014212\n",
      "[155/00769] train_loss: 0.013603\n",
      "[155/00819] train_loss: 0.013679\n",
      "[155/00869] train_loss: 0.013727\n",
      "[155/00919] train_loss: 0.013782\n",
      "[155/00969] train_loss: 0.013616\n",
      "[155/01019] train_loss: 0.014616\n",
      "[155/01069] train_loss: 0.013492\n",
      "[155/01119] train_loss: 0.013389\n",
      "[155/01169] train_loss: 0.013490\n",
      "[155/01219] train_loss: 0.013455\n",
      "[156/00043] train_loss: 0.014768\n",
      "[156/00093] train_loss: 0.014487\n",
      "[156/00143] train_loss: 0.014169\n",
      "[156/00193] train_loss: 0.014717\n",
      "[156/00243] train_loss: 0.013967\n",
      "[156/00293] train_loss: 0.013810\n",
      "[156/00343] train_loss: 0.014572\n",
      "[156/00393] train_loss: 0.014121\n",
      "[156/00443] train_loss: 0.013897\n",
      "[156/00493] train_loss: 0.013876\n",
      "[156/00543] train_loss: 0.013895\n",
      "[156/00593] train_loss: 0.013833\n",
      "[156/00643] train_loss: 0.014097\n",
      "[156/00693] train_loss: 0.013425\n",
      "[156/00743] train_loss: 0.014168\n",
      "[156/00793] train_loss: 0.014129\n",
      "[156/00843] train_loss: 0.013889\n",
      "[156/00893] train_loss: 0.013678\n",
      "[156/00943] train_loss: 0.013601\n",
      "[156/00993] train_loss: 0.013211\n",
      "[156/01043] train_loss: 0.013152\n",
      "[156/01093] train_loss: 0.014748\n",
      "[156/01143] train_loss: 0.013311\n",
      "[156/01193] train_loss: 0.014072\n",
      "[157/00017] train_loss: 0.014302\n",
      "[157/00067] train_loss: 0.014425\n",
      "[157/00117] train_loss: 0.014644\n",
      "[157/00167] train_loss: 0.014476\n",
      "[157/00217] train_loss: 0.013995\n",
      "[157/00267] train_loss: 0.014257\n",
      "[157/00317] train_loss: 0.014560\n",
      "[157/00367] train_loss: 0.014209\n",
      "[157/00417] train_loss: 0.013054\n",
      "[157/00467] train_loss: 0.013663\n",
      "[157/00517] train_loss: 0.014055\n",
      "[157/00567] train_loss: 0.012803\n",
      "[157/00617] train_loss: 0.013779\n",
      "[157/00667] train_loss: 0.014293\n",
      "[157/00717] train_loss: 0.013940\n",
      "[157/00767] train_loss: 0.013478\n",
      "[157/00817] train_loss: 0.013662\n",
      "[157/00867] train_loss: 0.013907\n",
      "[157/00917] train_loss: 0.013437\n",
      "[157/00967] train_loss: 0.014143\n",
      "[157/01017] train_loss: 0.014565\n",
      "[157/01067] train_loss: 0.013282\n",
      "[157/01117] train_loss: 0.013039\n",
      "[157/01167] train_loss: 0.014612\n",
      "[157/01217] train_loss: 0.013683\n",
      "[158/00041] train_loss: 0.014654\n",
      "[158/00091] train_loss: 0.014926\n",
      "[158/00141] train_loss: 0.014670\n",
      "[158/00191] train_loss: 0.014004\n",
      "[158/00241] train_loss: 0.013864\n",
      "[158/00291] train_loss: 0.014419\n",
      "[158/00341] train_loss: 0.014449\n",
      "[158/00391] train_loss: 0.014401\n",
      "[158/00441] train_loss: 0.012733\n",
      "[158/00491] train_loss: 0.013487\n",
      "[158/00541] train_loss: 0.013627\n",
      "[158/00591] train_loss: 0.014190\n",
      "[158/00641] train_loss: 0.013599\n",
      "[158/00691] train_loss: 0.013137\n",
      "[158/00741] train_loss: 0.014116\n",
      "[158/00791] train_loss: 0.014243\n",
      "[158/00841] train_loss: 0.014256\n",
      "[158/00891] train_loss: 0.014263\n",
      "[158/00941] train_loss: 0.014568\n",
      "[158/00991] train_loss: 0.012710\n",
      "[158/01041] train_loss: 0.014127\n",
      "[158/01091] train_loss: 0.012996\n",
      "[158/01141] train_loss: 0.013339\n",
      "[158/01191] train_loss: 0.013649\n",
      "[159/00015] train_loss: 0.014179\n",
      "[159/00065] train_loss: 0.015872\n",
      "[159/00115] train_loss: 0.015041\n",
      "[159/00165] train_loss: 0.014556\n",
      "[159/00215] train_loss: 0.014364\n",
      "[159/00265] train_loss: 0.014599\n",
      "[159/00315] train_loss: 0.014415\n",
      "[159/00365] train_loss: 0.014044\n",
      "[159/00415] train_loss: 0.014160\n",
      "[159/00465] train_loss: 0.013719\n",
      "[159/00515] train_loss: 0.013993\n",
      "[159/00565] train_loss: 0.014588\n",
      "[159/00615] train_loss: 0.013694\n",
      "[159/00665] train_loss: 0.014100\n",
      "[159/00715] train_loss: 0.013590\n",
      "[159/00765] train_loss: 0.014808\n",
      "[159/00815] train_loss: 0.014219\n",
      "[159/00865] train_loss: 0.014115\n",
      "[159/00915] train_loss: 0.013523\n",
      "[159/00965] train_loss: 0.013389\n",
      "[159/01015] train_loss: 0.014146\n",
      "[159/01065] train_loss: 0.013414\n",
      "[159/01115] train_loss: 0.013849\n",
      "[159/01165] train_loss: 0.013680\n",
      "[159/01215] train_loss: 0.013729\n",
      "[160/00039] train_loss: 0.014764\n",
      "[160/00089] train_loss: 0.014522\n",
      "[160/00139] train_loss: 0.014089\n",
      "[160/00189] train_loss: 0.013611\n",
      "[160/00239] train_loss: 0.014208\n",
      "[160/00289] train_loss: 0.014561\n",
      "[160/00339] train_loss: 0.013519\n",
      "[160/00389] train_loss: 0.014263\n",
      "[160/00439] train_loss: 0.014259\n",
      "[160/00489] train_loss: 0.013918\n",
      "[160/00539] train_loss: 0.014919\n",
      "[160/00589] train_loss: 0.014056\n",
      "[160/00639] train_loss: 0.013503\n",
      "[160/00689] train_loss: 0.014059\n",
      "[160/00739] train_loss: 0.013523\n",
      "[160/00789] train_loss: 0.013535\n",
      "[160/00839] train_loss: 0.014243\n",
      "[160/00889] train_loss: 0.013413\n",
      "[160/00939] train_loss: 0.014156\n",
      "[160/00989] train_loss: 0.013442\n",
      "[160/01039] train_loss: 0.013039\n",
      "[160/01089] train_loss: 0.013199\n",
      "[160/01139] train_loss: 0.013853\n",
      "[160/01189] train_loss: 0.013737\n",
      "[161/00013] train_loss: 0.013928\n",
      "[161/00063] train_loss: 0.014695\n",
      "[161/00113] train_loss: 0.014537\n",
      "[161/00163] train_loss: 0.015095\n",
      "[161/00213] train_loss: 0.014687\n",
      "[161/00263] train_loss: 0.014440\n",
      "[161/00313] train_loss: 0.013228\n",
      "[161/00363] train_loss: 0.013656\n",
      "[161/00413] train_loss: 0.014263\n",
      "[161/00463] train_loss: 0.014429\n",
      "[161/00513] train_loss: 0.013575\n",
      "[161/00563] train_loss: 0.014831\n",
      "[161/00613] train_loss: 0.013460\n",
      "[161/00663] train_loss: 0.013992\n",
      "[161/00713] train_loss: 0.013439\n",
      "[161/00763] train_loss: 0.014039\n",
      "[161/00813] train_loss: 0.013697\n",
      "[161/00863] train_loss: 0.014056\n",
      "[161/00913] train_loss: 0.014552\n",
      "[161/00963] train_loss: 0.013660\n",
      "[161/01013] train_loss: 0.013256\n",
      "[161/01063] train_loss: 0.013825\n",
      "[161/01113] train_loss: 0.013601\n",
      "[161/01163] train_loss: 0.012627\n",
      "[161/01213] train_loss: 0.013424\n",
      "[162/00037] train_loss: 0.014364\n",
      "[162/00087] train_loss: 0.014660\n",
      "[162/00137] train_loss: 0.014701\n",
      "[162/00187] train_loss: 0.013901\n",
      "[162/00237] train_loss: 0.014703\n",
      "[162/00287] train_loss: 0.014349\n",
      "[162/00337] train_loss: 0.013823\n",
      "[162/00387] train_loss: 0.013999\n",
      "[162/00437] train_loss: 0.013565\n",
      "[162/00487] train_loss: 0.013647\n",
      "[162/00537] train_loss: 0.014569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[162/00587] train_loss: 0.013358\n",
      "[162/00637] train_loss: 0.013731\n",
      "[162/00687] train_loss: 0.013128\n",
      "[162/00737] train_loss: 0.014526\n",
      "[162/00787] train_loss: 0.013169\n",
      "[162/00837] train_loss: 0.014151\n",
      "[162/00887] train_loss: 0.013846\n",
      "[162/00937] train_loss: 0.013695\n",
      "[162/00987] train_loss: 0.013308\n",
      "[162/01037] train_loss: 0.014058\n",
      "[162/01087] train_loss: 0.012964\n",
      "[162/01137] train_loss: 0.013283\n",
      "[162/01187] train_loss: 0.013653\n",
      "[163/00011] train_loss: 0.014463\n",
      "[163/00061] train_loss: 0.014827\n",
      "[163/00111] train_loss: 0.014576\n",
      "[163/00161] train_loss: 0.014351\n",
      "[163/00211] train_loss: 0.014028\n",
      "[163/00261] train_loss: 0.014601\n",
      "[163/00311] train_loss: 0.013665\n",
      "[163/00361] train_loss: 0.014361\n",
      "[163/00411] train_loss: 0.014382\n",
      "[163/00461] train_loss: 0.013017\n",
      "[163/00511] train_loss: 0.013489\n",
      "[163/00561] train_loss: 0.014035\n",
      "[163/00611] train_loss: 0.013465\n",
      "[163/00661] train_loss: 0.013813\n",
      "[163/00711] train_loss: 0.013712\n",
      "[163/00761] train_loss: 0.014477\n",
      "[163/00811] train_loss: 0.013875\n",
      "[163/00861] train_loss: 0.013460\n",
      "[163/00911] train_loss: 0.013258\n",
      "[163/00961] train_loss: 0.013001\n",
      "[163/01011] train_loss: 0.014400\n",
      "[163/01061] train_loss: 0.013510\n",
      "[163/01111] train_loss: 0.013776\n",
      "[163/01161] train_loss: 0.013423\n",
      "[163/01211] train_loss: 0.013245\n",
      "[164/00035] train_loss: 0.015546\n",
      "[164/00085] train_loss: 0.014193\n",
      "[164/00135] train_loss: 0.015292\n",
      "[164/00185] train_loss: 0.014195\n",
      "[164/00235] train_loss: 0.014119\n",
      "[164/00285] train_loss: 0.014000\n",
      "[164/00335] train_loss: 0.014484\n",
      "[164/00385] train_loss: 0.013843\n",
      "[164/00435] train_loss: 0.014181\n",
      "[164/00485] train_loss: 0.013408\n",
      "[164/00535] train_loss: 0.014292\n",
      "[164/00585] train_loss: 0.014020\n",
      "[164/00635] train_loss: 0.013792\n",
      "[164/00685] train_loss: 0.013935\n",
      "[164/00735] train_loss: 0.013577\n",
      "[164/00785] train_loss: 0.013815\n",
      "[164/00835] train_loss: 0.013651\n",
      "[164/00885] train_loss: 0.013539\n",
      "[164/00935] train_loss: 0.014383\n",
      "[164/00985] train_loss: 0.013088\n",
      "[164/01035] train_loss: 0.013231\n",
      "[164/01085] train_loss: 0.013047\n",
      "[164/01135] train_loss: 0.013130\n",
      "[164/01185] train_loss: 0.013383\n",
      "[165/00009] train_loss: 0.013309\n",
      "[165/00059] train_loss: 0.015203\n",
      "[165/00109] train_loss: 0.014109\n",
      "[165/00159] train_loss: 0.013455\n",
      "[165/00209] train_loss: 0.014536\n",
      "[165/00259] train_loss: 0.013644\n",
      "[165/00309] train_loss: 0.014694\n",
      "[165/00359] train_loss: 0.014341\n",
      "[165/00409] train_loss: 0.014922\n",
      "[165/00459] train_loss: 0.013893\n",
      "[165/00509] train_loss: 0.014078\n",
      "[165/00559] train_loss: 0.013634\n",
      "[165/00609] train_loss: 0.013289\n",
      "[165/00659] train_loss: 0.013602\n",
      "[165/00709] train_loss: 0.013645\n",
      "[165/00759] train_loss: 0.013754\n",
      "[165/00809] train_loss: 0.013079\n",
      "[165/00859] train_loss: 0.013786\n",
      "[165/00909] train_loss: 0.014160\n",
      "[165/00959] train_loss: 0.013250\n",
      "[165/01009] train_loss: 0.013464\n",
      "[165/01059] train_loss: 0.013892\n",
      "[165/01109] train_loss: 0.014263\n",
      "[165/01159] train_loss: 0.013821\n",
      "[165/01209] train_loss: 0.013465\n",
      "[166/00033] train_loss: 0.014123\n",
      "[166/00083] train_loss: 0.014570\n",
      "[166/00133] train_loss: 0.013910\n",
      "[166/00183] train_loss: 0.013632\n",
      "[166/00233] train_loss: 0.013750\n",
      "[166/00283] train_loss: 0.014309\n",
      "[166/00333] train_loss: 0.014071\n",
      "[166/00383] train_loss: 0.013565\n",
      "[166/00433] train_loss: 0.014223\n",
      "[166/00483] train_loss: 0.013402\n",
      "[166/00533] train_loss: 0.013968\n",
      "[166/00583] train_loss: 0.014411\n",
      "[166/00633] train_loss: 0.013786\n",
      "[166/00683] train_loss: 0.013633\n",
      "[166/00733] train_loss: 0.013592\n",
      "[166/00783] train_loss: 0.013701\n",
      "[166/00833] train_loss: 0.013157\n",
      "[166/00883] train_loss: 0.013816\n",
      "[166/00933] train_loss: 0.014466\n",
      "[166/00983] train_loss: 0.013224\n",
      "[166/01033] train_loss: 0.013663\n",
      "[166/01083] train_loss: 0.013792\n",
      "[166/01133] train_loss: 0.013781\n",
      "[166/01183] train_loss: 0.013899\n",
      "[167/00007] train_loss: 0.014300\n",
      "[167/00057] train_loss: 0.014723\n",
      "[167/00107] train_loss: 0.014273\n",
      "[167/00157] train_loss: 0.014295\n",
      "[167/00207] train_loss: 0.014019\n",
      "[167/00257] train_loss: 0.013655\n",
      "[167/00307] train_loss: 0.014280\n",
      "[167/00357] train_loss: 0.013225\n",
      "[167/00407] train_loss: 0.013923\n",
      "[167/00457] train_loss: 0.013868\n",
      "[167/00507] train_loss: 0.013439\n",
      "[167/00557] train_loss: 0.013310\n",
      "[167/00607] train_loss: 0.013129\n",
      "[167/00657] train_loss: 0.014201\n",
      "[167/00707] train_loss: 0.013802\n",
      "[167/00757] train_loss: 0.013647\n",
      "[167/00807] train_loss: 0.014591\n",
      "[167/00857] train_loss: 0.014115\n",
      "[167/00907] train_loss: 0.014178\n",
      "[167/00957] train_loss: 0.013400\n",
      "[167/01007] train_loss: 0.014162\n",
      "[167/01057] train_loss: 0.013501\n",
      "[167/01107] train_loss: 0.014298\n",
      "[167/01157] train_loss: 0.013455\n",
      "[167/01207] train_loss: 0.013313\n",
      "[168/00031] train_loss: 0.014589\n",
      "[168/00081] train_loss: 0.014870\n",
      "[168/00131] train_loss: 0.014876\n",
      "[168/00181] train_loss: 0.013968\n",
      "[168/00231] train_loss: 0.013854\n",
      "[168/00281] train_loss: 0.015300\n",
      "[168/00331] train_loss: 0.013628\n",
      "[168/00381] train_loss: 0.013684\n",
      "[168/00431] train_loss: 0.013954\n",
      "[168/00481] train_loss: 0.013040\n",
      "[168/00531] train_loss: 0.013549\n",
      "[168/00581] train_loss: 0.013865\n",
      "[168/00631] train_loss: 0.013958\n",
      "[168/00681] train_loss: 0.013827\n",
      "[168/00731] train_loss: 0.014132\n",
      "[168/00781] train_loss: 0.013956\n",
      "[168/00831] train_loss: 0.013194\n",
      "[168/00881] train_loss: 0.013639\n",
      "[168/00931] train_loss: 0.013327\n",
      "[168/00981] train_loss: 0.013888\n",
      "[168/01031] train_loss: 0.013417\n",
      "[168/01081] train_loss: 0.013321\n",
      "[168/01131] train_loss: 0.013753\n",
      "[168/01181] train_loss: 0.014214\n",
      "[169/00005] train_loss: 0.014297\n",
      "[169/00055] train_loss: 0.014892\n",
      "[169/00105] train_loss: 0.014357\n",
      "[169/00155] train_loss: 0.014555\n",
      "[169/00205] train_loss: 0.014019\n",
      "[169/00255] train_loss: 0.014204\n",
      "[169/00305] train_loss: 0.013537\n",
      "[169/00355] train_loss: 0.014244\n",
      "[169/00405] train_loss: 0.013978\n",
      "[169/00455] train_loss: 0.014067\n",
      "[169/00505] train_loss: 0.013692\n",
      "[169/00555] train_loss: 0.013519\n",
      "[169/00605] train_loss: 0.013440\n",
      "[169/00655] train_loss: 0.013742\n",
      "[169/00705] train_loss: 0.013421\n",
      "[169/00755] train_loss: 0.013173\n",
      "[169/00805] train_loss: 0.014252\n",
      "[169/00855] train_loss: 0.013357\n",
      "[169/00905] train_loss: 0.013822\n",
      "[169/00955] train_loss: 0.013755\n",
      "[169/01005] train_loss: 0.014106\n",
      "[169/01055] train_loss: 0.014294\n",
      "[169/01105] train_loss: 0.012939\n",
      "[169/01155] train_loss: 0.013321\n",
      "[169/01205] train_loss: 0.013935\n",
      "[170/00029] train_loss: 0.014277\n",
      "[170/00079] train_loss: 0.015397\n",
      "[170/00129] train_loss: 0.014746\n",
      "[170/00179] train_loss: 0.015102\n",
      "[170/00229] train_loss: 0.014533\n",
      "[170/00279] train_loss: 0.013380\n",
      "[170/00329] train_loss: 0.013615\n",
      "[170/00379] train_loss: 0.013675\n",
      "[170/00429] train_loss: 0.014243\n",
      "[170/00479] train_loss: 0.013878\n",
      "[170/00529] train_loss: 0.013178\n",
      "[170/00579] train_loss: 0.014445\n",
      "[170/00629] train_loss: 0.013653\n",
      "[170/00679] train_loss: 0.013077\n",
      "[170/00729] train_loss: 0.013265\n",
      "[170/00779] train_loss: 0.013302\n",
      "[170/00829] train_loss: 0.013696\n",
      "[170/00879] train_loss: 0.013459\n",
      "[170/00929] train_loss: 0.013486\n",
      "[170/00979] train_loss: 0.013558\n",
      "[170/01029] train_loss: 0.013051\n",
      "[170/01079] train_loss: 0.012805\n",
      "[170/01129] train_loss: 0.013567\n",
      "[170/01179] train_loss: 0.013671\n",
      "[171/00003] train_loss: 0.013472\n",
      "[171/00053] train_loss: 0.014674\n",
      "[171/00103] train_loss: 0.014718\n",
      "[171/00153] train_loss: 0.014730\n",
      "[171/00203] train_loss: 0.014404\n",
      "[171/00253] train_loss: 0.015070\n",
      "[171/00303] train_loss: 0.013597\n",
      "[171/00353] train_loss: 0.013711\n",
      "[171/00403] train_loss: 0.013525\n",
      "[171/00453] train_loss: 0.014076\n",
      "[171/00503] train_loss: 0.013624\n",
      "[171/00553] train_loss: 0.013947\n",
      "[171/00603] train_loss: 0.013599\n",
      "[171/00653] train_loss: 0.013859\n",
      "[171/00703] train_loss: 0.013815\n",
      "[171/00753] train_loss: 0.013301\n",
      "[171/00803] train_loss: 0.013679\n",
      "[171/00853] train_loss: 0.013595\n",
      "[171/00903] train_loss: 0.013957\n",
      "[171/00953] train_loss: 0.013692\n",
      "[171/01003] train_loss: 0.013867\n",
      "[171/01053] train_loss: 0.013107\n",
      "[171/01103] train_loss: 0.014007\n",
      "[171/01153] train_loss: 0.012984\n",
      "[171/01203] train_loss: 0.012708\n",
      "[172/00027] train_loss: 0.014579\n",
      "[172/00077] train_loss: 0.014439\n",
      "[172/00127] train_loss: 0.014292\n",
      "[172/00177] train_loss: 0.013697\n",
      "[172/00227] train_loss: 0.014414\n",
      "[172/00277] train_loss: 0.014367\n",
      "[172/00327] train_loss: 0.013934\n",
      "[172/00377] train_loss: 0.014153\n",
      "[172/00427] train_loss: 0.014048\n",
      "[172/00477] train_loss: 0.013483\n",
      "[172/00527] train_loss: 0.014225\n",
      "[172/00577] train_loss: 0.013423\n",
      "[172/00627] train_loss: 0.013885\n",
      "[172/00677] train_loss: 0.014444\n",
      "[172/00727] train_loss: 0.013214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[172/00777] train_loss: 0.013714\n",
      "[172/00827] train_loss: 0.013956\n",
      "[172/00877] train_loss: 0.014017\n",
      "[172/00927] train_loss: 0.012703\n",
      "[172/00977] train_loss: 0.013666\n",
      "[172/01027] train_loss: 0.013010\n",
      "[172/01077] train_loss: 0.013760\n",
      "[172/01127] train_loss: 0.012795\n",
      "[172/01177] train_loss: 0.013938\n",
      "[173/00001] train_loss: 0.013000\n",
      "[173/00051] train_loss: 0.015174\n",
      "[173/00101] train_loss: 0.015322\n",
      "[173/00151] train_loss: 0.014319\n",
      "[173/00201] train_loss: 0.014196\n",
      "[173/00251] train_loss: 0.014131\n",
      "[173/00301] train_loss: 0.014307\n",
      "[173/00351] train_loss: 0.013814\n",
      "[173/00401] train_loss: 0.013765\n",
      "[173/00451] train_loss: 0.013647\n",
      "[173/00501] train_loss: 0.013712\n",
      "[173/00551] train_loss: 0.013838\n",
      "[173/00601] train_loss: 0.014393\n",
      "[173/00651] train_loss: 0.013281\n",
      "[173/00701] train_loss: 0.013568\n",
      "[173/00751] train_loss: 0.013628\n",
      "[173/00801] train_loss: 0.013878\n",
      "[173/00851] train_loss: 0.013382\n",
      "[173/00901] train_loss: 0.013728\n",
      "[173/00951] train_loss: 0.013244\n",
      "[173/01001] train_loss: 0.013876\n",
      "[173/01051] train_loss: 0.013337\n",
      "[173/01101] train_loss: 0.013829\n",
      "[173/01151] train_loss: 0.013848\n",
      "[173/01201] train_loss: 0.014221\n",
      "[174/00025] train_loss: 0.013577\n",
      "[174/00075] train_loss: 0.014964\n",
      "[174/00125] train_loss: 0.014570\n",
      "[174/00175] train_loss: 0.014155\n",
      "[174/00225] train_loss: 0.014591\n",
      "[174/00275] train_loss: 0.014182\n",
      "[174/00325] train_loss: 0.014202\n",
      "[174/00375] train_loss: 0.013531\n",
      "[174/00425] train_loss: 0.014358\n",
      "[174/00475] train_loss: 0.013624\n",
      "[174/00525] train_loss: 0.013654\n",
      "[174/00575] train_loss: 0.013269\n",
      "[174/00625] train_loss: 0.014327\n",
      "[174/00675] train_loss: 0.012920\n",
      "[174/00725] train_loss: 0.012915\n",
      "[174/00775] train_loss: 0.012877\n",
      "[174/00825] train_loss: 0.014125\n",
      "[174/00875] train_loss: 0.013686\n",
      "[174/00925] train_loss: 0.013632\n",
      "[174/00975] train_loss: 0.013430\n",
      "[174/01025] train_loss: 0.013662\n",
      "[174/01075] train_loss: 0.012840\n",
      "[174/01125] train_loss: 0.013393\n",
      "[174/01175] train_loss: 0.013336\n",
      "[174/01225] train_loss: 0.013714\n",
      "[175/00049] train_loss: 0.014740\n",
      "[175/00099] train_loss: 0.014566\n",
      "[175/00149] train_loss: 0.014263\n",
      "[175/00199] train_loss: 0.013637\n",
      "[175/00249] train_loss: 0.013399\n",
      "[175/00299] train_loss: 0.014065\n",
      "[175/00349] train_loss: 0.014460\n",
      "[175/00399] train_loss: 0.013476\n",
      "[175/00449] train_loss: 0.013273\n",
      "[175/00499] train_loss: 0.013951\n",
      "[175/00549] train_loss: 0.014388\n",
      "[175/00599] train_loss: 0.013087\n",
      "[175/00649] train_loss: 0.013097\n",
      "[175/00699] train_loss: 0.012815\n",
      "[175/00749] train_loss: 0.013110\n",
      "[175/00799] train_loss: 0.014048\n",
      "[175/00849] train_loss: 0.014053\n",
      "[175/00899] train_loss: 0.013057\n",
      "[175/00949] train_loss: 0.013384\n",
      "[175/00999] train_loss: 0.014060\n",
      "[175/01049] train_loss: 0.013370\n",
      "[175/01099] train_loss: 0.013370\n",
      "[175/01149] train_loss: 0.013331\n",
      "[175/01199] train_loss: 0.012870\n",
      "[176/00023] train_loss: 0.014595\n",
      "[176/00073] train_loss: 0.013998\n",
      "[176/00123] train_loss: 0.013744\n",
      "[176/00173] train_loss: 0.013892\n",
      "[176/00223] train_loss: 0.013646\n",
      "[176/00273] train_loss: 0.015414\n",
      "[176/00323] train_loss: 0.013715\n",
      "[176/00373] train_loss: 0.014285\n",
      "[176/00423] train_loss: 0.013563\n",
      "[176/00473] train_loss: 0.013811\n",
      "[176/00523] train_loss: 0.013829\n",
      "[176/00573] train_loss: 0.013130\n",
      "[176/00623] train_loss: 0.014055\n",
      "[176/00673] train_loss: 0.013946\n",
      "[176/00723] train_loss: 0.013088\n",
      "[176/00773] train_loss: 0.013899\n",
      "[176/00823] train_loss: 0.014107\n",
      "[176/00873] train_loss: 0.013101\n",
      "[176/00923] train_loss: 0.013207\n",
      "[176/00973] train_loss: 0.013573\n",
      "[176/01023] train_loss: 0.012712\n",
      "[176/01073] train_loss: 0.013077\n",
      "[176/01123] train_loss: 0.013888\n",
      "[176/01173] train_loss: 0.013898\n",
      "[176/01223] train_loss: 0.013792\n",
      "[177/00047] train_loss: 0.014614\n",
      "[177/00097] train_loss: 0.014293\n",
      "[177/00147] train_loss: 0.014845\n",
      "[177/00197] train_loss: 0.014500\n",
      "[177/00247] train_loss: 0.014282\n",
      "[177/00297] train_loss: 0.014093\n",
      "[177/00347] train_loss: 0.014088\n",
      "[177/00397] train_loss: 0.014577\n",
      "[177/00447] train_loss: 0.014017\n",
      "[177/00497] train_loss: 0.013289\n",
      "[177/00547] train_loss: 0.012996\n",
      "[177/00597] train_loss: 0.013980\n",
      "[177/00647] train_loss: 0.014005\n",
      "[177/00697] train_loss: 0.013386\n",
      "[177/00747] train_loss: 0.013762\n",
      "[177/00797] train_loss: 0.013551\n",
      "[177/00847] train_loss: 0.013787\n",
      "[177/00897] train_loss: 0.013334\n",
      "[177/00947] train_loss: 0.013439\n",
      "[177/00997] train_loss: 0.013184\n",
      "[177/01047] train_loss: 0.012764\n",
      "[177/01097] train_loss: 0.013570\n",
      "[177/01147] train_loss: 0.012845\n",
      "[177/01197] train_loss: 0.013761\n",
      "[178/00021] train_loss: 0.013656\n",
      "[178/00071] train_loss: 0.014782\n",
      "[178/00121] train_loss: 0.014583\n",
      "[178/00171] train_loss: 0.013566\n",
      "[178/00221] train_loss: 0.014739\n",
      "[178/00271] train_loss: 0.013823\n",
      "[178/00321] train_loss: 0.013448\n",
      "[178/00371] train_loss: 0.013340\n",
      "[178/00421] train_loss: 0.013629\n",
      "[178/00471] train_loss: 0.014031\n",
      "[178/00521] train_loss: 0.013253\n",
      "[178/00571] train_loss: 0.013718\n",
      "[178/00621] train_loss: 0.012881\n",
      "[178/00671] train_loss: 0.013424\n",
      "[178/00721] train_loss: 0.013378\n",
      "[178/00771] train_loss: 0.013460\n",
      "[178/00821] train_loss: 0.014089\n",
      "[178/00871] train_loss: 0.014151\n",
      "[178/00921] train_loss: 0.014166\n",
      "[178/00971] train_loss: 0.012812\n",
      "[178/01021] train_loss: 0.013478\n",
      "[178/01071] train_loss: 0.013594\n",
      "[178/01121] train_loss: 0.012841\n",
      "[178/01171] train_loss: 0.014226\n",
      "[178/01221] train_loss: 0.013502\n",
      "[179/00045] train_loss: 0.014141\n",
      "[179/00095] train_loss: 0.014912\n",
      "[179/00145] train_loss: 0.014581\n",
      "[179/00195] train_loss: 0.013420\n",
      "[179/00245] train_loss: 0.013915\n",
      "[179/00295] train_loss: 0.013489\n",
      "[179/00345] train_loss: 0.013928\n",
      "[179/00395] train_loss: 0.013743\n",
      "[179/00445] train_loss: 0.013854\n",
      "[179/00495] train_loss: 0.013269\n",
      "[179/00545] train_loss: 0.013541\n",
      "[179/00595] train_loss: 0.013764\n",
      "[179/00645] train_loss: 0.013760\n",
      "[179/00695] train_loss: 0.013632\n",
      "[179/00745] train_loss: 0.013553\n",
      "[179/00795] train_loss: 0.013898\n",
      "[179/00845] train_loss: 0.013081\n",
      "[179/00895] train_loss: 0.013219\n",
      "[179/00945] train_loss: 0.013295\n",
      "[179/00995] train_loss: 0.014305\n",
      "[179/01045] train_loss: 0.013269\n",
      "[179/01095] train_loss: 0.014032\n",
      "[179/01145] train_loss: 0.013117\n",
      "[179/01195] train_loss: 0.013302\n",
      "[180/00019] train_loss: 0.013356\n",
      "[180/00069] train_loss: 0.014617\n",
      "[180/00119] train_loss: 0.014029\n",
      "[180/00169] train_loss: 0.014135\n",
      "[180/00219] train_loss: 0.014058\n",
      "[180/00269] train_loss: 0.014005\n",
      "[180/00319] train_loss: 0.014098\n",
      "[180/00369] train_loss: 0.014158\n",
      "[180/00419] train_loss: 0.014235\n",
      "[180/00469] train_loss: 0.013299\n",
      "[180/00519] train_loss: 0.013656\n",
      "[180/00569] train_loss: 0.013323\n",
      "[180/00619] train_loss: 0.014117\n",
      "[180/00669] train_loss: 0.012802\n",
      "[180/00719] train_loss: 0.013529\n",
      "[180/00769] train_loss: 0.013680\n",
      "[180/00819] train_loss: 0.013319\n",
      "[180/00869] train_loss: 0.014015\n",
      "[180/00919] train_loss: 0.013596\n",
      "[180/00969] train_loss: 0.013366\n",
      "[180/01019] train_loss: 0.013245\n",
      "[180/01069] train_loss: 0.013745\n",
      "[180/01119] train_loss: 0.012906\n",
      "[180/01169] train_loss: 0.013594\n",
      "[180/01219] train_loss: 0.012347\n",
      "[181/00043] train_loss: 0.013973\n",
      "[181/00093] train_loss: 0.014347\n",
      "[181/00143] train_loss: 0.014525\n",
      "[181/00193] train_loss: 0.013980\n",
      "[181/00243] train_loss: 0.014756\n",
      "[181/00293] train_loss: 0.013940\n",
      "[181/00343] train_loss: 0.014156\n",
      "[181/00393] train_loss: 0.013789\n",
      "[181/00443] train_loss: 0.014042\n",
      "[181/00493] train_loss: 0.013245\n",
      "[181/00543] train_loss: 0.013186\n",
      "[181/00593] train_loss: 0.014364\n",
      "[181/00643] train_loss: 0.013061\n",
      "[181/00693] train_loss: 0.013828\n",
      "[181/00743] train_loss: 0.013617\n",
      "[181/00793] train_loss: 0.013271\n",
      "[181/00843] train_loss: 0.013373\n",
      "[181/00893] train_loss: 0.013329\n",
      "[181/00943] train_loss: 0.013864\n",
      "[181/00993] train_loss: 0.013909\n",
      "[181/01043] train_loss: 0.013193\n",
      "[181/01093] train_loss: 0.013591\n",
      "[181/01143] train_loss: 0.012985\n",
      "[181/01193] train_loss: 0.014076\n",
      "[182/00017] train_loss: 0.013559\n",
      "[182/00067] train_loss: 0.014853\n",
      "[182/00117] train_loss: 0.013484\n",
      "[182/00167] train_loss: 0.014410\n",
      "[182/00217] train_loss: 0.014241\n",
      "[182/00267] train_loss: 0.013796\n",
      "[182/00317] train_loss: 0.013694\n",
      "[182/00367] train_loss: 0.013592\n",
      "[182/00417] train_loss: 0.013415\n",
      "[182/00467] train_loss: 0.013640\n",
      "[182/00517] train_loss: 0.013732\n",
      "[182/00567] train_loss: 0.013413\n",
      "[182/00617] train_loss: 0.013468\n",
      "[182/00667] train_loss: 0.013279\n",
      "[182/00717] train_loss: 0.013418\n",
      "[182/00767] train_loss: 0.013664\n",
      "[182/00817] train_loss: 0.013081\n",
      "[182/00867] train_loss: 0.013329\n",
      "[182/00917] train_loss: 0.013764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[182/00967] train_loss: 0.013412\n",
      "[182/01017] train_loss: 0.013668\n",
      "[182/01067] train_loss: 0.013440\n",
      "[182/01117] train_loss: 0.013162\n",
      "[182/01167] train_loss: 0.013779\n",
      "[182/01217] train_loss: 0.013953\n",
      "[183/00041] train_loss: 0.014547\n",
      "[183/00091] train_loss: 0.014319\n",
      "[183/00141] train_loss: 0.015019\n",
      "[183/00191] train_loss: 0.014343\n",
      "[183/00241] train_loss: 0.013163\n",
      "[183/00291] train_loss: 0.013400\n",
      "[183/00341] train_loss: 0.013199\n",
      "[183/00391] train_loss: 0.013877\n",
      "[183/00441] train_loss: 0.013704\n",
      "[183/00491] train_loss: 0.013467\n",
      "[183/00541] train_loss: 0.014329\n",
      "[183/00591] train_loss: 0.013566\n",
      "[183/00641] train_loss: 0.013396\n",
      "[183/00691] train_loss: 0.013471\n",
      "[183/00741] train_loss: 0.012831\n",
      "[183/00791] train_loss: 0.013769\n",
      "[183/00841] train_loss: 0.013328\n",
      "[183/00891] train_loss: 0.014401\n",
      "[183/00941] train_loss: 0.013617\n",
      "[183/00991] train_loss: 0.013458\n",
      "[183/01041] train_loss: 0.013986\n",
      "[183/01091] train_loss: 0.013119\n",
      "[183/01141] train_loss: 0.013292\n",
      "[183/01191] train_loss: 0.013920\n",
      "[184/00015] train_loss: 0.013592\n",
      "[184/00065] train_loss: 0.015350\n",
      "[184/00115] train_loss: 0.014662\n",
      "[184/00165] train_loss: 0.014202\n",
      "[184/00215] train_loss: 0.013991\n",
      "[184/00265] train_loss: 0.013985\n",
      "[184/00315] train_loss: 0.013173\n",
      "[184/00365] train_loss: 0.013379\n",
      "[184/00415] train_loss: 0.013668\n",
      "[184/00465] train_loss: 0.013296\n",
      "[184/00515] train_loss: 0.013326\n",
      "[184/00565] train_loss: 0.013376\n",
      "[184/00615] train_loss: 0.013434\n",
      "[184/00665] train_loss: 0.013289\n",
      "[184/00715] train_loss: 0.013704\n",
      "[184/00765] train_loss: 0.013885\n",
      "[184/00815] train_loss: 0.013357\n",
      "[184/00865] train_loss: 0.014483\n",
      "[184/00915] train_loss: 0.013859\n",
      "[184/00965] train_loss: 0.013716\n",
      "[184/01015] train_loss: 0.013460\n",
      "[184/01065] train_loss: 0.012714\n",
      "[184/01115] train_loss: 0.013498\n",
      "[184/01165] train_loss: 0.012462\n",
      "[184/01215] train_loss: 0.012715\n",
      "[185/00039] train_loss: 0.014953\n",
      "[185/00089] train_loss: 0.014078\n",
      "[185/00139] train_loss: 0.014109\n",
      "[185/00189] train_loss: 0.013940\n",
      "[185/00239] train_loss: 0.014161\n",
      "[185/00289] train_loss: 0.013594\n",
      "[185/00339] train_loss: 0.013372\n",
      "[185/00389] train_loss: 0.014394\n",
      "[185/00439] train_loss: 0.013922\n",
      "[185/00489] train_loss: 0.013176\n",
      "[185/00539] train_loss: 0.013349\n",
      "[185/00589] train_loss: 0.014180\n",
      "[185/00639] train_loss: 0.013378\n",
      "[185/00689] train_loss: 0.013883\n",
      "[185/00739] train_loss: 0.013649\n",
      "[185/00789] train_loss: 0.013450\n",
      "[185/00839] train_loss: 0.013481\n",
      "[185/00889] train_loss: 0.013840\n",
      "[185/00939] train_loss: 0.013683\n",
      "[185/00989] train_loss: 0.013550\n",
      "[185/01039] train_loss: 0.013009\n",
      "[185/01089] train_loss: 0.012827\n",
      "[185/01139] train_loss: 0.013698\n",
      "[185/01189] train_loss: 0.012627\n",
      "[186/00013] train_loss: 0.014051\n",
      "[186/00063] train_loss: 0.013563\n",
      "[186/00113] train_loss: 0.015021\n",
      "[186/00163] train_loss: 0.013071\n",
      "[186/00213] train_loss: 0.013825\n",
      "[186/00263] train_loss: 0.014155\n",
      "[186/00313] train_loss: 0.013910\n",
      "[186/00363] train_loss: 0.013171\n",
      "[186/00413] train_loss: 0.013300\n",
      "[186/00463] train_loss: 0.013171\n",
      "[186/00513] train_loss: 0.013422\n",
      "[186/00563] train_loss: 0.013826\n",
      "[186/00613] train_loss: 0.013276\n",
      "[186/00663] train_loss: 0.013094\n",
      "[186/00713] train_loss: 0.013493\n",
      "[186/00763] train_loss: 0.013005\n",
      "[186/00813] train_loss: 0.014340\n",
      "[186/00863] train_loss: 0.013147\n",
      "[186/00913] train_loss: 0.014049\n",
      "[186/00963] train_loss: 0.013008\n",
      "[186/01013] train_loss: 0.012732\n",
      "[186/01063] train_loss: 0.013428\n",
      "[186/01113] train_loss: 0.013243\n",
      "[186/01163] train_loss: 0.013785\n",
      "[186/01213] train_loss: 0.012841\n",
      "[187/00037] train_loss: 0.014306\n",
      "[187/00087] train_loss: 0.015358\n",
      "[187/00137] train_loss: 0.013917\n",
      "[187/00187] train_loss: 0.013638\n",
      "[187/00237] train_loss: 0.013801\n",
      "[187/00287] train_loss: 0.014348\n",
      "[187/00337] train_loss: 0.013267\n",
      "[187/00387] train_loss: 0.014320\n",
      "[187/00437] train_loss: 0.013212\n",
      "[187/00487] train_loss: 0.013886\n",
      "[187/00537] train_loss: 0.014130\n",
      "[187/00587] train_loss: 0.013327\n",
      "[187/00637] train_loss: 0.013617\n",
      "[187/00687] train_loss: 0.013244\n",
      "[187/00737] train_loss: 0.012329\n",
      "[187/00787] train_loss: 0.013196\n",
      "[187/00837] train_loss: 0.013216\n",
      "[187/00887] train_loss: 0.013521\n",
      "[187/00937] train_loss: 0.014060\n",
      "[187/00987] train_loss: 0.013318\n",
      "[187/01037] train_loss: 0.013418\n",
      "[187/01087] train_loss: 0.013835\n",
      "[187/01137] train_loss: 0.012949\n",
      "[187/01187] train_loss: 0.013352\n",
      "[188/00011] train_loss: 0.013289\n",
      "[188/00061] train_loss: 0.014820\n",
      "[188/00111] train_loss: 0.014239\n",
      "[188/00161] train_loss: 0.014277\n",
      "[188/00211] train_loss: 0.014810\n",
      "[188/00261] train_loss: 0.013831\n",
      "[188/00311] train_loss: 0.013372\n",
      "[188/00361] train_loss: 0.013611\n",
      "[188/00411] train_loss: 0.013930\n",
      "[188/00461] train_loss: 0.013139\n",
      "[188/00511] train_loss: 0.013461\n",
      "[188/00561] train_loss: 0.013381\n",
      "[188/00611] train_loss: 0.012882\n",
      "[188/00661] train_loss: 0.013238\n",
      "[188/00711] train_loss: 0.014110\n",
      "[188/00761] train_loss: 0.012994\n",
      "[188/00811] train_loss: 0.012812\n",
      "[188/00861] train_loss: 0.013233\n",
      "[188/00911] train_loss: 0.013610\n",
      "[188/00961] train_loss: 0.013200\n",
      "[188/01011] train_loss: 0.013505\n",
      "[188/01061] train_loss: 0.013628\n",
      "[188/01111] train_loss: 0.013473\n",
      "[188/01161] train_loss: 0.013828\n",
      "[188/01211] train_loss: 0.013224\n",
      "[189/00035] train_loss: 0.014856\n",
      "[189/00085] train_loss: 0.013967\n",
      "[189/00135] train_loss: 0.014076\n",
      "[189/00185] train_loss: 0.014917\n",
      "[189/00235] train_loss: 0.014543\n",
      "[189/00285] train_loss: 0.014748\n",
      "[189/00335] train_loss: 0.014174\n",
      "[189/00385] train_loss: 0.013716\n",
      "[189/00435] train_loss: 0.013297\n",
      "[189/00485] train_loss: 0.013213\n",
      "[189/00535] train_loss: 0.013174\n",
      "[189/00585] train_loss: 0.013123\n",
      "[189/00635] train_loss: 0.013478\n",
      "[189/00685] train_loss: 0.013129\n",
      "[189/00735] train_loss: 0.012791\n",
      "[189/00785] train_loss: 0.014218\n",
      "[189/00835] train_loss: 0.013901\n",
      "[189/00885] train_loss: 0.013450\n",
      "[189/00935] train_loss: 0.013546\n",
      "[189/00985] train_loss: 0.013218\n",
      "[189/01035] train_loss: 0.012745\n",
      "[189/01085] train_loss: 0.012808\n",
      "[189/01135] train_loss: 0.013240\n",
      "[189/01185] train_loss: 0.013296\n",
      "[190/00009] train_loss: 0.013740\n",
      "[190/00059] train_loss: 0.014414\n",
      "[190/00109] train_loss: 0.013569\n",
      "[190/00159] train_loss: 0.014388\n",
      "[190/00209] train_loss: 0.014176\n",
      "[190/00259] train_loss: 0.013847\n",
      "[190/00309] train_loss: 0.013828\n",
      "[190/00359] train_loss: 0.013206\n",
      "[190/00409] train_loss: 0.013245\n",
      "[190/00459] train_loss: 0.013328\n",
      "[190/00509] train_loss: 0.014036\n",
      "[190/00559] train_loss: 0.013777\n",
      "[190/00609] train_loss: 0.013606\n",
      "[190/00659] train_loss: 0.013276\n",
      "[190/00709] train_loss: 0.013519\n",
      "[190/00759] train_loss: 0.013078\n",
      "[190/00809] train_loss: 0.012906\n",
      "[190/00859] train_loss: 0.013288\n",
      "[190/00909] train_loss: 0.014497\n",
      "[190/00959] train_loss: 0.013625\n",
      "[190/01009] train_loss: 0.014277\n",
      "[190/01059] train_loss: 0.013615\n",
      "[190/01109] train_loss: 0.012738\n",
      "[190/01159] train_loss: 0.012771\n",
      "[190/01209] train_loss: 0.013814\n",
      "[191/00033] train_loss: 0.014052\n",
      "[191/00083] train_loss: 0.014453\n",
      "[191/00133] train_loss: 0.013757\n",
      "[191/00183] train_loss: 0.014184\n",
      "[191/00233] train_loss: 0.013444\n",
      "[191/00283] train_loss: 0.013288\n",
      "[191/00333] train_loss: 0.013697\n",
      "[191/00383] train_loss: 0.012511\n",
      "[191/00433] train_loss: 0.013643\n",
      "[191/00483] train_loss: 0.013410\n",
      "[191/00533] train_loss: 0.013242\n",
      "[191/00583] train_loss: 0.013736\n",
      "[191/00633] train_loss: 0.014067\n",
      "[191/00683] train_loss: 0.013338\n",
      "[191/00733] train_loss: 0.013396\n",
      "[191/00783] train_loss: 0.013747\n",
      "[191/00833] train_loss: 0.012851\n",
      "[191/00883] train_loss: 0.014224\n",
      "[191/00933] train_loss: 0.013273\n",
      "[191/00983] train_loss: 0.013472\n",
      "[191/01033] train_loss: 0.013111\n",
      "[191/01083] train_loss: 0.013342\n",
      "[191/01133] train_loss: 0.013583\n",
      "[191/01183] train_loss: 0.013773\n",
      "[192/00007] train_loss: 0.013383\n",
      "[192/00057] train_loss: 0.013895\n",
      "[192/00107] train_loss: 0.014499\n",
      "[192/00157] train_loss: 0.013687\n",
      "[192/00207] train_loss: 0.014520\n",
      "[192/00257] train_loss: 0.014070\n",
      "[192/00307] train_loss: 0.013545\n",
      "[192/00357] train_loss: 0.013368\n",
      "[192/00407] train_loss: 0.013780\n",
      "[192/00457] train_loss: 0.013853\n",
      "[192/00507] train_loss: 0.013020\n",
      "[192/00557] train_loss: 0.014125\n",
      "[192/00607] train_loss: 0.013820\n",
      "[192/00657] train_loss: 0.014024\n",
      "[192/00707] train_loss: 0.013781\n",
      "[192/00757] train_loss: 0.014445\n",
      "[192/00807] train_loss: 0.013429\n",
      "[192/00857] train_loss: 0.013500\n",
      "[192/00907] train_loss: 0.013258\n",
      "[192/00957] train_loss: 0.012907\n",
      "[192/01007] train_loss: 0.012996\n",
      "[192/01057] train_loss: 0.013287\n",
      "[192/01107] train_loss: 0.013130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[192/01157] train_loss: 0.013072\n",
      "[192/01207] train_loss: 0.012292\n",
      "[193/00031] train_loss: 0.014264\n",
      "[193/00081] train_loss: 0.014028\n",
      "[193/00131] train_loss: 0.013896\n",
      "[193/00181] train_loss: 0.014564\n",
      "[193/00231] train_loss: 0.013729\n",
      "[193/00281] train_loss: 0.013479\n",
      "[193/00331] train_loss: 0.013766\n",
      "[193/00381] train_loss: 0.014128\n",
      "[193/00431] train_loss: 0.014000\n",
      "[193/00481] train_loss: 0.014092\n",
      "[193/00531] train_loss: 0.013515\n",
      "[193/00581] train_loss: 0.013472\n",
      "[193/00631] train_loss: 0.012916\n",
      "[193/00681] train_loss: 0.012811\n",
      "[193/00731] train_loss: 0.012852\n",
      "[193/00781] train_loss: 0.014193\n",
      "[193/00831] train_loss: 0.014493\n",
      "[193/00881] train_loss: 0.013396\n",
      "[193/00931] train_loss: 0.013282\n",
      "[193/00981] train_loss: 0.012801\n",
      "[193/01031] train_loss: 0.013405\n",
      "[193/01081] train_loss: 0.013602\n",
      "[193/01131] train_loss: 0.012621\n",
      "[193/01181] train_loss: 0.013198\n",
      "[194/00005] train_loss: 0.012871\n",
      "[194/00055] train_loss: 0.013235\n",
      "[194/00105] train_loss: 0.014721\n",
      "[194/00155] train_loss: 0.013948\n",
      "[194/00205] train_loss: 0.013662\n",
      "[194/00255] train_loss: 0.013579\n",
      "[194/00305] train_loss: 0.013912\n",
      "[194/00355] train_loss: 0.013454\n",
      "[194/00405] train_loss: 0.013753\n",
      "[194/00455] train_loss: 0.013710\n",
      "[194/00505] train_loss: 0.012721\n",
      "[194/00555] train_loss: 0.013485\n",
      "[194/00605] train_loss: 0.013774\n",
      "[194/00655] train_loss: 0.014121\n",
      "[194/00705] train_loss: 0.013297\n",
      "[194/00755] train_loss: 0.013861\n",
      "[194/00805] train_loss: 0.013143\n",
      "[194/00855] train_loss: 0.013824\n",
      "[194/00905] train_loss: 0.013341\n",
      "[194/00955] train_loss: 0.012830\n",
      "[194/01005] train_loss: 0.013658\n",
      "[194/01055] train_loss: 0.013098\n",
      "[194/01105] train_loss: 0.013473\n",
      "[194/01155] train_loss: 0.013007\n",
      "[194/01205] train_loss: 0.013911\n",
      "[195/00029] train_loss: 0.014429\n",
      "[195/00079] train_loss: 0.014570\n",
      "[195/00129] train_loss: 0.014732\n",
      "[195/00179] train_loss: 0.013777\n",
      "[195/00229] train_loss: 0.013891\n",
      "[195/00279] train_loss: 0.014176\n",
      "[195/00329] train_loss: 0.013492\n",
      "[195/00379] train_loss: 0.013106\n",
      "[195/00429] train_loss: 0.014190\n",
      "[195/00479] train_loss: 0.012980\n",
      "[195/00529] train_loss: 0.012802\n",
      "[195/00579] train_loss: 0.013054\n",
      "[195/00629] train_loss: 0.012058\n",
      "[195/00679] train_loss: 0.013589\n",
      "[195/00729] train_loss: 0.012792\n",
      "[195/00779] train_loss: 0.012663\n",
      "[195/00829] train_loss: 0.013379\n",
      "[195/00879] train_loss: 0.013430\n",
      "[195/00929] train_loss: 0.014106\n",
      "[195/00979] train_loss: 0.012841\n",
      "[195/01029] train_loss: 0.013671\n",
      "[195/01079] train_loss: 0.014108\n",
      "[195/01129] train_loss: 0.013052\n",
      "[195/01179] train_loss: 0.012774\n",
      "[196/00003] train_loss: 0.013605\n",
      "[196/00053] train_loss: 0.014675\n",
      "[196/00103] train_loss: 0.014062\n",
      "[196/00153] train_loss: 0.014051\n",
      "[196/00203] train_loss: 0.014237\n",
      "[196/00253] train_loss: 0.014182\n",
      "[196/00303] train_loss: 0.013202\n",
      "[196/00353] train_loss: 0.013759\n",
      "[196/00403] train_loss: 0.013293\n",
      "[196/00453] train_loss: 0.013523\n",
      "[196/00503] train_loss: 0.013228\n",
      "[196/00553] train_loss: 0.013237\n",
      "[196/00603] train_loss: 0.013663\n",
      "[196/00653] train_loss: 0.013279\n",
      "[196/00703] train_loss: 0.013310\n",
      "[196/00753] train_loss: 0.013420\n",
      "[196/00803] train_loss: 0.013401\n",
      "[196/00853] train_loss: 0.013491\n",
      "[196/00903] train_loss: 0.012916\n",
      "[196/00953] train_loss: 0.013408\n",
      "[196/01003] train_loss: 0.013308\n",
      "[196/01053] train_loss: 0.013014\n",
      "[196/01103] train_loss: 0.013515\n",
      "[196/01153] train_loss: 0.013760\n",
      "[196/01203] train_loss: 0.013342\n",
      "[197/00027] train_loss: 0.013322\n",
      "[197/00077] train_loss: 0.014668\n",
      "[197/00127] train_loss: 0.013454\n",
      "[197/00177] train_loss: 0.013507\n",
      "[197/00227] train_loss: 0.014463\n",
      "[197/00277] train_loss: 0.014086\n",
      "[197/00327] train_loss: 0.013712\n",
      "[197/00377] train_loss: 0.013884\n",
      "[197/00427] train_loss: 0.013493\n",
      "[197/00477] train_loss: 0.014055\n",
      "[197/00527] train_loss: 0.013092\n",
      "[197/00577] train_loss: 0.013046\n",
      "[197/00627] train_loss: 0.013327\n",
      "[197/00677] train_loss: 0.013786\n",
      "[197/00727] train_loss: 0.013925\n",
      "[197/00777] train_loss: 0.013318\n",
      "[197/00827] train_loss: 0.013464\n",
      "[197/00877] train_loss: 0.013052\n",
      "[197/00927] train_loss: 0.013159\n",
      "[197/00977] train_loss: 0.013215\n",
      "[197/01027] train_loss: 0.012817\n",
      "[197/01077] train_loss: 0.013649\n",
      "[197/01127] train_loss: 0.013102\n",
      "[197/01177] train_loss: 0.013029\n",
      "[198/00001] train_loss: 0.013031\n",
      "[198/00051] train_loss: 0.014235\n",
      "[198/00101] train_loss: 0.014400\n",
      "[198/00151] train_loss: 0.013821\n",
      "[198/00201] train_loss: 0.012840\n",
      "[198/00251] train_loss: 0.013477\n",
      "[198/00301] train_loss: 0.013926\n",
      "[198/00351] train_loss: 0.013123\n",
      "[198/00401] train_loss: 0.013477\n",
      "[198/00451] train_loss: 0.013618\n",
      "[198/00501] train_loss: 0.014212\n",
      "[198/00551] train_loss: 0.013349\n",
      "[198/00601] train_loss: 0.013366\n",
      "[198/00651] train_loss: 0.013509\n",
      "[198/00701] train_loss: 0.012934\n",
      "[198/00751] train_loss: 0.014190\n",
      "[198/00801] train_loss: 0.013760\n",
      "[198/00851] train_loss: 0.012522\n",
      "[198/00901] train_loss: 0.013573\n",
      "[198/00951] train_loss: 0.013264\n",
      "[198/01001] train_loss: 0.013217\n",
      "[198/01051] train_loss: 0.012786\n",
      "[198/01101] train_loss: 0.013486\n",
      "[198/01151] train_loss: 0.013304\n",
      "[198/01201] train_loss: 0.013798\n",
      "[199/00025] train_loss: 0.014075\n",
      "[199/00075] train_loss: 0.014940\n",
      "[199/00125] train_loss: 0.014137\n",
      "[199/00175] train_loss: 0.013360\n",
      "[199/00225] train_loss: 0.014900\n",
      "[199/00275] train_loss: 0.014091\n",
      "[199/00325] train_loss: 0.013443\n",
      "[199/00375] train_loss: 0.014117\n",
      "[199/00425] train_loss: 0.012958\n",
      "[199/00475] train_loss: 0.014210\n",
      "[199/00525] train_loss: 0.014109\n",
      "[199/00575] train_loss: 0.014069\n",
      "[199/00625] train_loss: 0.012586\n",
      "[199/00675] train_loss: 0.013566\n",
      "[199/00725] train_loss: 0.013373\n",
      "[199/00775] train_loss: 0.013756\n",
      "[199/00825] train_loss: 0.013261\n",
      "[199/00875] train_loss: 0.012871\n",
      "[199/00925] train_loss: 0.013618\n",
      "[199/00975] train_loss: 0.012631\n",
      "[199/01025] train_loss: 0.013662\n",
      "[199/01075] train_loss: 0.012697\n",
      "[199/01125] train_loss: 0.011902\n",
      "[199/01175] train_loss: 0.012797\n",
      "[199/01225] train_loss: 0.012749\n",
      "[200/00049] train_loss: 0.014988\n",
      "[200/00099] train_loss: 0.013335\n",
      "[200/00149] train_loss: 0.015360\n",
      "[200/00199] train_loss: 0.013807\n",
      "[200/00249] train_loss: 0.013908\n",
      "[200/00299] train_loss: 0.013765\n",
      "[200/00349] train_loss: 0.014551\n",
      "[200/00399] train_loss: 0.013699\n",
      "[200/00449] train_loss: 0.013269\n",
      "[200/00499] train_loss: 0.013915\n",
      "[200/00549] train_loss: 0.013387\n",
      "[200/00599] train_loss: 0.012802\n",
      "[200/00649] train_loss: 0.013109\n",
      "[200/00699] train_loss: 0.013350\n",
      "[200/00749] train_loss: 0.012921\n",
      "[200/00799] train_loss: 0.013200\n",
      "[200/00849] train_loss: 0.013201\n",
      "[200/00899] train_loss: 0.012722\n",
      "[200/00949] train_loss: 0.014105\n",
      "[200/00999] train_loss: 0.013488\n",
      "[200/01049] train_loss: 0.013668\n",
      "[200/01099] train_loss: 0.013189\n",
      "[200/01149] train_loss: 0.013587\n",
      "[200/01199] train_loss: 0.012800\n",
      "[201/00023] train_loss: 0.014191\n",
      "[201/00073] train_loss: 0.014327\n",
      "[201/00123] train_loss: 0.013779\n",
      "[201/00173] train_loss: 0.013324\n",
      "[201/00223] train_loss: 0.013451\n",
      "[201/00273] train_loss: 0.013483\n",
      "[201/00323] train_loss: 0.013331\n",
      "[201/00373] train_loss: 0.013687\n",
      "[201/00423] train_loss: 0.012876\n",
      "[201/00473] train_loss: 0.013743\n",
      "[201/00523] train_loss: 0.013118\n",
      "[201/00573] train_loss: 0.013396\n",
      "[201/00623] train_loss: 0.013007\n",
      "[201/00673] train_loss: 0.013280\n",
      "[201/00723] train_loss: 0.013581\n",
      "[201/00773] train_loss: 0.013551\n",
      "[201/00823] train_loss: 0.013528\n",
      "[201/00873] train_loss: 0.012763\n",
      "[201/00923] train_loss: 0.012879\n",
      "[201/00973] train_loss: 0.012986\n",
      "[201/01023] train_loss: 0.013497\n",
      "[201/01073] train_loss: 0.013489\n",
      "[201/01123] train_loss: 0.012666\n",
      "[201/01173] train_loss: 0.013797\n",
      "[201/01223] train_loss: 0.012964\n",
      "[202/00047] train_loss: 0.014470\n",
      "[202/00097] train_loss: 0.013899\n",
      "[202/00147] train_loss: 0.013870\n",
      "[202/00197] train_loss: 0.014175\n",
      "[202/00247] train_loss: 0.014093\n",
      "[202/00297] train_loss: 0.012898\n",
      "[202/00347] train_loss: 0.014002\n",
      "[202/00397] train_loss: 0.012854\n",
      "[202/00447] train_loss: 0.013081\n",
      "[202/00497] train_loss: 0.013239\n",
      "[202/00547] train_loss: 0.013748\n",
      "[202/00597] train_loss: 0.013699\n",
      "[202/00647] train_loss: 0.013635\n",
      "[202/00697] train_loss: 0.012667\n",
      "[202/00747] train_loss: 0.013129\n",
      "[202/00797] train_loss: 0.013351\n",
      "[202/00847] train_loss: 0.013758\n",
      "[202/00897] train_loss: 0.012927\n",
      "[202/00947] train_loss: 0.014068\n",
      "[202/00997] train_loss: 0.012858\n",
      "[202/01047] train_loss: 0.014353\n",
      "[202/01097] train_loss: 0.013196\n",
      "[202/01147] train_loss: 0.013190\n",
      "[202/01197] train_loss: 0.012523\n",
      "[203/00021] train_loss: 0.013480\n",
      "[203/00071] train_loss: 0.014196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[203/00121] train_loss: 0.014636\n",
      "[203/00171] train_loss: 0.014435\n",
      "[203/00221] train_loss: 0.013423\n",
      "[203/00271] train_loss: 0.013870\n",
      "[203/00321] train_loss: 0.013641\n",
      "[203/00371] train_loss: 0.013283\n",
      "[203/00421] train_loss: 0.013178\n",
      "[203/00471] train_loss: 0.013047\n",
      "[203/00521] train_loss: 0.012868\n",
      "[203/00571] train_loss: 0.013403\n",
      "[203/00621] train_loss: 0.012577\n",
      "[203/00671] train_loss: 0.013633\n",
      "[203/00721] train_loss: 0.013427\n",
      "[203/00771] train_loss: 0.013877\n",
      "[203/00821] train_loss: 0.012634\n",
      "[203/00871] train_loss: 0.013067\n",
      "[203/00921] train_loss: 0.013450\n",
      "[203/00971] train_loss: 0.012640\n",
      "[203/01021] train_loss: 0.013725\n",
      "[203/01071] train_loss: 0.013446\n",
      "[203/01121] train_loss: 0.012738\n",
      "[203/01171] train_loss: 0.012184\n",
      "[203/01221] train_loss: 0.013488\n",
      "[204/00045] train_loss: 0.014284\n",
      "[204/00095] train_loss: 0.014249\n",
      "[204/00145] train_loss: 0.014966\n",
      "[204/00195] train_loss: 0.013534\n",
      "[204/00245] train_loss: 0.013209\n",
      "[204/00295] train_loss: 0.013478\n",
      "[204/00345] train_loss: 0.013031\n",
      "[204/00395] train_loss: 0.013815\n",
      "[204/00445] train_loss: 0.013586\n",
      "[204/00495] train_loss: 0.013858\n",
      "[204/00545] train_loss: 0.012699\n",
      "[204/00595] train_loss: 0.013540\n",
      "[204/00645] train_loss: 0.013630\n",
      "[204/00695] train_loss: 0.013326\n",
      "[204/00745] train_loss: 0.013386\n",
      "[204/00795] train_loss: 0.013883\n",
      "[204/00845] train_loss: 0.013420\n",
      "[204/00895] train_loss: 0.013061\n",
      "[204/00945] train_loss: 0.012226\n",
      "[204/00995] train_loss: 0.013153\n",
      "[204/01045] train_loss: 0.013175\n",
      "[204/01095] train_loss: 0.012842\n",
      "[204/01145] train_loss: 0.013528\n",
      "[204/01195] train_loss: 0.013683\n",
      "[205/00019] train_loss: 0.013758\n",
      "[205/00069] train_loss: 0.014147\n",
      "[205/00119] train_loss: 0.013579\n",
      "[205/00169] train_loss: 0.013895\n",
      "[205/00219] train_loss: 0.014202\n",
      "[205/00269] train_loss: 0.013112\n",
      "[205/00319] train_loss: 0.014272\n",
      "[205/00369] train_loss: 0.013362\n",
      "[205/00419] train_loss: 0.013112\n",
      "[205/00469] train_loss: 0.013008\n",
      "[205/00519] train_loss: 0.013360\n",
      "[205/00569] train_loss: 0.013432\n",
      "[205/00619] train_loss: 0.013363\n",
      "[205/00669] train_loss: 0.013162\n",
      "[205/00719] train_loss: 0.013700\n",
      "[205/00769] train_loss: 0.013358\n",
      "[205/00819] train_loss: 0.013791\n",
      "[205/00869] train_loss: 0.013460\n",
      "[205/00919] train_loss: 0.013308\n",
      "[205/00969] train_loss: 0.012910\n",
      "[205/01019] train_loss: 0.012976\n",
      "[205/01069] train_loss: 0.012550\n",
      "[205/01119] train_loss: 0.013199\n",
      "[205/01169] train_loss: 0.012872\n",
      "[205/01219] train_loss: 0.012870\n",
      "[206/00043] train_loss: 0.014435\n",
      "[206/00093] train_loss: 0.013743\n",
      "[206/00143] train_loss: 0.014946\n",
      "[206/00193] train_loss: 0.013976\n",
      "[206/00243] train_loss: 0.013647\n",
      "[206/00293] train_loss: 0.012987\n",
      "[206/00343] train_loss: 0.013182\n",
      "[206/00393] train_loss: 0.013203\n",
      "[206/00443] train_loss: 0.013848\n",
      "[206/00493] train_loss: 0.012877\n",
      "[206/00543] train_loss: 0.013772\n",
      "[206/00593] train_loss: 0.013465\n",
      "[206/00643] train_loss: 0.014016\n",
      "[206/00693] train_loss: 0.012793\n",
      "[206/00743] train_loss: 0.013459\n",
      "[206/00793] train_loss: 0.012758\n",
      "[206/00843] train_loss: 0.013559\n",
      "[206/00893] train_loss: 0.012913\n",
      "[206/00943] train_loss: 0.013356\n",
      "[206/00993] train_loss: 0.012817\n",
      "[206/01043] train_loss: 0.013553\n",
      "[206/01093] train_loss: 0.013055\n",
      "[206/01143] train_loss: 0.012989\n",
      "[206/01193] train_loss: 0.013283\n",
      "[207/00017] train_loss: 0.013479\n",
      "[207/00067] train_loss: 0.013814\n",
      "[207/00117] train_loss: 0.013434\n",
      "[207/00167] train_loss: 0.014107\n",
      "[207/00217] train_loss: 0.013797\n",
      "[207/00267] train_loss: 0.014396\n",
      "[207/00317] train_loss: 0.013676\n",
      "[207/00367] train_loss: 0.013330\n",
      "[207/00417] train_loss: 0.013181\n",
      "[207/00467] train_loss: 0.012917\n",
      "[207/00517] train_loss: 0.013268\n",
      "[207/00567] train_loss: 0.012409\n",
      "[207/00617] train_loss: 0.013751\n",
      "[207/00667] train_loss: 0.013103\n",
      "[207/00717] train_loss: 0.013355\n",
      "[207/00767] train_loss: 0.013255\n",
      "[207/00817] train_loss: 0.014575\n",
      "[207/00867] train_loss: 0.012963\n",
      "[207/00917] train_loss: 0.013021\n",
      "[207/00967] train_loss: 0.013734\n",
      "[207/01017] train_loss: 0.013331\n",
      "[207/01067] train_loss: 0.012811\n",
      "[207/01117] train_loss: 0.013293\n",
      "[207/01167] train_loss: 0.012948\n",
      "[207/01217] train_loss: 0.013110\n",
      "[208/00041] train_loss: 0.014207\n",
      "[208/00091] train_loss: 0.013493\n",
      "[208/00141] train_loss: 0.013819\n",
      "[208/00191] train_loss: 0.013737\n",
      "[208/00241] train_loss: 0.013845\n",
      "[208/00291] train_loss: 0.013735\n",
      "[208/00341] train_loss: 0.014288\n",
      "[208/00391] train_loss: 0.013581\n",
      "[208/00441] train_loss: 0.013551\n",
      "[208/00491] train_loss: 0.012943\n",
      "[208/00541] train_loss: 0.013610\n",
      "[208/00591] train_loss: 0.013022\n",
      "[208/00641] train_loss: 0.012856\n",
      "[208/00691] train_loss: 0.013159\n",
      "[208/00741] train_loss: 0.012143\n",
      "[208/00791] train_loss: 0.013521\n",
      "[208/00841] train_loss: 0.013533\n",
      "[208/00891] train_loss: 0.012850\n",
      "[208/00941] train_loss: 0.013557\n",
      "[208/00991] train_loss: 0.013756\n",
      "[208/01041] train_loss: 0.013085\n",
      "[208/01091] train_loss: 0.013203\n",
      "[208/01141] train_loss: 0.013127\n",
      "[208/01191] train_loss: 0.013168\n",
      "[209/00015] train_loss: 0.013800\n",
      "[209/00065] train_loss: 0.014102\n",
      "[209/00115] train_loss: 0.013679\n",
      "[209/00165] train_loss: 0.013208\n",
      "[209/00215] train_loss: 0.013922\n",
      "[209/00265] train_loss: 0.014078\n",
      "[209/00315] train_loss: 0.013362\n",
      "[209/00365] train_loss: 0.012773\n",
      "[209/00415] train_loss: 0.013237\n",
      "[209/00465] train_loss: 0.012473\n",
      "[209/00515] train_loss: 0.013626\n",
      "[209/00565] train_loss: 0.013611\n",
      "[209/00615] train_loss: 0.012949\n",
      "[209/00665] train_loss: 0.013857\n",
      "[209/00715] train_loss: 0.013560\n",
      "[209/00765] train_loss: 0.012986\n",
      "[209/00815] train_loss: 0.013586\n",
      "[209/00865] train_loss: 0.013009\n",
      "[209/00915] train_loss: 0.012722\n",
      "[209/00965] train_loss: 0.013028\n",
      "[209/01015] train_loss: 0.013445\n",
      "[209/01065] train_loss: 0.013407\n",
      "[209/01115] train_loss: 0.013055\n",
      "[209/01165] train_loss: 0.014464\n",
      "[209/01215] train_loss: 0.013338\n",
      "[210/00039] train_loss: 0.014967\n",
      "[210/00089] train_loss: 0.014473\n",
      "[210/00139] train_loss: 0.014849\n",
      "[210/00189] train_loss: 0.013201\n",
      "[210/00239] train_loss: 0.012894\n",
      "[210/00289] train_loss: 0.013996\n",
      "[210/00339] train_loss: 0.013794\n",
      "[210/00389] train_loss: 0.013041\n",
      "[210/00439] train_loss: 0.012870\n",
      "[210/00489] train_loss: 0.012889\n",
      "[210/00539] train_loss: 0.013570\n",
      "[210/00589] train_loss: 0.012406\n",
      "[210/00639] train_loss: 0.013484\n",
      "[210/00689] train_loss: 0.013714\n",
      "[210/00739] train_loss: 0.013291\n",
      "[210/00789] train_loss: 0.012325\n",
      "[210/00839] train_loss: 0.013647\n",
      "[210/00889] train_loss: 0.013246\n",
      "[210/00939] train_loss: 0.012828\n",
      "[210/00989] train_loss: 0.013323\n",
      "[210/01039] train_loss: 0.013017\n",
      "[210/01089] train_loss: 0.013054\n",
      "[210/01139] train_loss: 0.012961\n",
      "[210/01189] train_loss: 0.013597\n",
      "[211/00013] train_loss: 0.013511\n",
      "[211/00063] train_loss: 0.013942\n",
      "[211/00113] train_loss: 0.013867\n",
      "[211/00163] train_loss: 0.013838\n",
      "[211/00213] train_loss: 0.013859\n",
      "[211/00263] train_loss: 0.013629\n",
      "[211/00313] train_loss: 0.012622\n",
      "[211/00363] train_loss: 0.013259\n",
      "[211/00413] train_loss: 0.014389\n",
      "[211/00463] train_loss: 0.012689\n",
      "[211/00513] train_loss: 0.013201\n",
      "[211/00563] train_loss: 0.013558\n",
      "[211/00613] train_loss: 0.013434\n",
      "[211/00663] train_loss: 0.013355\n",
      "[211/00713] train_loss: 0.013158\n",
      "[211/00763] train_loss: 0.013072\n",
      "[211/00813] train_loss: 0.012869\n",
      "[211/00863] train_loss: 0.014397\n",
      "[211/00913] train_loss: 0.012881\n",
      "[211/00963] train_loss: 0.012736\n",
      "[211/01013] train_loss: 0.012014\n",
      "[211/01063] train_loss: 0.012604\n",
      "[211/01113] train_loss: 0.014281\n",
      "[211/01163] train_loss: 0.012951\n",
      "[211/01213] train_loss: 0.013075\n",
      "[212/00037] train_loss: 0.014297\n",
      "[212/00087] train_loss: 0.014439\n",
      "[212/00137] train_loss: 0.013945\n",
      "[212/00187] train_loss: 0.013877\n",
      "[212/00237] train_loss: 0.013816\n",
      "[212/00287] train_loss: 0.013454\n",
      "[212/00337] train_loss: 0.013390\n",
      "[212/00387] train_loss: 0.013739\n",
      "[212/00437] train_loss: 0.013788\n",
      "[212/00487] train_loss: 0.012710\n",
      "[212/00537] train_loss: 0.013854\n",
      "[212/00587] train_loss: 0.013059\n",
      "[212/00637] train_loss: 0.014026\n",
      "[212/00687] train_loss: 0.012943\n",
      "[212/00737] train_loss: 0.012319\n",
      "[212/00787] train_loss: 0.012826\n",
      "[212/00837] train_loss: 0.012751\n",
      "[212/00887] train_loss: 0.013248\n",
      "[212/00937] train_loss: 0.013937\n",
      "[212/00987] train_loss: 0.012920\n",
      "[212/01037] train_loss: 0.013194\n",
      "[212/01087] train_loss: 0.013292\n",
      "[212/01137] train_loss: 0.012476\n",
      "[212/01187] train_loss: 0.013084\n",
      "[213/00011] train_loss: 0.013645\n",
      "[213/00061] train_loss: 0.014864\n",
      "[213/00111] train_loss: 0.013633\n",
      "[213/00161] train_loss: 0.014325\n",
      "[213/00211] train_loss: 0.013450\n",
      "[213/00261] train_loss: 0.014452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[213/00311] train_loss: 0.013775\n",
      "[213/00361] train_loss: 0.013429\n",
      "[213/00411] train_loss: 0.013583\n",
      "[213/00461] train_loss: 0.013599\n",
      "[213/00511] train_loss: 0.012705\n",
      "[213/00561] train_loss: 0.013081\n",
      "[213/00611] train_loss: 0.013145\n",
      "[213/00661] train_loss: 0.012954\n",
      "[213/00711] train_loss: 0.013151\n",
      "[213/00761] train_loss: 0.012629\n",
      "[213/00811] train_loss: 0.012673\n",
      "[213/00861] train_loss: 0.012525\n",
      "[213/00911] train_loss: 0.013050\n",
      "[213/00961] train_loss: 0.012758\n",
      "[213/01011] train_loss: 0.013274\n",
      "[213/01061] train_loss: 0.012817\n",
      "[213/01111] train_loss: 0.013681\n",
      "[213/01161] train_loss: 0.012977\n",
      "[213/01211] train_loss: 0.013066\n",
      "[214/00035] train_loss: 0.013908\n",
      "[214/00085] train_loss: 0.014090\n",
      "[214/00135] train_loss: 0.013331\n",
      "[214/00185] train_loss: 0.013802\n",
      "[214/00235] train_loss: 0.013134\n",
      "[214/00285] train_loss: 0.013249\n",
      "[214/00335] train_loss: 0.014505\n",
      "[214/00385] train_loss: 0.012931\n",
      "[214/00435] train_loss: 0.012768\n",
      "[214/00485] train_loss: 0.013008\n",
      "[214/00535] train_loss: 0.012759\n",
      "[214/00585] train_loss: 0.012799\n",
      "[214/00635] train_loss: 0.013355\n",
      "[214/00685] train_loss: 0.012770\n",
      "[214/00735] train_loss: 0.014247\n",
      "[214/00785] train_loss: 0.013355\n",
      "[214/00835] train_loss: 0.012430\n",
      "[214/00885] train_loss: 0.013710\n",
      "[214/00935] train_loss: 0.013086\n",
      "[214/00985] train_loss: 0.013095\n",
      "[214/01035] train_loss: 0.012688\n",
      "[214/01085] train_loss: 0.013247\n",
      "[214/01135] train_loss: 0.013069\n",
      "[214/01185] train_loss: 0.013379\n",
      "[215/00009] train_loss: 0.012908\n",
      "[215/00059] train_loss: 0.014258\n",
      "[215/00109] train_loss: 0.013932\n",
      "[215/00159] train_loss: 0.013482\n",
      "[215/00209] train_loss: 0.013909\n",
      "[215/00259] train_loss: 0.012871\n",
      "[215/00309] train_loss: 0.012899\n",
      "[215/00359] train_loss: 0.013258\n",
      "[215/00409] train_loss: 0.013101\n",
      "[215/00459] train_loss: 0.013502\n",
      "[215/00509] train_loss: 0.013334\n",
      "[215/00559] train_loss: 0.014172\n",
      "[215/00609] train_loss: 0.012983\n",
      "[215/00659] train_loss: 0.012635\n",
      "[215/00709] train_loss: 0.012955\n",
      "[215/00759] train_loss: 0.012239\n",
      "[215/00809] train_loss: 0.012972\n",
      "[215/00859] train_loss: 0.013117\n",
      "[215/00909] train_loss: 0.012938\n",
      "[215/00959] train_loss: 0.013463\n",
      "[215/01009] train_loss: 0.013299\n",
      "[215/01059] train_loss: 0.012847\n",
      "[215/01109] train_loss: 0.013675\n",
      "[215/01159] train_loss: 0.013229\n",
      "[215/01209] train_loss: 0.013421\n",
      "[216/00033] train_loss: 0.014134\n",
      "[216/00083] train_loss: 0.014143\n",
      "[216/00133] train_loss: 0.013197\n",
      "[216/00183] train_loss: 0.014190\n",
      "[216/00233] train_loss: 0.013807\n",
      "[216/00283] train_loss: 0.013183\n",
      "[216/00333] train_loss: 0.013495\n",
      "[216/00383] train_loss: 0.014395\n",
      "[216/00433] train_loss: 0.014203\n",
      "[216/00483] train_loss: 0.013439\n",
      "[216/00533] train_loss: 0.013387\n",
      "[216/00583] train_loss: 0.013229\n",
      "[216/00633] train_loss: 0.013906\n",
      "[216/00683] train_loss: 0.013479\n",
      "[216/00733] train_loss: 0.013083\n",
      "[216/00783] train_loss: 0.012900\n",
      "[216/00833] train_loss: 0.013094\n",
      "[216/00883] train_loss: 0.012601\n",
      "[216/00933] train_loss: 0.012756\n",
      "[216/00983] train_loss: 0.012930\n",
      "[216/01033] train_loss: 0.013449\n",
      "[216/01083] train_loss: 0.013641\n",
      "[216/01133] train_loss: 0.013063\n",
      "[216/01183] train_loss: 0.012944\n",
      "[217/00007] train_loss: 0.013209\n",
      "[217/00057] train_loss: 0.013915\n",
      "[217/00107] train_loss: 0.014161\n",
      "[217/00157] train_loss: 0.013745\n",
      "[217/00207] train_loss: 0.013718\n",
      "[217/00257] train_loss: 0.013875\n",
      "[217/00307] train_loss: 0.014237\n",
      "[217/00357] train_loss: 0.013004\n",
      "[217/00407] train_loss: 0.014225\n",
      "[217/00457] train_loss: 0.013679\n",
      "[217/00507] train_loss: 0.013450\n",
      "[217/00557] train_loss: 0.013575\n",
      "[217/00607] train_loss: 0.013360\n",
      "[217/00657] train_loss: 0.013358\n",
      "[217/00707] train_loss: 0.013517\n",
      "[217/00757] train_loss: 0.012219\n",
      "[217/00807] train_loss: 0.013220\n",
      "[217/00857] train_loss: 0.013123\n",
      "[217/00907] train_loss: 0.012570\n",
      "[217/00957] train_loss: 0.013507\n",
      "[217/01007] train_loss: 0.012799\n",
      "[217/01057] train_loss: 0.012215\n",
      "[217/01107] train_loss: 0.013539\n",
      "[217/01157] train_loss: 0.013184\n",
      "[217/01207] train_loss: 0.012697\n",
      "[218/00031] train_loss: 0.013348\n",
      "[218/00081] train_loss: 0.013591\n",
      "[218/00131] train_loss: 0.013709\n",
      "[218/00181] train_loss: 0.014122\n",
      "[218/00231] train_loss: 0.013843\n",
      "[218/00281] train_loss: 0.013474\n",
      "[218/00331] train_loss: 0.012951\n",
      "[218/00381] train_loss: 0.013951\n",
      "[218/00431] train_loss: 0.013239\n",
      "[218/00481] train_loss: 0.013922\n",
      "[218/00531] train_loss: 0.012787\n",
      "[218/00581] train_loss: 0.012593\n",
      "[218/00631] train_loss: 0.014013\n",
      "[218/00681] train_loss: 0.013150\n",
      "[218/00731] train_loss: 0.013274\n",
      "[218/00781] train_loss: 0.013499\n",
      "[218/00831] train_loss: 0.013401\n",
      "[218/00881] train_loss: 0.012893\n",
      "[218/00931] train_loss: 0.013007\n",
      "[218/00981] train_loss: 0.012961\n",
      "[218/01031] train_loss: 0.013537\n",
      "[218/01081] train_loss: 0.012900\n",
      "[218/01131] train_loss: 0.012641\n",
      "[218/01181] train_loss: 0.013444\n",
      "[219/00005] train_loss: 0.012732\n",
      "[219/00055] train_loss: 0.014669\n",
      "[219/00105] train_loss: 0.014016\n",
      "[219/00155] train_loss: 0.014693\n",
      "[219/00205] train_loss: 0.013182\n",
      "[219/00255] train_loss: 0.013514\n",
      "[219/00305] train_loss: 0.014100\n",
      "[219/00355] train_loss: 0.012284\n",
      "[219/00405] train_loss: 0.013557\n",
      "[219/00455] train_loss: 0.013442\n",
      "[219/00505] train_loss: 0.012654\n",
      "[219/00555] train_loss: 0.012704\n",
      "[219/00605] train_loss: 0.012678\n",
      "[219/00655] train_loss: 0.013421\n",
      "[219/00705] train_loss: 0.013201\n",
      "[219/00755] train_loss: 0.012965\n",
      "[219/00805] train_loss: 0.013109\n",
      "[219/00855] train_loss: 0.012122\n",
      "[219/00905] train_loss: 0.013391\n",
      "[219/00955] train_loss: 0.012982\n",
      "[219/01005] train_loss: 0.013613\n",
      "[219/01055] train_loss: 0.013629\n",
      "[219/01105] train_loss: 0.012288\n",
      "[219/01155] train_loss: 0.013026\n",
      "[219/01205] train_loss: 0.012525\n",
      "[220/00029] train_loss: 0.014015\n",
      "[220/00079] train_loss: 0.014074\n",
      "[220/00129] train_loss: 0.013937\n",
      "[220/00179] train_loss: 0.013869\n",
      "[220/00229] train_loss: 0.013738\n",
      "[220/00279] train_loss: 0.013278\n",
      "[220/00329] train_loss: 0.013603\n",
      "[220/00379] train_loss: 0.013808\n",
      "[220/00429] train_loss: 0.012931\n",
      "[220/00479] train_loss: 0.013438\n",
      "[220/00529] train_loss: 0.012674\n",
      "[220/00579] train_loss: 0.013963\n",
      "[220/00629] train_loss: 0.013074\n",
      "[220/00679] train_loss: 0.012817\n",
      "[220/00729] train_loss: 0.012999\n",
      "[220/00779] train_loss: 0.013743\n",
      "[220/00829] train_loss: 0.012852\n",
      "[220/00879] train_loss: 0.014026\n",
      "[220/00929] train_loss: 0.013373\n",
      "[220/00979] train_loss: 0.013001\n",
      "[220/01029] train_loss: 0.012812\n",
      "[220/01079] train_loss: 0.013035\n",
      "[220/01129] train_loss: 0.014026\n",
      "[220/01179] train_loss: 0.013365\n",
      "[221/00003] train_loss: 0.012496\n",
      "[221/00053] train_loss: 0.014477\n",
      "[221/00103] train_loss: 0.014435\n",
      "[221/00153] train_loss: 0.013792\n",
      "[221/00203] train_loss: 0.013432\n",
      "[221/00253] train_loss: 0.013206\n",
      "[221/00303] train_loss: 0.013365\n",
      "[221/00353] train_loss: 0.013323\n",
      "[221/00403] train_loss: 0.012724\n",
      "[221/00453] train_loss: 0.012747\n",
      "[221/00503] train_loss: 0.014051\n",
      "[221/00553] train_loss: 0.012832\n",
      "[221/00603] train_loss: 0.013462\n",
      "[221/00653] train_loss: 0.013300\n",
      "[221/00703] train_loss: 0.013078\n",
      "[221/00753] train_loss: 0.012247\n",
      "[221/00803] train_loss: 0.013227\n",
      "[221/00853] train_loss: 0.012894\n",
      "[221/00903] train_loss: 0.013799\n",
      "[221/00953] train_loss: 0.012893\n",
      "[221/01003] train_loss: 0.012556\n",
      "[221/01053] train_loss: 0.012946\n",
      "[221/01103] train_loss: 0.013008\n",
      "[221/01153] train_loss: 0.012930\n",
      "[221/01203] train_loss: 0.013074\n",
      "[222/00027] train_loss: 0.013496\n",
      "[222/00077] train_loss: 0.013726\n",
      "[222/00127] train_loss: 0.013784\n",
      "[222/00177] train_loss: 0.012964\n",
      "[222/00227] train_loss: 0.012890\n",
      "[222/00277] train_loss: 0.013562\n",
      "[222/00327] train_loss: 0.013266\n",
      "[222/00377] train_loss: 0.013704\n",
      "[222/00427] train_loss: 0.013654\n",
      "[222/00477] train_loss: 0.013380\n",
      "[222/00527] train_loss: 0.013484\n",
      "[222/00577] train_loss: 0.012870\n",
      "[222/00627] train_loss: 0.013287\n",
      "[222/00677] train_loss: 0.013059\n",
      "[222/00727] train_loss: 0.013049\n",
      "[222/00777] train_loss: 0.012374\n",
      "[222/00827] train_loss: 0.013121\n",
      "[222/00877] train_loss: 0.012468\n",
      "[222/00927] train_loss: 0.014791\n",
      "[222/00977] train_loss: 0.013909\n",
      "[222/01027] train_loss: 0.012954\n",
      "[222/01077] train_loss: 0.013536\n",
      "[222/01127] train_loss: 0.012776\n",
      "[222/01177] train_loss: 0.012896\n",
      "[223/00001] train_loss: 0.012696\n",
      "[223/00051] train_loss: 0.013955\n",
      "[223/00101] train_loss: 0.014705\n",
      "[223/00151] train_loss: 0.014202\n",
      "[223/00201] train_loss: 0.013785\n",
      "[223/00251] train_loss: 0.013421\n",
      "[223/00301] train_loss: 0.014090\n",
      "[223/00351] train_loss: 0.012512\n",
      "[223/00401] train_loss: 0.012574\n",
      "[223/00451] train_loss: 0.013786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[223/00501] train_loss: 0.013165\n",
      "[223/00551] train_loss: 0.012463\n",
      "[223/00601] train_loss: 0.013281\n",
      "[223/00651] train_loss: 0.012443\n",
      "[223/00701] train_loss: 0.012661\n",
      "[223/00751] train_loss: 0.013503\n",
      "[223/00801] train_loss: 0.012558\n",
      "[223/00851] train_loss: 0.012954\n",
      "[223/00901] train_loss: 0.012513\n",
      "[223/00951] train_loss: 0.012277\n",
      "[223/01001] train_loss: 0.013412\n",
      "[223/01051] train_loss: 0.012829\n",
      "[223/01101] train_loss: 0.013059\n",
      "[223/01151] train_loss: 0.012590\n",
      "[223/01201] train_loss: 0.013589\n",
      "[224/00025] train_loss: 0.013143\n",
      "[224/00075] train_loss: 0.013675\n",
      "[224/00125] train_loss: 0.013293\n",
      "[224/00175] train_loss: 0.013957\n",
      "[224/00225] train_loss: 0.013097\n",
      "[224/00275] train_loss: 0.014411\n",
      "[224/00325] train_loss: 0.013259\n",
      "[224/00375] train_loss: 0.012972\n",
      "[224/00425] train_loss: 0.013324\n",
      "[224/00475] train_loss: 0.014064\n",
      "[224/00525] train_loss: 0.012925\n",
      "[224/00575] train_loss: 0.012896\n",
      "[224/00625] train_loss: 0.013067\n",
      "[224/00675] train_loss: 0.013266\n",
      "[224/00725] train_loss: 0.012669\n",
      "[224/00775] train_loss: 0.012993\n",
      "[224/00825] train_loss: 0.013663\n",
      "[224/00875] train_loss: 0.013197\n",
      "[224/00925] train_loss: 0.013481\n",
      "[224/00975] train_loss: 0.014445\n",
      "[224/01025] train_loss: 0.013554\n",
      "[224/01075] train_loss: 0.013759\n",
      "[224/01125] train_loss: 0.012977\n",
      "[224/01175] train_loss: 0.012557\n",
      "[224/01225] train_loss: 0.012769\n",
      "[225/00049] train_loss: 0.013878\n",
      "[225/00099] train_loss: 0.013913\n",
      "[225/00149] train_loss: 0.013263\n",
      "[225/00199] train_loss: 0.012537\n",
      "[225/00249] train_loss: 0.013436\n",
      "[225/00299] train_loss: 0.013320\n",
      "[225/00349] train_loss: 0.013669\n",
      "[225/00399] train_loss: 0.014425\n",
      "[225/00449] train_loss: 0.012657\n",
      "[225/00499] train_loss: 0.013542\n",
      "[225/00549] train_loss: 0.013444\n",
      "[225/00599] train_loss: 0.013345\n",
      "[225/00649] train_loss: 0.013661\n",
      "[225/00699] train_loss: 0.013518\n",
      "[225/00749] train_loss: 0.012828\n",
      "[225/00799] train_loss: 0.013988\n",
      "[225/00849] train_loss: 0.013553\n",
      "[225/00899] train_loss: 0.013099\n",
      "[225/00949] train_loss: 0.013366\n",
      "[225/00999] train_loss: 0.012555\n",
      "[225/01049] train_loss: 0.012817\n",
      "[225/01099] train_loss: 0.012657\n",
      "[225/01149] train_loss: 0.012700\n",
      "[225/01199] train_loss: 0.013395\n",
      "[226/00023] train_loss: 0.013509\n",
      "[226/00073] train_loss: 0.014630\n",
      "[226/00123] train_loss: 0.013289\n",
      "[226/00173] train_loss: 0.013568\n",
      "[226/00223] train_loss: 0.012712\n",
      "[226/00273] train_loss: 0.013515\n",
      "[226/00323] train_loss: 0.013337\n",
      "[226/00373] train_loss: 0.013194\n",
      "[226/00423] train_loss: 0.013211\n",
      "[226/00473] train_loss: 0.014073\n",
      "[226/00523] train_loss: 0.013051\n",
      "[226/00573] train_loss: 0.013049\n",
      "[226/00623] train_loss: 0.012557\n",
      "[226/00673] train_loss: 0.012983\n",
      "[226/00723] train_loss: 0.013576\n",
      "[226/00773] train_loss: 0.012246\n",
      "[226/00823] train_loss: 0.013646\n",
      "[226/00873] train_loss: 0.012747\n",
      "[226/00923] train_loss: 0.013202\n",
      "[226/00973] train_loss: 0.013293\n",
      "[226/01023] train_loss: 0.013212\n",
      "[226/01073] train_loss: 0.012834\n",
      "[226/01123] train_loss: 0.013283\n",
      "[226/01173] train_loss: 0.013058\n",
      "[226/01223] train_loss: 0.013436\n",
      "[227/00047] train_loss: 0.014272\n",
      "[227/00097] train_loss: 0.014000\n",
      "[227/00147] train_loss: 0.013326\n",
      "[227/00197] train_loss: 0.013848\n",
      "[227/00247] train_loss: 0.012776\n",
      "[227/00297] train_loss: 0.014255\n",
      "[227/00347] train_loss: 0.013512\n",
      "[227/00397] train_loss: 0.012980\n",
      "[227/00447] train_loss: 0.013021\n",
      "[227/00497] train_loss: 0.013584\n",
      "[227/00547] train_loss: 0.013196\n",
      "[227/00597] train_loss: 0.012947\n",
      "[227/00647] train_loss: 0.013820\n",
      "[227/00697] train_loss: 0.012633\n",
      "[227/00747] train_loss: 0.012721\n",
      "[227/00797] train_loss: 0.013120\n",
      "[227/00847] train_loss: 0.013069\n",
      "[227/00897] train_loss: 0.013022\n",
      "[227/00947] train_loss: 0.013425\n",
      "[227/00997] train_loss: 0.012728\n",
      "[227/01047] train_loss: 0.012941\n",
      "[227/01097] train_loss: 0.012377\n",
      "[227/01147] train_loss: 0.013408\n",
      "[227/01197] train_loss: 0.013438\n",
      "[228/00021] train_loss: 0.013096\n",
      "[228/00071] train_loss: 0.014572\n",
      "[228/00121] train_loss: 0.013538\n",
      "[228/00171] train_loss: 0.013334\n",
      "[228/00221] train_loss: 0.013288\n",
      "[228/00271] train_loss: 0.013235\n",
      "[228/00321] train_loss: 0.012912\n",
      "[228/00371] train_loss: 0.013728\n",
      "[228/00421] train_loss: 0.013638\n",
      "[228/00471] train_loss: 0.013706\n",
      "[228/00521] train_loss: 0.013196\n",
      "[228/00571] train_loss: 0.012921\n",
      "[228/00621] train_loss: 0.012676\n",
      "[228/00671] train_loss: 0.013047\n",
      "[228/00721] train_loss: 0.013062\n",
      "[228/00771] train_loss: 0.013179\n",
      "[228/00821] train_loss: 0.012363\n",
      "[228/00871] train_loss: 0.012821\n",
      "[228/00921] train_loss: 0.013179\n",
      "[228/00971] train_loss: 0.012865\n",
      "[228/01021] train_loss: 0.013929\n",
      "[228/01071] train_loss: 0.013473\n",
      "[228/01121] train_loss: 0.013281\n",
      "[228/01171] train_loss: 0.012665\n",
      "[228/01221] train_loss: 0.012541\n",
      "[229/00045] train_loss: 0.014422\n",
      "[229/00095] train_loss: 0.013907\n",
      "[229/00145] train_loss: 0.013857\n",
      "[229/00195] train_loss: 0.013284\n",
      "[229/00245] train_loss: 0.013553\n",
      "[229/00295] train_loss: 0.013327\n",
      "[229/00345] train_loss: 0.013158\n",
      "[229/00395] train_loss: 0.013334\n",
      "[229/00445] train_loss: 0.013906\n",
      "[229/00495] train_loss: 0.013187\n",
      "[229/00545] train_loss: 0.012715\n",
      "[229/00595] train_loss: 0.013479\n",
      "[229/00645] train_loss: 0.013289\n",
      "[229/00695] train_loss: 0.012634\n",
      "[229/00745] train_loss: 0.012416\n",
      "[229/00795] train_loss: 0.012384\n",
      "[229/00845] train_loss: 0.012967\n",
      "[229/00895] train_loss: 0.012855\n",
      "[229/00945] train_loss: 0.013703\n",
      "[229/00995] train_loss: 0.012891\n",
      "[229/01045] train_loss: 0.013243\n",
      "[229/01095] train_loss: 0.013799\n",
      "[229/01145] train_loss: 0.013015\n",
      "[229/01195] train_loss: 0.012962\n",
      "[230/00019] train_loss: 0.013420\n",
      "[230/00069] train_loss: 0.013827\n",
      "[230/00119] train_loss: 0.012740\n",
      "[230/00169] train_loss: 0.013485\n",
      "[230/00219] train_loss: 0.013604\n",
      "[230/00269] train_loss: 0.013727\n",
      "[230/00319] train_loss: 0.013094\n",
      "[230/00369] train_loss: 0.013120\n",
      "[230/00419] train_loss: 0.012987\n",
      "[230/00469] train_loss: 0.013421\n",
      "[230/00519] train_loss: 0.012863\n",
      "[230/00569] train_loss: 0.013557\n",
      "[230/00619] train_loss: 0.013594\n",
      "[230/00669] train_loss: 0.012749\n",
      "[230/00719] train_loss: 0.013397\n",
      "[230/00769] train_loss: 0.013257\n",
      "[230/00819] train_loss: 0.013457\n",
      "[230/00869] train_loss: 0.012855\n",
      "[230/00919] train_loss: 0.013425\n",
      "[230/00969] train_loss: 0.012873\n",
      "[230/01019] train_loss: 0.013226\n",
      "[230/01069] train_loss: 0.013654\n",
      "[230/01119] train_loss: 0.013221\n",
      "[230/01169] train_loss: 0.012979\n",
      "[230/01219] train_loss: 0.012629\n",
      "[231/00043] train_loss: 0.014531\n",
      "[231/00093] train_loss: 0.013959\n",
      "[231/00143] train_loss: 0.013331\n",
      "[231/00193] train_loss: 0.014302\n",
      "[231/00243] train_loss: 0.013341\n",
      "[231/00293] train_loss: 0.013482\n",
      "[231/00343] train_loss: 0.012689\n",
      "[231/00393] train_loss: 0.013580\n",
      "[231/00443] train_loss: 0.012572\n",
      "[231/00493] train_loss: 0.013149\n",
      "[231/00543] train_loss: 0.012724\n",
      "[231/00593] train_loss: 0.013265\n",
      "[231/00643] train_loss: 0.013472\n",
      "[231/00693] train_loss: 0.012545\n",
      "[231/00743] train_loss: 0.011909\n",
      "[231/00793] train_loss: 0.012758\n",
      "[231/00843] train_loss: 0.012363\n",
      "[231/00893] train_loss: 0.012987\n",
      "[231/00943] train_loss: 0.012716\n",
      "[231/00993] train_loss: 0.012961\n",
      "[231/01043] train_loss: 0.013542\n",
      "[231/01093] train_loss: 0.012865\n",
      "[231/01143] train_loss: 0.012950\n",
      "[231/01193] train_loss: 0.013258\n",
      "[232/00017] train_loss: 0.013781\n",
      "[232/00067] train_loss: 0.014276\n",
      "[232/00117] train_loss: 0.013427\n",
      "[232/00167] train_loss: 0.014115\n",
      "[232/00217] train_loss: 0.013142\n",
      "[232/00267] train_loss: 0.012653\n",
      "[232/00317] train_loss: 0.013505\n",
      "[232/00367] train_loss: 0.013417\n",
      "[232/00417] train_loss: 0.013555\n",
      "[232/00467] train_loss: 0.013165\n",
      "[232/00517] train_loss: 0.013289\n",
      "[232/00567] train_loss: 0.013092\n",
      "[232/00617] train_loss: 0.012687\n",
      "[232/00667] train_loss: 0.013186\n",
      "[232/00717] train_loss: 0.013785\n",
      "[232/00767] train_loss: 0.012967\n",
      "[232/00817] train_loss: 0.012709\n",
      "[232/00867] train_loss: 0.013128\n",
      "[232/00917] train_loss: 0.013356\n",
      "[232/00967] train_loss: 0.013207\n",
      "[232/01017] train_loss: 0.013391\n",
      "[232/01067] train_loss: 0.012782\n",
      "[232/01117] train_loss: 0.013081\n",
      "[232/01167] train_loss: 0.013076\n",
      "[232/01217] train_loss: 0.012885\n",
      "[233/00041] train_loss: 0.013998\n",
      "[233/00091] train_loss: 0.013735\n",
      "[233/00141] train_loss: 0.014229\n",
      "[233/00191] train_loss: 0.013258\n",
      "[233/00241] train_loss: 0.014076\n",
      "[233/00291] train_loss: 0.012303\n",
      "[233/00341] train_loss: 0.013783\n",
      "[233/00391] train_loss: 0.013070\n",
      "[233/00441] train_loss: 0.012865\n",
      "[233/00491] train_loss: 0.013281\n",
      "[233/00541] train_loss: 0.012317\n",
      "[233/00591] train_loss: 0.013289\n",
      "[233/00641] train_loss: 0.013068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[233/00691] train_loss: 0.013141\n",
      "[233/00741] train_loss: 0.012082\n",
      "[233/00791] train_loss: 0.012716\n",
      "[233/00841] train_loss: 0.013308\n",
      "[233/00891] train_loss: 0.013225\n",
      "[233/00941] train_loss: 0.013306\n",
      "[233/00991] train_loss: 0.012849\n",
      "[233/01041] train_loss: 0.013570\n",
      "[233/01091] train_loss: 0.012668\n",
      "[233/01141] train_loss: 0.013104\n",
      "[233/01191] train_loss: 0.012521\n",
      "[234/00015] train_loss: 0.014103\n",
      "[234/00065] train_loss: 0.013960\n",
      "[234/00115] train_loss: 0.013901\n",
      "[234/00165] train_loss: 0.014147\n",
      "[234/00215] train_loss: 0.013476\n",
      "[234/00265] train_loss: 0.012782\n",
      "[234/00315] train_loss: 0.013800\n",
      "[234/00365] train_loss: 0.013477\n",
      "[234/00415] train_loss: 0.013210\n",
      "[234/00465] train_loss: 0.013338\n",
      "[234/00515] train_loss: 0.013562\n",
      "[234/00565] train_loss: 0.013379\n",
      "[234/00615] train_loss: 0.013010\n",
      "[234/00665] train_loss: 0.013073\n",
      "[234/00715] train_loss: 0.013375\n",
      "[234/00765] train_loss: 0.013909\n",
      "[234/00815] train_loss: 0.012722\n",
      "[234/00865] train_loss: 0.013502\n",
      "[234/00915] train_loss: 0.012848\n",
      "[234/00965] train_loss: 0.012668\n",
      "[234/01015] train_loss: 0.013002\n",
      "[234/01065] train_loss: 0.011961\n",
      "[234/01115] train_loss: 0.012726\n",
      "[234/01165] train_loss: 0.012344\n",
      "[234/01215] train_loss: 0.012211\n",
      "[235/00039] train_loss: 0.013849\n",
      "[235/00089] train_loss: 0.013892\n",
      "[235/00139] train_loss: 0.013338\n",
      "[235/00189] train_loss: 0.014373\n",
      "[235/00239] train_loss: 0.012980\n",
      "[235/00289] train_loss: 0.013177\n",
      "[235/00339] train_loss: 0.013259\n",
      "[235/00389] train_loss: 0.013646\n",
      "[235/00439] train_loss: 0.013947\n",
      "[235/00489] train_loss: 0.013325\n",
      "[235/00539] train_loss: 0.012237\n",
      "[235/00589] train_loss: 0.013153\n",
      "[235/00639] train_loss: 0.012790\n",
      "[235/00689] train_loss: 0.013794\n",
      "[235/00739] train_loss: 0.013313\n",
      "[235/00789] train_loss: 0.013008\n",
      "[235/00839] train_loss: 0.013027\n",
      "[235/00889] train_loss: 0.013214\n",
      "[235/00939] train_loss: 0.013519\n",
      "[235/00989] train_loss: 0.012379\n",
      "[235/01039] train_loss: 0.012896\n",
      "[235/01089] train_loss: 0.012714\n",
      "[235/01139] train_loss: 0.013201\n",
      "[235/01189] train_loss: 0.012679\n",
      "[236/00013] train_loss: 0.013277\n",
      "[236/00063] train_loss: 0.014413\n",
      "[236/00113] train_loss: 0.013738\n",
      "[236/00163] train_loss: 0.013858\n",
      "[236/00213] train_loss: 0.013288\n",
      "[236/00263] train_loss: 0.013193\n",
      "[236/00313] train_loss: 0.013195\n",
      "[236/00363] train_loss: 0.012443\n",
      "[236/00413] train_loss: 0.013027\n",
      "[236/00463] train_loss: 0.012798\n",
      "[236/00513] train_loss: 0.012857\n",
      "[236/00563] train_loss: 0.012732\n",
      "[236/00613] train_loss: 0.013538\n",
      "[236/00663] train_loss: 0.012510\n",
      "[236/00713] train_loss: 0.013527\n",
      "[236/00763] train_loss: 0.012740\n",
      "[236/00813] train_loss: 0.012682\n",
      "[236/00863] train_loss: 0.012985\n",
      "[236/00913] train_loss: 0.014054\n",
      "[236/00963] train_loss: 0.013210\n",
      "[236/01013] train_loss: 0.013110\n",
      "[236/01063] train_loss: 0.013421\n",
      "[236/01113] train_loss: 0.013230\n",
      "[236/01163] train_loss: 0.013555\n",
      "[236/01213] train_loss: 0.013557\n",
      "[237/00037] train_loss: 0.013240\n",
      "[237/00087] train_loss: 0.013724\n",
      "[237/00137] train_loss: 0.012605\n",
      "[237/00187] train_loss: 0.013043\n",
      "[237/00237] train_loss: 0.013988\n",
      "[237/00287] train_loss: 0.013480\n",
      "[237/00337] train_loss: 0.013776\n",
      "[237/00387] train_loss: 0.014047\n",
      "[237/00437] train_loss: 0.013304\n",
      "[237/00487] train_loss: 0.013263\n",
      "[237/00537] train_loss: 0.012266\n",
      "[237/00587] train_loss: 0.013686\n",
      "[237/00637] train_loss: 0.012341\n",
      "[237/00687] train_loss: 0.012527\n",
      "[237/00737] train_loss: 0.012868\n",
      "[237/00787] train_loss: 0.013301\n",
      "[237/00837] train_loss: 0.013018\n",
      "[237/00887] train_loss: 0.013173\n",
      "[237/00937] train_loss: 0.012971\n",
      "[237/00987] train_loss: 0.012727\n",
      "[237/01037] train_loss: 0.013519\n",
      "[237/01087] train_loss: 0.012978\n",
      "[237/01137] train_loss: 0.013381\n",
      "[237/01187] train_loss: 0.012324\n",
      "[238/00011] train_loss: 0.012549\n",
      "[238/00061] train_loss: 0.013975\n",
      "[238/00111] train_loss: 0.013176\n",
      "[238/00161] train_loss: 0.013454\n",
      "[238/00211] train_loss: 0.013663\n",
      "[238/00261] train_loss: 0.013276\n",
      "[238/00311] train_loss: 0.013662\n",
      "[238/00361] train_loss: 0.013889\n",
      "[238/00411] train_loss: 0.013268\n",
      "[238/00461] train_loss: 0.013154\n",
      "[238/00511] train_loss: 0.012519\n",
      "[238/00561] train_loss: 0.012806\n",
      "[238/00611] train_loss: 0.013415\n",
      "[238/00661] train_loss: 0.012917\n",
      "[238/00711] train_loss: 0.012845\n",
      "[238/00761] train_loss: 0.013299\n",
      "[238/00811] train_loss: 0.013126\n",
      "[238/00861] train_loss: 0.013287\n",
      "[238/00911] train_loss: 0.013845\n",
      "[238/00961] train_loss: 0.013231\n",
      "[238/01011] train_loss: 0.012444\n",
      "[238/01061] train_loss: 0.013329\n",
      "[238/01111] train_loss: 0.012369\n",
      "[238/01161] train_loss: 0.013468\n",
      "[238/01211] train_loss: 0.012923\n",
      "[239/00035] train_loss: 0.013085\n",
      "[239/00085] train_loss: 0.013869\n",
      "[239/00135] train_loss: 0.013513\n",
      "[239/00185] train_loss: 0.013325\n",
      "[239/00235] train_loss: 0.014149\n",
      "[239/00285] train_loss: 0.012906\n",
      "[239/00335] train_loss: 0.013451\n",
      "[239/00385] train_loss: 0.012429\n",
      "[239/00435] train_loss: 0.013600\n",
      "[239/00485] train_loss: 0.013115\n",
      "[239/00535] train_loss: 0.013455\n",
      "[239/00585] train_loss: 0.013318\n",
      "[239/00635] train_loss: 0.012554\n",
      "[239/00685] train_loss: 0.013562\n",
      "[239/00735] train_loss: 0.013066\n",
      "[239/00785] train_loss: 0.013115\n",
      "[239/00835] train_loss: 0.013174\n",
      "[239/00885] train_loss: 0.012903\n",
      "[239/00935] train_loss: 0.012799\n",
      "[239/00985] train_loss: 0.012778\n",
      "[239/01035] train_loss: 0.012501\n",
      "[239/01085] train_loss: 0.012662\n",
      "[239/01135] train_loss: 0.012639\n",
      "[239/01185] train_loss: 0.013050\n",
      "[240/00009] train_loss: 0.013018\n",
      "[240/00059] train_loss: 0.013776\n",
      "[240/00109] train_loss: 0.013813\n",
      "[240/00159] train_loss: 0.014063\n",
      "[240/00209] train_loss: 0.013800\n",
      "[240/00259] train_loss: 0.013425\n",
      "[240/00309] train_loss: 0.013207\n",
      "[240/00359] train_loss: 0.013042\n",
      "[240/00409] train_loss: 0.013236\n",
      "[240/00459] train_loss: 0.013053\n",
      "[240/00509] train_loss: 0.013644\n",
      "[240/00559] train_loss: 0.013459\n",
      "[240/00609] train_loss: 0.013456\n",
      "[240/00659] train_loss: 0.013145\n",
      "[240/00709] train_loss: 0.013346\n",
      "[240/00759] train_loss: 0.013274\n",
      "[240/00809] train_loss: 0.012484\n",
      "[240/00859] train_loss: 0.013100\n",
      "[240/00909] train_loss: 0.012274\n",
      "[240/00959] train_loss: 0.012993\n",
      "[240/01009] train_loss: 0.012956\n",
      "[240/01059] train_loss: 0.012287\n",
      "[240/01109] train_loss: 0.013077\n",
      "[240/01159] train_loss: 0.012096\n",
      "[240/01209] train_loss: 0.011923\n",
      "[241/00033] train_loss: 0.013675\n",
      "[241/00083] train_loss: 0.014269\n",
      "[241/00133] train_loss: 0.013678\n",
      "[241/00183] train_loss: 0.013890\n",
      "[241/00233] train_loss: 0.013604\n",
      "[241/00283] train_loss: 0.013289\n",
      "[241/00333] train_loss: 0.012923\n",
      "[241/00383] train_loss: 0.013027\n",
      "[241/00433] train_loss: 0.012864\n",
      "[241/00483] train_loss: 0.012997\n",
      "[241/00533] train_loss: 0.012805\n",
      "[241/00583] train_loss: 0.013030\n",
      "[241/00633] train_loss: 0.012403\n",
      "[241/00683] train_loss: 0.013201\n",
      "[241/00733] train_loss: 0.013480\n",
      "[241/00783] train_loss: 0.012790\n",
      "[241/00833] train_loss: 0.012750\n",
      "[241/00883] train_loss: 0.013353\n",
      "[241/00933] train_loss: 0.012592\n",
      "[241/00983] train_loss: 0.013513\n",
      "[241/01033] train_loss: 0.012274\n",
      "[241/01083] train_loss: 0.013295\n",
      "[241/01133] train_loss: 0.013359\n",
      "[241/01183] train_loss: 0.011874\n",
      "[242/00007] train_loss: 0.012532\n",
      "[242/00057] train_loss: 0.014928\n",
      "[242/00107] train_loss: 0.013940\n",
      "[242/00157] train_loss: 0.013726\n",
      "[242/00207] train_loss: 0.013133\n",
      "[242/00257] train_loss: 0.013543\n",
      "[242/00307] train_loss: 0.013031\n",
      "[242/00357] train_loss: 0.013074\n",
      "[242/00407] train_loss: 0.013184\n",
      "[242/00457] train_loss: 0.013148\n",
      "[242/00507] train_loss: 0.013289\n",
      "[242/00557] train_loss: 0.013240\n",
      "[242/00607] train_loss: 0.012546\n",
      "[242/00657] train_loss: 0.013613\n",
      "[242/00707] train_loss: 0.013221\n",
      "[242/00757] train_loss: 0.013157\n",
      "[242/00807] train_loss: 0.012996\n",
      "[242/00857] train_loss: 0.012536\n",
      "[242/00907] train_loss: 0.013023\n",
      "[242/00957] train_loss: 0.012849\n",
      "[242/01007] train_loss: 0.012565\n",
      "[242/01057] train_loss: 0.012630\n",
      "[242/01107] train_loss: 0.012796\n",
      "[242/01157] train_loss: 0.012618\n",
      "[242/01207] train_loss: 0.013148\n",
      "[243/00031] train_loss: 0.014328\n",
      "[243/00081] train_loss: 0.013938\n",
      "[243/00131] train_loss: 0.014015\n",
      "[243/00181] train_loss: 0.014181\n",
      "[243/00231] train_loss: 0.013761\n",
      "[243/00281] train_loss: 0.013388\n",
      "[243/00331] train_loss: 0.013458\n",
      "[243/00381] train_loss: 0.012719\n",
      "[243/00431] train_loss: 0.013111\n",
      "[243/00481] train_loss: 0.013376\n",
      "[243/00531] train_loss: 0.013269\n",
      "[243/00581] train_loss: 0.012715\n",
      "[243/00631] train_loss: 0.012704\n",
      "[243/00681] train_loss: 0.012131\n",
      "[243/00731] train_loss: 0.012753\n",
      "[243/00781] train_loss: 0.013406\n",
      "[243/00831] train_loss: 0.013631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[243/00881] train_loss: 0.012427\n",
      "[243/00931] train_loss: 0.012187\n",
      "[243/00981] train_loss: 0.012535\n",
      "[243/01031] train_loss: 0.013151\n",
      "[243/01081] train_loss: 0.012096\n",
      "[243/01131] train_loss: 0.012562\n",
      "[243/01181] train_loss: 0.013352\n",
      "[244/00005] train_loss: 0.012791\n",
      "[244/00055] train_loss: 0.013721\n",
      "[244/00105] train_loss: 0.013476\n",
      "[244/00155] train_loss: 0.012980\n",
      "[244/00205] train_loss: 0.013545\n",
      "[244/00255] train_loss: 0.013994\n",
      "[244/00305] train_loss: 0.013212\n",
      "[244/00355] train_loss: 0.013576\n",
      "[244/00405] train_loss: 0.013113\n",
      "[244/00455] train_loss: 0.013261\n",
      "[244/00505] train_loss: 0.012506\n",
      "[244/00555] train_loss: 0.012900\n",
      "[244/00605] train_loss: 0.013580\n",
      "[244/00655] train_loss: 0.013384\n",
      "[244/00705] train_loss: 0.012834\n",
      "[244/00755] train_loss: 0.012743\n",
      "[244/00805] train_loss: 0.012954\n",
      "[244/00855] train_loss: 0.012567\n",
      "[244/00905] train_loss: 0.012868\n",
      "[244/00955] train_loss: 0.013498\n",
      "[244/01005] train_loss: 0.013299\n",
      "[244/01055] train_loss: 0.012397\n",
      "[244/01105] train_loss: 0.013133\n",
      "[244/01155] train_loss: 0.012915\n",
      "[244/01205] train_loss: 0.012145\n",
      "[245/00029] train_loss: 0.013265\n",
      "[245/00079] train_loss: 0.013253\n",
      "[245/00129] train_loss: 0.014240\n",
      "[245/00179] train_loss: 0.013476\n",
      "[245/00229] train_loss: 0.013651\n",
      "[245/00279] train_loss: 0.013157\n",
      "[245/00329] train_loss: 0.012994\n",
      "[245/00379] train_loss: 0.013206\n",
      "[245/00429] train_loss: 0.013079\n",
      "[245/00479] train_loss: 0.013808\n",
      "[245/00529] train_loss: 0.013265\n",
      "[245/00579] train_loss: 0.012906\n",
      "[245/00629] train_loss: 0.013540\n",
      "[245/00679] train_loss: 0.013023\n",
      "[245/00729] train_loss: 0.013275\n",
      "[245/00779] train_loss: 0.013498\n",
      "[245/00829] train_loss: 0.013215\n",
      "[245/00879] train_loss: 0.013760\n",
      "[245/00929] train_loss: 0.013418\n",
      "[245/00979] train_loss: 0.012926\n",
      "[245/01029] train_loss: 0.012794\n",
      "[245/01079] train_loss: 0.012935\n",
      "[245/01129] train_loss: 0.012458\n",
      "[245/01179] train_loss: 0.012485\n",
      "[246/00003] train_loss: 0.013130\n",
      "[246/00053] train_loss: 0.013116\n",
      "[246/00103] train_loss: 0.014039\n",
      "[246/00153] train_loss: 0.013030\n",
      "[246/00203] train_loss: 0.013301\n",
      "[246/00253] train_loss: 0.012645\n",
      "[246/00303] train_loss: 0.012964\n",
      "[246/00353] train_loss: 0.013025\n",
      "[246/00403] train_loss: 0.013552\n",
      "[246/00453] train_loss: 0.013724\n",
      "[246/00503] train_loss: 0.012824\n",
      "[246/00553] train_loss: 0.013734\n",
      "[246/00603] train_loss: 0.013705\n",
      "[246/00653] train_loss: 0.013407\n",
      "[246/00703] train_loss: 0.012795\n",
      "[246/00753] train_loss: 0.012622\n",
      "[246/00803] train_loss: 0.012990\n",
      "[246/00853] train_loss: 0.012264\n",
      "[246/00903] train_loss: 0.013200\n",
      "[246/00953] train_loss: 0.012341\n",
      "[246/01003] train_loss: 0.012962\n",
      "[246/01053] train_loss: 0.012663\n",
      "[246/01103] train_loss: 0.013153\n",
      "[246/01153] train_loss: 0.012401\n",
      "[246/01203] train_loss: 0.013412\n",
      "[247/00027] train_loss: 0.013284\n",
      "[247/00077] train_loss: 0.013297\n",
      "[247/00127] train_loss: 0.013848\n",
      "[247/00177] train_loss: 0.013449\n",
      "[247/00227] train_loss: 0.013682\n",
      "[247/00277] train_loss: 0.012992\n",
      "[247/00327] train_loss: 0.013019\n",
      "[247/00377] train_loss: 0.012645\n",
      "[247/00427] train_loss: 0.012700\n",
      "[247/00477] train_loss: 0.012781\n",
      "[247/00527] train_loss: 0.013379\n",
      "[247/00577] train_loss: 0.012527\n",
      "[247/00627] train_loss: 0.013017\n",
      "[247/00677] train_loss: 0.012898\n",
      "[247/00727] train_loss: 0.013695\n",
      "[247/00777] train_loss: 0.012844\n",
      "[247/00827] train_loss: 0.013826\n",
      "[247/00877] train_loss: 0.012776\n",
      "[247/00927] train_loss: 0.013005\n",
      "[247/00977] train_loss: 0.013052\n",
      "[247/01027] train_loss: 0.012614\n",
      "[247/01077] train_loss: 0.012566\n",
      "[247/01127] train_loss: 0.013464\n",
      "[247/01177] train_loss: 0.012913\n",
      "[248/00001] train_loss: 0.013074\n",
      "[248/00051] train_loss: 0.013295\n",
      "[248/00101] train_loss: 0.013723\n",
      "[248/00151] train_loss: 0.013848\n",
      "[248/00201] train_loss: 0.013021\n",
      "[248/00251] train_loss: 0.014355\n",
      "[248/00301] train_loss: 0.013471\n",
      "[248/00351] train_loss: 0.012624\n",
      "[248/00401] train_loss: 0.013066\n",
      "[248/00451] train_loss: 0.013905\n",
      "[248/00501] train_loss: 0.013402\n",
      "[248/00551] train_loss: 0.013241\n",
      "[248/00601] train_loss: 0.013103\n",
      "[248/00651] train_loss: 0.013033\n",
      "[248/00701] train_loss: 0.011849\n",
      "[248/00751] train_loss: 0.013305\n",
      "[248/00801] train_loss: 0.013141\n",
      "[248/00851] train_loss: 0.012363\n",
      "[248/00901] train_loss: 0.013724\n",
      "[248/00951] train_loss: 0.012208\n",
      "[248/01001] train_loss: 0.012275\n",
      "[248/01051] train_loss: 0.013525\n",
      "[248/01101] train_loss: 0.013300\n",
      "[248/01151] train_loss: 0.012122\n",
      "[248/01201] train_loss: 0.013368\n",
      "[249/00025] train_loss: 0.013246\n",
      "[249/00075] train_loss: 0.013295\n",
      "[249/00125] train_loss: 0.013241\n",
      "[249/00175] train_loss: 0.013845\n",
      "[249/00225] train_loss: 0.013724\n",
      "[249/00275] train_loss: 0.013308\n",
      "[249/00325] train_loss: 0.013394\n",
      "[249/00375] train_loss: 0.013444\n",
      "[249/00425] train_loss: 0.013118\n",
      "[249/00475] train_loss: 0.013290\n",
      "[249/00525] train_loss: 0.012801\n",
      "[249/00575] train_loss: 0.013041\n",
      "[249/00625] train_loss: 0.013171\n",
      "[249/00675] train_loss: 0.012905\n",
      "[249/00725] train_loss: 0.013083\n",
      "[249/00775] train_loss: 0.012113\n",
      "[249/00825] train_loss: 0.013200\n",
      "[249/00875] train_loss: 0.013464\n",
      "[249/00925] train_loss: 0.012462\n",
      "[249/00975] train_loss: 0.012907\n",
      "[249/01025] train_loss: 0.012644\n",
      "[249/01075] train_loss: 0.013298\n",
      "[249/01125] train_loss: 0.011942\n",
      "[249/01175] train_loss: 0.012314\n",
      "[249/01225] train_loss: 0.013051\n",
      "[250/00049] train_loss: 0.013886\n",
      "[250/00099] train_loss: 0.013903\n",
      "[250/00149] train_loss: 0.013518\n",
      "[250/00199] train_loss: 0.013509\n",
      "[250/00249] train_loss: 0.012254\n",
      "[250/00299] train_loss: 0.013306\n",
      "[250/00349] train_loss: 0.013137\n",
      "[250/00399] train_loss: 0.013148\n",
      "[250/00449] train_loss: 0.012730\n",
      "[250/00499] train_loss: 0.012737\n",
      "[250/00549] train_loss: 0.012824\n",
      "[250/00599] train_loss: 0.012150\n",
      "[250/00649] train_loss: 0.013650\n",
      "[250/00699] train_loss: 0.012643\n",
      "[250/00749] train_loss: 0.013528\n",
      "[250/00799] train_loss: 0.012829\n",
      "[250/00849] train_loss: 0.012771\n",
      "[250/00899] train_loss: 0.013182\n",
      "[250/00949] train_loss: 0.012639\n",
      "[250/00999] train_loss: 0.013031\n",
      "[250/01049] train_loss: 0.012721\n",
      "[250/01099] train_loss: 0.012558\n",
      "[250/01149] train_loss: 0.013308\n",
      "[250/01199] train_loss: 0.012776\n",
      "[251/00023] train_loss: 0.012970\n",
      "[251/00073] train_loss: 0.013527\n",
      "[251/00123] train_loss: 0.013162\n",
      "[251/00173] train_loss: 0.012946\n",
      "[251/00223] train_loss: 0.013153\n",
      "[251/00273] train_loss: 0.013891\n",
      "[251/00323] train_loss: 0.013214\n",
      "[251/00373] train_loss: 0.012528\n",
      "[251/00423] train_loss: 0.013363\n",
      "[251/00473] train_loss: 0.013283\n",
      "[251/00523] train_loss: 0.012760\n",
      "[251/00573] train_loss: 0.012462\n",
      "[251/00623] train_loss: 0.012425\n",
      "[251/00673] train_loss: 0.012430\n",
      "[251/00723] train_loss: 0.013486\n",
      "[251/00773] train_loss: 0.013026\n",
      "[251/00823] train_loss: 0.012878\n",
      "[251/00873] train_loss: 0.013107\n",
      "[251/00923] train_loss: 0.013009\n",
      "[251/00973] train_loss: 0.013513\n",
      "[251/01023] train_loss: 0.012712\n",
      "[251/01073] train_loss: 0.012979\n",
      "[251/01123] train_loss: 0.013060\n",
      "[251/01173] train_loss: 0.012408\n",
      "[251/01223] train_loss: 0.013008\n",
      "[252/00047] train_loss: 0.012759\n",
      "[252/00097] train_loss: 0.013056\n",
      "[252/00147] train_loss: 0.013992\n",
      "[252/00197] train_loss: 0.013511\n",
      "[252/00247] train_loss: 0.013060\n",
      "[252/00297] train_loss: 0.013173\n",
      "[252/00347] train_loss: 0.013524\n",
      "[252/00397] train_loss: 0.013415\n",
      "[252/00447] train_loss: 0.013587\n",
      "[252/00497] train_loss: 0.013026\n",
      "[252/00547] train_loss: 0.012349\n",
      "[252/00597] train_loss: 0.013253\n",
      "[252/00647] train_loss: 0.012457\n",
      "[252/00697] train_loss: 0.013012\n",
      "[252/00747] train_loss: 0.012585\n",
      "[252/00797] train_loss: 0.013575\n",
      "[252/00847] train_loss: 0.012395\n",
      "[252/00897] train_loss: 0.013385\n",
      "[252/00947] train_loss: 0.013218\n",
      "[252/00997] train_loss: 0.012342\n",
      "[252/01047] train_loss: 0.013277\n",
      "[252/01097] train_loss: 0.012652\n",
      "[252/01147] train_loss: 0.012961\n",
      "[252/01197] train_loss: 0.012527\n",
      "[253/00021] train_loss: 0.012910\n",
      "[253/00071] train_loss: 0.014031\n",
      "[253/00121] train_loss: 0.014106\n",
      "[253/00171] train_loss: 0.013909\n",
      "[253/00221] train_loss: 0.012854\n",
      "[253/00271] train_loss: 0.013417\n",
      "[253/00321] train_loss: 0.012958\n",
      "[253/00371] train_loss: 0.013025\n",
      "[253/00421] train_loss: 0.012983\n",
      "[253/00471] train_loss: 0.013028\n",
      "[253/00521] train_loss: 0.012687\n",
      "[253/00571] train_loss: 0.013960\n",
      "[253/00621] train_loss: 0.013083\n",
      "[253/00671] train_loss: 0.013414\n",
      "[253/00721] train_loss: 0.012385\n",
      "[253/00771] train_loss: 0.013176\n",
      "[253/00821] train_loss: 0.012905\n",
      "[253/00871] train_loss: 0.012985\n",
      "[253/00921] train_loss: 0.013269\n",
      "[253/00971] train_loss: 0.012754\n",
      "[253/01021] train_loss: 0.012749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[253/01071] train_loss: 0.012914\n",
      "[253/01121] train_loss: 0.012575\n",
      "[253/01171] train_loss: 0.013322\n",
      "[253/01221] train_loss: 0.012655\n",
      "[254/00045] train_loss: 0.013320\n",
      "[254/00095] train_loss: 0.014035\n",
      "[254/00145] train_loss: 0.013669\n",
      "[254/00195] train_loss: 0.013029\n",
      "[254/00245] train_loss: 0.012641\n",
      "[254/00295] train_loss: 0.013126\n",
      "[254/00345] train_loss: 0.012961\n",
      "[254/00395] train_loss: 0.012850\n",
      "[254/00445] train_loss: 0.012900\n",
      "[254/00495] train_loss: 0.012276\n",
      "[254/00545] train_loss: 0.013441\n",
      "[254/00595] train_loss: 0.014064\n",
      "[254/00645] train_loss: 0.012856\n",
      "[254/00695] train_loss: 0.013324\n",
      "[254/00745] train_loss: 0.013389\n",
      "[254/00795] train_loss: 0.012653\n",
      "[254/00845] train_loss: 0.013521\n",
      "[254/00895] train_loss: 0.012393\n",
      "[254/00945] train_loss: 0.013345\n",
      "[254/00995] train_loss: 0.013436\n",
      "[254/01045] train_loss: 0.013131\n",
      "[254/01095] train_loss: 0.012427\n",
      "[254/01145] train_loss: 0.012523\n",
      "[254/01195] train_loss: 0.012388\n",
      "[255/00019] train_loss: 0.013273\n",
      "[255/00069] train_loss: 0.012936\n",
      "[255/00119] train_loss: 0.014046\n",
      "[255/00169] train_loss: 0.012541\n",
      "[255/00219] train_loss: 0.013535\n",
      "[255/00269] train_loss: 0.013651\n",
      "[255/00319] train_loss: 0.012765\n",
      "[255/00369] train_loss: 0.012550\n",
      "[255/00419] train_loss: 0.012696\n",
      "[255/00469] train_loss: 0.013018\n",
      "[255/00519] train_loss: 0.013480\n",
      "[255/00569] train_loss: 0.013438\n",
      "[255/00619] train_loss: 0.012731\n",
      "[255/00669] train_loss: 0.013052\n",
      "[255/00719] train_loss: 0.012963\n",
      "[255/00769] train_loss: 0.012637\n",
      "[255/00819] train_loss: 0.012341\n",
      "[255/00869] train_loss: 0.013031\n",
      "[255/00919] train_loss: 0.013041\n",
      "[255/00969] train_loss: 0.012848\n",
      "[255/01019] train_loss: 0.012539\n",
      "[255/01069] train_loss: 0.013368\n",
      "[255/01119] train_loss: 0.013272\n",
      "[255/01169] train_loss: 0.012101\n",
      "[255/01219] train_loss: 0.012429\n",
      "[256/00043] train_loss: 0.013432\n",
      "[256/00093] train_loss: 0.012870\n",
      "[256/00143] train_loss: 0.013063\n",
      "[256/00193] train_loss: 0.013224\n",
      "[256/00243] train_loss: 0.012252\n",
      "[256/00293] train_loss: 0.013035\n",
      "[256/00343] train_loss: 0.013503\n",
      "[256/00393] train_loss: 0.012671\n",
      "[256/00443] train_loss: 0.012338\n",
      "[256/00493] train_loss: 0.012752\n",
      "[256/00543] train_loss: 0.013017\n",
      "[256/00593] train_loss: 0.013475\n",
      "[256/00643] train_loss: 0.013450\n",
      "[256/00693] train_loss: 0.013277\n",
      "[256/00743] train_loss: 0.012929\n",
      "[256/00793] train_loss: 0.012367\n",
      "[256/00843] train_loss: 0.012992\n",
      "[256/00893] train_loss: 0.013253\n",
      "[256/00943] train_loss: 0.012385\n",
      "[256/00993] train_loss: 0.012563\n",
      "[256/01043] train_loss: 0.012945\n",
      "[256/01093] train_loss: 0.012946\n",
      "[256/01143] train_loss: 0.013257\n",
      "[256/01193] train_loss: 0.013130\n",
      "[257/00017] train_loss: 0.012610\n",
      "[257/00067] train_loss: 0.013793\n",
      "[257/00117] train_loss: 0.013733\n",
      "[257/00167] train_loss: 0.013028\n",
      "[257/00217] train_loss: 0.012687\n",
      "[257/00267] train_loss: 0.013004\n",
      "[257/00317] train_loss: 0.012870\n",
      "[257/00367] train_loss: 0.012734\n",
      "[257/00417] train_loss: 0.012893\n",
      "[257/00467] train_loss: 0.012822\n",
      "[257/00517] train_loss: 0.012394\n",
      "[257/00567] train_loss: 0.013517\n",
      "[257/00617] train_loss: 0.012577\n",
      "[257/00667] train_loss: 0.013279\n",
      "[257/00717] train_loss: 0.013335\n",
      "[257/00767] train_loss: 0.013155\n",
      "[257/00817] train_loss: 0.013295\n",
      "[257/00867] train_loss: 0.012954\n",
      "[257/00917] train_loss: 0.012997\n",
      "[257/00967] train_loss: 0.013133\n",
      "[257/01017] train_loss: 0.012982\n",
      "[257/01067] train_loss: 0.013187\n",
      "[257/01117] train_loss: 0.013505\n",
      "[257/01167] train_loss: 0.012414\n",
      "[257/01217] train_loss: 0.012931\n",
      "[258/00041] train_loss: 0.013343\n",
      "[258/00091] train_loss: 0.013346\n",
      "[258/00141] train_loss: 0.013476\n",
      "[258/00191] train_loss: 0.013141\n",
      "[258/00241] train_loss: 0.013423\n",
      "[258/00291] train_loss: 0.014051\n",
      "[258/00341] train_loss: 0.012851\n",
      "[258/00391] train_loss: 0.012994\n",
      "[258/00441] train_loss: 0.012730\n",
      "[258/00491] train_loss: 0.013037\n",
      "[258/00541] train_loss: 0.012864\n",
      "[258/00591] train_loss: 0.012139\n",
      "[258/00641] train_loss: 0.012716\n",
      "[258/00691] train_loss: 0.013168\n",
      "[258/00741] train_loss: 0.013079\n",
      "[258/00791] train_loss: 0.012134\n",
      "[258/00841] train_loss: 0.012510\n",
      "[258/00891] train_loss: 0.012556\n",
      "[258/00941] train_loss: 0.013358\n",
      "[258/00991] train_loss: 0.012886\n",
      "[258/01041] train_loss: 0.013675\n",
      "[258/01091] train_loss: 0.012451\n",
      "[258/01141] train_loss: 0.013450\n",
      "[258/01191] train_loss: 0.013211\n",
      "[259/00015] train_loss: 0.012458\n",
      "[259/00065] train_loss: 0.013840\n",
      "[259/00115] train_loss: 0.013646\n",
      "[259/00165] train_loss: 0.013216\n",
      "[259/00215] train_loss: 0.013526\n",
      "[259/00265] train_loss: 0.013203\n",
      "[259/00315] train_loss: 0.013791\n",
      "[259/00365] train_loss: 0.012760\n",
      "[259/00415] train_loss: 0.012822\n",
      "[259/00465] train_loss: 0.012972\n",
      "[259/00515] train_loss: 0.013058\n",
      "[259/00565] train_loss: 0.013017\n",
      "[259/00615] train_loss: 0.013207\n",
      "[259/00665] train_loss: 0.012397\n",
      "[259/00715] train_loss: 0.012904\n",
      "[259/00765] train_loss: 0.013012\n",
      "[259/00815] train_loss: 0.013492\n",
      "[259/00865] train_loss: 0.012846\n",
      "[259/00915] train_loss: 0.013252\n",
      "[259/00965] train_loss: 0.012328\n",
      "[259/01015] train_loss: 0.012530\n",
      "[259/01065] train_loss: 0.013230\n",
      "[259/01115] train_loss: 0.013430\n",
      "[259/01165] train_loss: 0.012627\n",
      "[259/01215] train_loss: 0.013090\n",
      "[260/00039] train_loss: 0.013701\n",
      "[260/00089] train_loss: 0.013870\n",
      "[260/00139] train_loss: 0.013645\n",
      "[260/00189] train_loss: 0.013795\n",
      "[260/00239] train_loss: 0.013370\n",
      "[260/00289] train_loss: 0.013309\n",
      "[260/00339] train_loss: 0.012568\n",
      "[260/00389] train_loss: 0.013241\n",
      "[260/00439] train_loss: 0.012415\n",
      "[260/00489] train_loss: 0.013220\n",
      "[260/00539] train_loss: 0.012349\n",
      "[260/00589] train_loss: 0.012678\n",
      "[260/00639] train_loss: 0.013383\n",
      "[260/00689] train_loss: 0.012980\n",
      "[260/00739] train_loss: 0.013272\n",
      "[260/00789] train_loss: 0.012185\n",
      "[260/00839] train_loss: 0.012677\n",
      "[260/00889] train_loss: 0.013898\n",
      "[260/00939] train_loss: 0.013032\n",
      "[260/00989] train_loss: 0.012173\n",
      "[260/01039] train_loss: 0.013070\n",
      "[260/01089] train_loss: 0.012772\n",
      "[260/01139] train_loss: 0.012557\n",
      "[260/01189] train_loss: 0.012783\n",
      "[261/00013] train_loss: 0.013499\n",
      "[261/00063] train_loss: 0.013295\n",
      "[261/00113] train_loss: 0.013287\n",
      "[261/00163] train_loss: 0.013362\n",
      "[261/00213] train_loss: 0.013086\n",
      "[261/00263] train_loss: 0.013782\n",
      "[261/00313] train_loss: 0.013063\n",
      "[261/00363] train_loss: 0.012599\n",
      "[261/00413] train_loss: 0.013529\n",
      "[261/00463] train_loss: 0.012806\n",
      "[261/00513] train_loss: 0.012772\n",
      "[261/00563] train_loss: 0.012938\n",
      "[261/00613] train_loss: 0.012913\n",
      "[261/00663] train_loss: 0.013743\n",
      "[261/00713] train_loss: 0.013728\n",
      "[261/00763] train_loss: 0.012368\n",
      "[261/00813] train_loss: 0.012415\n",
      "[261/00863] train_loss: 0.013621\n",
      "[261/00913] train_loss: 0.013244\n",
      "[261/00963] train_loss: 0.012524\n",
      "[261/01013] train_loss: 0.013045\n",
      "[261/01063] train_loss: 0.012962\n",
      "[261/01113] train_loss: 0.012102\n",
      "[261/01163] train_loss: 0.012763\n",
      "[261/01213] train_loss: 0.012550\n",
      "[262/00037] train_loss: 0.013192\n",
      "[262/00087] train_loss: 0.013519\n",
      "[262/00137] train_loss: 0.013649\n",
      "[262/00187] train_loss: 0.013124\n",
      "[262/00237] train_loss: 0.013483\n",
      "[262/00287] train_loss: 0.013026\n",
      "[262/00337] train_loss: 0.013049\n",
      "[262/00387] train_loss: 0.012776\n",
      "[262/00437] train_loss: 0.013214\n",
      "[262/00487] train_loss: 0.013390\n",
      "[262/00537] train_loss: 0.012280\n",
      "[262/00587] train_loss: 0.013538\n",
      "[262/00637] train_loss: 0.013060\n",
      "[262/00687] train_loss: 0.013669\n",
      "[262/00737] train_loss: 0.013402\n",
      "[262/00787] train_loss: 0.012135\n",
      "[262/00837] train_loss: 0.012346\n",
      "[262/00887] train_loss: 0.012461\n",
      "[262/00937] train_loss: 0.012714\n",
      "[262/00987] train_loss: 0.012631\n",
      "[262/01037] train_loss: 0.013096\n",
      "[262/01087] train_loss: 0.012842\n",
      "[262/01137] train_loss: 0.012386\n",
      "[262/01187] train_loss: 0.012665\n",
      "[263/00011] train_loss: 0.012645\n",
      "[263/00061] train_loss: 0.014187\n",
      "[263/00111] train_loss: 0.013354\n",
      "[263/00161] train_loss: 0.013636\n",
      "[263/00211] train_loss: 0.013324\n",
      "[263/00261] train_loss: 0.013930\n",
      "[263/00311] train_loss: 0.013160\n",
      "[263/00361] train_loss: 0.013667\n",
      "[263/00411] train_loss: 0.013025\n",
      "[263/00461] train_loss: 0.013201\n",
      "[263/00511] train_loss: 0.012822\n",
      "[263/00561] train_loss: 0.012351\n",
      "[263/00611] train_loss: 0.012411\n",
      "[263/00661] train_loss: 0.012348\n",
      "[263/00711] train_loss: 0.012649\n",
      "[263/00761] train_loss: 0.013038\n",
      "[263/00811] train_loss: 0.013236\n",
      "[263/00861] train_loss: 0.013223\n",
      "[263/00911] train_loss: 0.012695\n",
      "[263/00961] train_loss: 0.012934\n",
      "[263/01011] train_loss: 0.013445\n",
      "[263/01061] train_loss: 0.011928\n",
      "[263/01111] train_loss: 0.012360\n",
      "[263/01161] train_loss: 0.012385\n",
      "[263/01211] train_loss: 0.012469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[264/00035] train_loss: 0.013920\n",
      "[264/00085] train_loss: 0.013527\n",
      "[264/00135] train_loss: 0.013271\n",
      "[264/00185] train_loss: 0.012820\n",
      "[264/00235] train_loss: 0.012719\n",
      "[264/00285] train_loss: 0.013951\n",
      "[264/00335] train_loss: 0.013117\n",
      "[264/00385] train_loss: 0.013278\n",
      "[264/00435] train_loss: 0.013007\n",
      "[264/00485] train_loss: 0.012792\n",
      "[264/00535] train_loss: 0.012652\n",
      "[264/00585] train_loss: 0.014257\n",
      "[264/00635] train_loss: 0.012991\n",
      "[264/00685] train_loss: 0.013166\n",
      "[264/00735] train_loss: 0.013080\n",
      "[264/00785] train_loss: 0.012569\n",
      "[264/00835] train_loss: 0.012130\n",
      "[264/00885] train_loss: 0.012481\n",
      "[264/00935] train_loss: 0.012634\n",
      "[264/00985] train_loss: 0.012927\n",
      "[264/01035] train_loss: 0.013320\n",
      "[264/01085] train_loss: 0.013197\n",
      "[264/01135] train_loss: 0.012924\n",
      "[264/01185] train_loss: 0.012943\n",
      "[265/00009] train_loss: 0.012706\n",
      "[265/00059] train_loss: 0.013792\n",
      "[265/00109] train_loss: 0.014022\n",
      "[265/00159] train_loss: 0.013217\n",
      "[265/00209] train_loss: 0.013143\n",
      "[265/00259] train_loss: 0.013321\n",
      "[265/00309] train_loss: 0.012358\n",
      "[265/00359] train_loss: 0.013026\n",
      "[265/00409] train_loss: 0.012709\n",
      "[265/00459] train_loss: 0.012682\n",
      "[265/00509] train_loss: 0.014048\n",
      "[265/00559] train_loss: 0.013897\n",
      "[265/00609] train_loss: 0.012928\n",
      "[265/00659] train_loss: 0.012900\n",
      "[265/00709] train_loss: 0.012967\n",
      "[265/00759] train_loss: 0.012789\n",
      "[265/00809] train_loss: 0.012867\n",
      "[265/00859] train_loss: 0.012493\n",
      "[265/00909] train_loss: 0.012982\n",
      "[265/00959] train_loss: 0.013080\n",
      "[265/01009] train_loss: 0.012138\n",
      "[265/01059] train_loss: 0.012907\n",
      "[265/01109] train_loss: 0.012628\n",
      "[265/01159] train_loss: 0.012969\n",
      "[265/01209] train_loss: 0.012219\n",
      "[266/00033] train_loss: 0.014176\n",
      "[266/00083] train_loss: 0.013125\n",
      "[266/00133] train_loss: 0.014122\n",
      "[266/00183] train_loss: 0.012495\n",
      "[266/00233] train_loss: 0.013694\n",
      "[266/00283] train_loss: 0.012398\n",
      "[266/00333] train_loss: 0.013100\n",
      "[266/00383] train_loss: 0.013163\n",
      "[266/00433] train_loss: 0.013400\n",
      "[266/00483] train_loss: 0.012869\n",
      "[266/00533] train_loss: 0.013325\n",
      "[266/00583] train_loss: 0.012694\n",
      "[266/00633] train_loss: 0.012472\n",
      "[266/00683] train_loss: 0.012551\n",
      "[266/00733] train_loss: 0.012000\n",
      "[266/00783] train_loss: 0.012625\n",
      "[266/00833] train_loss: 0.013452\n",
      "[266/00883] train_loss: 0.012694\n",
      "[266/00933] train_loss: 0.012888\n",
      "[266/00983] train_loss: 0.012867\n",
      "[266/01033] train_loss: 0.012443\n",
      "[266/01083] train_loss: 0.012626\n",
      "[266/01133] train_loss: 0.012731\n",
      "[266/01183] train_loss: 0.012637\n",
      "[267/00007] train_loss: 0.013103\n",
      "[267/00057] train_loss: 0.013793\n",
      "[267/00107] train_loss: 0.012820\n",
      "[267/00157] train_loss: 0.014163\n",
      "[267/00207] train_loss: 0.012763\n",
      "[267/00257] train_loss: 0.013491\n",
      "[267/00307] train_loss: 0.012687\n",
      "[267/00357] train_loss: 0.012868\n",
      "[267/00407] train_loss: 0.013109\n",
      "[267/00457] train_loss: 0.012296\n",
      "[267/00507] train_loss: 0.013192\n",
      "[267/00557] train_loss: 0.012353\n",
      "[267/00607] train_loss: 0.012817\n",
      "[267/00657] train_loss: 0.012886\n",
      "[267/00707] train_loss: 0.013113\n",
      "[267/00757] train_loss: 0.013073\n",
      "[267/00807] train_loss: 0.012191\n",
      "[267/00857] train_loss: 0.012632\n",
      "[267/00907] train_loss: 0.012092\n",
      "[267/00957] train_loss: 0.012764\n",
      "[267/01007] train_loss: 0.013636\n",
      "[267/01057] train_loss: 0.012752\n",
      "[267/01107] train_loss: 0.012589\n",
      "[267/01157] train_loss: 0.013492\n",
      "[267/01207] train_loss: 0.012434\n",
      "[268/00031] train_loss: 0.013399\n",
      "[268/00081] train_loss: 0.013364\n",
      "[268/00131] train_loss: 0.014016\n",
      "[268/00181] train_loss: 0.013502\n",
      "[268/00231] train_loss: 0.012993\n",
      "[268/00281] train_loss: 0.012524\n",
      "[268/00331] train_loss: 0.013095\n",
      "[268/00381] train_loss: 0.013723\n",
      "[268/00431] train_loss: 0.013325\n",
      "[268/00481] train_loss: 0.012649\n",
      "[268/00531] train_loss: 0.013225\n",
      "[268/00581] train_loss: 0.012646\n",
      "[268/00631] train_loss: 0.013295\n",
      "[268/00681] train_loss: 0.013369\n",
      "[268/00731] train_loss: 0.012927\n",
      "[268/00781] train_loss: 0.013013\n",
      "[268/00831] train_loss: 0.013294\n",
      "[268/00881] train_loss: 0.013195\n",
      "[268/00931] train_loss: 0.011781\n",
      "[268/00981] train_loss: 0.013164\n",
      "[268/01031] train_loss: 0.012409\n",
      "[268/01081] train_loss: 0.012676\n",
      "[268/01131] train_loss: 0.012396\n",
      "[268/01181] train_loss: 0.013010\n",
      "[269/00005] train_loss: 0.013047\n",
      "[269/00055] train_loss: 0.013772\n",
      "[269/00105] train_loss: 0.013354\n",
      "[269/00155] train_loss: 0.013587\n",
      "[269/00205] train_loss: 0.013212\n",
      "[269/00255] train_loss: 0.012488\n",
      "[269/00305] train_loss: 0.012501\n",
      "[269/00355] train_loss: 0.012584\n",
      "[269/00405] train_loss: 0.013034\n",
      "[269/00455] train_loss: 0.013734\n",
      "[269/00505] train_loss: 0.012582\n",
      "[269/00555] train_loss: 0.012660\n",
      "[269/00605] train_loss: 0.012878\n",
      "[269/00655] train_loss: 0.012868\n",
      "[269/00705] train_loss: 0.012799\n",
      "[269/00755] train_loss: 0.012390\n",
      "[269/00805] train_loss: 0.012630\n",
      "[269/00855] train_loss: 0.013068\n",
      "[269/00905] train_loss: 0.012137\n",
      "[269/00955] train_loss: 0.013273\n",
      "[269/01005] train_loss: 0.012790\n",
      "[269/01055] train_loss: 0.012050\n",
      "[269/01105] train_loss: 0.012701\n",
      "[269/01155] train_loss: 0.012834\n",
      "[269/01205] train_loss: 0.012799\n",
      "[270/00029] train_loss: 0.012880\n",
      "[270/00079] train_loss: 0.013735\n",
      "[270/00129] train_loss: 0.013282\n",
      "[270/00179] train_loss: 0.013540\n",
      "[270/00229] train_loss: 0.013053\n",
      "[270/00279] train_loss: 0.013579\n",
      "[270/00329] train_loss: 0.013019\n",
      "[270/00379] train_loss: 0.012926\n",
      "[270/00429] train_loss: 0.012887\n",
      "[270/00479] train_loss: 0.012273\n",
      "[270/00529] train_loss: 0.012680\n",
      "[270/00579] train_loss: 0.012773\n",
      "[270/00629] train_loss: 0.013180\n",
      "[270/00679] train_loss: 0.013099\n",
      "[270/00729] train_loss: 0.012594\n",
      "[270/00779] train_loss: 0.012646\n",
      "[270/00829] train_loss: 0.012469\n",
      "[270/00879] train_loss: 0.012528\n",
      "[270/00929] train_loss: 0.013359\n",
      "[270/00979] train_loss: 0.012534\n",
      "[270/01029] train_loss: 0.012830\n",
      "[270/01079] train_loss: 0.012952\n",
      "[270/01129] train_loss: 0.013170\n",
      "[270/01179] train_loss: 0.012344\n",
      "[271/00003] train_loss: 0.012672\n",
      "[271/00053] train_loss: 0.013742\n",
      "[271/00103] train_loss: 0.013868\n",
      "[271/00153] train_loss: 0.013120\n",
      "[271/00203] train_loss: 0.013237\n",
      "[271/00253] train_loss: 0.012827\n",
      "[271/00303] train_loss: 0.012702\n",
      "[271/00353] train_loss: 0.012694\n",
      "[271/00403] train_loss: 0.013977\n",
      "[271/00453] train_loss: 0.013252\n",
      "[271/00503] train_loss: 0.013283\n",
      "[271/00553] train_loss: 0.012069\n",
      "[271/00603] train_loss: 0.013397\n",
      "[271/00653] train_loss: 0.013543\n",
      "[271/00703] train_loss: 0.012288\n",
      "[271/00753] train_loss: 0.012221\n",
      "[271/00803] train_loss: 0.013131\n",
      "[271/00853] train_loss: 0.013172\n",
      "[271/00903] train_loss: 0.012906\n",
      "[271/00953] train_loss: 0.013552\n",
      "[271/01003] train_loss: 0.012489\n",
      "[271/01053] train_loss: 0.012355\n",
      "[271/01103] train_loss: 0.012729\n",
      "[271/01153] train_loss: 0.012971\n",
      "[271/01203] train_loss: 0.013074\n",
      "[272/00027] train_loss: 0.013091\n",
      "[272/00077] train_loss: 0.013122\n",
      "[272/00127] train_loss: 0.013921\n",
      "[272/00177] train_loss: 0.012779\n",
      "[272/00227] train_loss: 0.013282\n",
      "[272/00277] train_loss: 0.012511\n",
      "[272/00327] train_loss: 0.013228\n",
      "[272/00377] train_loss: 0.012912\n",
      "[272/00427] train_loss: 0.012825\n",
      "[272/00477] train_loss: 0.013087\n",
      "[272/00527] train_loss: 0.013201\n",
      "[272/00577] train_loss: 0.012748\n",
      "[272/00627] train_loss: 0.012885\n",
      "[272/00677] train_loss: 0.012324\n",
      "[272/00727] train_loss: 0.012935\n",
      "[272/00777] train_loss: 0.012864\n",
      "[272/00827] train_loss: 0.012081\n",
      "[272/00877] train_loss: 0.012947\n",
      "[272/00927] train_loss: 0.012068\n",
      "[272/00977] train_loss: 0.012662\n",
      "[272/01027] train_loss: 0.012970\n",
      "[272/01077] train_loss: 0.012344\n",
      "[272/01127] train_loss: 0.012149\n",
      "[272/01177] train_loss: 0.013319\n",
      "[273/00001] train_loss: 0.012619\n",
      "[273/00051] train_loss: 0.013177\n",
      "[273/00101] train_loss: 0.014532\n",
      "[273/00151] train_loss: 0.012734\n",
      "[273/00201] train_loss: 0.013105\n",
      "[273/00251] train_loss: 0.012791\n",
      "[273/00301] train_loss: 0.013159\n",
      "[273/00351] train_loss: 0.012886\n",
      "[273/00401] train_loss: 0.012859\n",
      "[273/00451] train_loss: 0.013195\n",
      "[273/00501] train_loss: 0.012776\n",
      "[273/00551] train_loss: 0.013222\n",
      "[273/00601] train_loss: 0.012644\n",
      "[273/00651] train_loss: 0.012829\n",
      "[273/00701] train_loss: 0.013436\n",
      "[273/00751] train_loss: 0.012564\n",
      "[273/00801] train_loss: 0.012866\n",
      "[273/00851] train_loss: 0.013362\n",
      "[273/00901] train_loss: 0.013092\n",
      "[273/00951] train_loss: 0.013481\n",
      "[273/01001] train_loss: 0.012984\n",
      "[273/01051] train_loss: 0.013029\n",
      "[273/01101] train_loss: 0.012573\n",
      "[273/01151] train_loss: 0.012702\n",
      "[273/01201] train_loss: 0.012572\n",
      "[274/00025] train_loss: 0.013413\n",
      "[274/00075] train_loss: 0.013582\n",
      "[274/00125] train_loss: 0.012909\n",
      "[274/00175] train_loss: 0.013221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[274/00225] train_loss: 0.012720\n",
      "[274/00275] train_loss: 0.013542\n",
      "[274/00325] train_loss: 0.012535\n",
      "[274/00375] train_loss: 0.012286\n",
      "[274/00425] train_loss: 0.012455\n",
      "[274/00475] train_loss: 0.013603\n",
      "[274/00525] train_loss: 0.012804\n",
      "[274/00575] train_loss: 0.013455\n",
      "[274/00625] train_loss: 0.012342\n",
      "[274/00675] train_loss: 0.012531\n",
      "[274/00725] train_loss: 0.012534\n",
      "[274/00775] train_loss: 0.013615\n",
      "[274/00825] train_loss: 0.013003\n",
      "[274/00875] train_loss: 0.012406\n",
      "[274/00925] train_loss: 0.013302\n",
      "[274/00975] train_loss: 0.012807\n",
      "[274/01025] train_loss: 0.012688\n",
      "[274/01075] train_loss: 0.013385\n",
      "[274/01125] train_loss: 0.012558\n",
      "[274/01175] train_loss: 0.013057\n",
      "[274/01225] train_loss: 0.013410\n",
      "[275/00049] train_loss: 0.013718\n",
      "[275/00099] train_loss: 0.013647\n",
      "[275/00149] train_loss: 0.012977\n",
      "[275/00199] train_loss: 0.013047\n",
      "[275/00249] train_loss: 0.013411\n",
      "[275/00299] train_loss: 0.013328\n",
      "[275/00349] train_loss: 0.012428\n",
      "[275/00399] train_loss: 0.012769\n",
      "[275/00449] train_loss: 0.012598\n",
      "[275/00499] train_loss: 0.012370\n",
      "[275/00549] train_loss: 0.012841\n",
      "[275/00599] train_loss: 0.013116\n",
      "[275/00649] train_loss: 0.013074\n",
      "[275/00699] train_loss: 0.013557\n",
      "[275/00749] train_loss: 0.012811\n",
      "[275/00799] train_loss: 0.012211\n",
      "[275/00849] train_loss: 0.013141\n",
      "[275/00899] train_loss: 0.013043\n",
      "[275/00949] train_loss: 0.012826\n",
      "[275/00999] train_loss: 0.012676\n",
      "[275/01049] train_loss: 0.012421\n",
      "[275/01099] train_loss: 0.013637\n",
      "[275/01149] train_loss: 0.012899\n",
      "[275/01199] train_loss: 0.012577\n",
      "[276/00023] train_loss: 0.012962\n",
      "[276/00073] train_loss: 0.014206\n",
      "[276/00123] train_loss: 0.013990\n",
      "[276/00173] train_loss: 0.013460\n",
      "[276/00223] train_loss: 0.013200\n",
      "[276/00273] train_loss: 0.012375\n",
      "[276/00323] train_loss: 0.013124\n",
      "[276/00373] train_loss: 0.012065\n",
      "[276/00423] train_loss: 0.013053\n",
      "[276/00473] train_loss: 0.013213\n",
      "[276/00523] train_loss: 0.012556\n",
      "[276/00573] train_loss: 0.012532\n",
      "[276/00623] train_loss: 0.013059\n",
      "[276/00673] train_loss: 0.012807\n",
      "[276/00723] train_loss: 0.012341\n",
      "[276/00773] train_loss: 0.012785\n",
      "[276/00823] train_loss: 0.012948\n",
      "[276/00873] train_loss: 0.012141\n",
      "[276/00923] train_loss: 0.012426\n",
      "[276/00973] train_loss: 0.013277\n",
      "[276/01023] train_loss: 0.012431\n",
      "[276/01073] train_loss: 0.012696\n",
      "[276/01123] train_loss: 0.012510\n",
      "[276/01173] train_loss: 0.012546\n",
      "[276/01223] train_loss: 0.012719\n",
      "[277/00047] train_loss: 0.013441\n",
      "[277/00097] train_loss: 0.012948\n",
      "[277/00147] train_loss: 0.012997\n",
      "[277/00197] train_loss: 0.013075\n",
      "[277/00247] train_loss: 0.012711\n",
      "[277/00297] train_loss: 0.013156\n",
      "[277/00347] train_loss: 0.012485\n",
      "[277/00397] train_loss: 0.013553\n",
      "[277/00447] train_loss: 0.013080\n",
      "[277/00497] train_loss: 0.012829\n",
      "[277/00547] train_loss: 0.012863\n",
      "[277/00597] train_loss: 0.012762\n",
      "[277/00647] train_loss: 0.012647\n",
      "[277/00697] train_loss: 0.013221\n",
      "[277/00747] train_loss: 0.012632\n",
      "[277/00797] train_loss: 0.012348\n",
      "[277/00847] train_loss: 0.013236\n",
      "[277/00897] train_loss: 0.013228\n",
      "[277/00947] train_loss: 0.012832\n",
      "[277/00997] train_loss: 0.012436\n",
      "[277/01047] train_loss: 0.011990\n",
      "[277/01097] train_loss: 0.012886\n",
      "[277/01147] train_loss: 0.013683\n",
      "[277/01197] train_loss: 0.012538\n",
      "[278/00021] train_loss: 0.012578\n",
      "[278/00071] train_loss: 0.013934\n",
      "[278/00121] train_loss: 0.013382\n",
      "[278/00171] train_loss: 0.013073\n",
      "[278/00221] train_loss: 0.013344\n",
      "[278/00271] train_loss: 0.013643\n",
      "[278/00321] train_loss: 0.012913\n",
      "[278/00371] train_loss: 0.012701\n",
      "[278/00421] train_loss: 0.013051\n",
      "[278/00471] train_loss: 0.012362\n",
      "[278/00521] train_loss: 0.012367\n",
      "[278/00571] train_loss: 0.013019\n",
      "[278/00621] train_loss: 0.012286\n",
      "[278/00671] train_loss: 0.012847\n",
      "[278/00721] train_loss: 0.012521\n",
      "[278/00771] train_loss: 0.012168\n",
      "[278/00821] train_loss: 0.012246\n",
      "[278/00871] train_loss: 0.012715\n",
      "[278/00921] train_loss: 0.012603\n",
      "[278/00971] train_loss: 0.012443\n",
      "[278/01021] train_loss: 0.013141\n",
      "[278/01071] train_loss: 0.013468\n",
      "[278/01121] train_loss: 0.012388\n",
      "[278/01171] train_loss: 0.012565\n",
      "[278/01221] train_loss: 0.012218\n",
      "[279/00045] train_loss: 0.013701\n",
      "[279/00095] train_loss: 0.014043\n",
      "[279/00145] train_loss: 0.012795\n",
      "[279/00195] train_loss: 0.012995\n",
      "[279/00245] train_loss: 0.012611\n",
      "[279/00295] train_loss: 0.013397\n",
      "[279/00345] train_loss: 0.013099\n",
      "[279/00395] train_loss: 0.012750\n",
      "[279/00445] train_loss: 0.012502\n",
      "[279/00495] train_loss: 0.012467\n",
      "[279/00545] train_loss: 0.013494\n",
      "[279/00595] train_loss: 0.012430\n",
      "[279/00645] train_loss: 0.012326\n",
      "[279/00695] train_loss: 0.013248\n",
      "[279/00745] train_loss: 0.012760\n",
      "[279/00795] train_loss: 0.012384\n",
      "[279/00845] train_loss: 0.013097\n",
      "[279/00895] train_loss: 0.013002\n",
      "[279/00945] train_loss: 0.012479\n",
      "[279/00995] train_loss: 0.012969\n",
      "[279/01045] train_loss: 0.012609\n",
      "[279/01095] train_loss: 0.012773\n",
      "[279/01145] train_loss: 0.012457\n",
      "[279/01195] train_loss: 0.012464\n",
      "[280/00019] train_loss: 0.012572\n",
      "[280/00069] train_loss: 0.013374\n",
      "[280/00119] train_loss: 0.013260\n",
      "[280/00169] train_loss: 0.013584\n",
      "[280/00219] train_loss: 0.013453\n",
      "[280/00269] train_loss: 0.013085\n",
      "[280/00319] train_loss: 0.013515\n",
      "[280/00369] train_loss: 0.012798\n",
      "[280/00419] train_loss: 0.013090\n",
      "[280/00469] train_loss: 0.012747\n",
      "[280/00519] train_loss: 0.013209\n",
      "[280/00569] train_loss: 0.013306\n",
      "[280/00619] train_loss: 0.011825\n",
      "[280/00669] train_loss: 0.012465\n",
      "[280/00719] train_loss: 0.013306\n",
      "[280/00769] train_loss: 0.012789\n",
      "[280/00819] train_loss: 0.012791\n",
      "[280/00869] train_loss: 0.012924\n",
      "[280/00919] train_loss: 0.012963\n",
      "[280/00969] train_loss: 0.012919\n",
      "[280/01019] train_loss: 0.012447\n",
      "[280/01069] train_loss: 0.012305\n",
      "[280/01119] train_loss: 0.012489\n",
      "[280/01169] train_loss: 0.012763\n",
      "[280/01219] train_loss: 0.012172\n",
      "[281/00043] train_loss: 0.013102\n",
      "[281/00093] train_loss: 0.013377\n",
      "[281/00143] train_loss: 0.013468\n",
      "[281/00193] train_loss: 0.013246\n",
      "[281/00243] train_loss: 0.012959\n",
      "[281/00293] train_loss: 0.012553\n",
      "[281/00343] train_loss: 0.013262\n",
      "[281/00393] train_loss: 0.012971\n",
      "[281/00443] train_loss: 0.012915\n",
      "[281/00493] train_loss: 0.014154\n",
      "[281/00543] train_loss: 0.012267\n",
      "[281/00593] train_loss: 0.013022\n",
      "[281/00643] train_loss: 0.013625\n",
      "[281/00693] train_loss: 0.013009\n",
      "[281/00743] train_loss: 0.012477\n",
      "[281/00793] train_loss: 0.012516\n",
      "[281/00843] train_loss: 0.013503\n",
      "[281/00893] train_loss: 0.012614\n",
      "[281/00943] train_loss: 0.012545\n",
      "[281/00993] train_loss: 0.012399\n",
      "[281/01043] train_loss: 0.012219\n",
      "[281/01093] train_loss: 0.012491\n",
      "[281/01143] train_loss: 0.012702\n",
      "[281/01193] train_loss: 0.013279\n",
      "[282/00017] train_loss: 0.012747\n",
      "[282/00067] train_loss: 0.013349\n",
      "[282/00117] train_loss: 0.013001\n",
      "[282/00167] train_loss: 0.013563\n",
      "[282/00217] train_loss: 0.013204\n",
      "[282/00267] train_loss: 0.012959\n",
      "[282/00317] train_loss: 0.012886\n",
      "[282/00367] train_loss: 0.013181\n",
      "[282/00417] train_loss: 0.013447\n",
      "[282/00467] train_loss: 0.012395\n",
      "[282/00517] train_loss: 0.012311\n",
      "[282/00567] train_loss: 0.012664\n",
      "[282/00617] train_loss: 0.012291\n",
      "[282/00667] train_loss: 0.013000\n",
      "[282/00717] train_loss: 0.012898\n",
      "[282/00767] train_loss: 0.012574\n",
      "[282/00817] train_loss: 0.012931\n",
      "[282/00867] train_loss: 0.012988\n",
      "[282/00917] train_loss: 0.013063\n",
      "[282/00967] train_loss: 0.012884\n",
      "[282/01017] train_loss: 0.012727\n",
      "[282/01067] train_loss: 0.012722\n",
      "[282/01117] train_loss: 0.012321\n",
      "[282/01167] train_loss: 0.012508\n",
      "[282/01217] train_loss: 0.012712\n",
      "[283/00041] train_loss: 0.014092\n",
      "[283/00091] train_loss: 0.012834\n",
      "[283/00141] train_loss: 0.012903\n",
      "[283/00191] train_loss: 0.012489\n",
      "[283/00241] train_loss: 0.012647\n",
      "[283/00291] train_loss: 0.013799\n",
      "[283/00341] train_loss: 0.012349\n",
      "[283/00391] train_loss: 0.013494\n",
      "[283/00441] train_loss: 0.012096\n",
      "[283/00491] train_loss: 0.012817\n",
      "[283/00541] train_loss: 0.013027\n",
      "[283/00591] train_loss: 0.012521\n",
      "[283/00641] train_loss: 0.012906\n",
      "[283/00691] train_loss: 0.012488\n",
      "[283/00741] train_loss: 0.012502\n",
      "[283/00791] train_loss: 0.012804\n",
      "[283/00841] train_loss: 0.012616\n",
      "[283/00891] train_loss: 0.012119\n",
      "[283/00941] train_loss: 0.013891\n",
      "[283/00991] train_loss: 0.012675\n",
      "[283/01041] train_loss: 0.012418\n",
      "[283/01091] train_loss: 0.012566\n",
      "[283/01141] train_loss: 0.013267\n",
      "[283/01191] train_loss: 0.012826\n",
      "[284/00015] train_loss: 0.012918\n",
      "[284/00065] train_loss: 0.014283\n",
      "[284/00115] train_loss: 0.013004\n",
      "[284/00165] train_loss: 0.012599\n",
      "[284/00215] train_loss: 0.013417\n",
      "[284/00265] train_loss: 0.013153\n",
      "[284/00315] train_loss: 0.013211\n",
      "[284/00365] train_loss: 0.013135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[284/00415] train_loss: 0.012690\n",
      "[284/00465] train_loss: 0.012477\n",
      "[284/00515] train_loss: 0.012817\n",
      "[284/00565] train_loss: 0.012910\n",
      "[284/00615] train_loss: 0.013421\n",
      "[284/00665] train_loss: 0.013323\n",
      "[284/00715] train_loss: 0.012916\n",
      "[284/00765] train_loss: 0.012702\n",
      "[284/00815] train_loss: 0.012359\n",
      "[284/00865] train_loss: 0.013166\n",
      "[284/00915] train_loss: 0.012667\n",
      "[284/00965] train_loss: 0.012873\n",
      "[284/01015] train_loss: 0.012843\n",
      "[284/01065] train_loss: 0.013104\n",
      "[284/01115] train_loss: 0.012828\n",
      "[284/01165] train_loss: 0.012585\n",
      "[284/01215] train_loss: 0.011846\n",
      "[285/00039] train_loss: 0.013463\n",
      "[285/00089] train_loss: 0.013407\n",
      "[285/00139] train_loss: 0.013210\n",
      "[285/00189] train_loss: 0.012473\n",
      "[285/00239] train_loss: 0.012676\n",
      "[285/00289] train_loss: 0.013484\n",
      "[285/00339] train_loss: 0.012211\n",
      "[285/00389] train_loss: 0.013262\n",
      "[285/00439] train_loss: 0.013590\n",
      "[285/00489] train_loss: 0.013266\n",
      "[285/00539] train_loss: 0.012870\n",
      "[285/00589] train_loss: 0.011877\n",
      "[285/00639] train_loss: 0.012088\n",
      "[285/00689] train_loss: 0.012690\n",
      "[285/00739] train_loss: 0.012445\n",
      "[285/00789] train_loss: 0.013502\n",
      "[285/00839] train_loss: 0.012444\n",
      "[285/00889] train_loss: 0.012626\n",
      "[285/00939] train_loss: 0.012786\n",
      "[285/00989] train_loss: 0.012905\n",
      "[285/01039] train_loss: 0.013081\n",
      "[285/01089] train_loss: 0.012384\n",
      "[285/01139] train_loss: 0.013423\n",
      "[285/01189] train_loss: 0.013279\n",
      "[286/00013] train_loss: 0.013283\n",
      "[286/00063] train_loss: 0.014142\n",
      "[286/00113] train_loss: 0.013253\n",
      "[286/00163] train_loss: 0.012998\n",
      "[286/00213] train_loss: 0.013290\n",
      "[286/00263] train_loss: 0.013000\n",
      "[286/00313] train_loss: 0.012744\n",
      "[286/00363] train_loss: 0.012125\n",
      "[286/00413] train_loss: 0.013996\n",
      "[286/00463] train_loss: 0.012941\n",
      "[286/00513] train_loss: 0.012673\n",
      "[286/00563] train_loss: 0.012497\n",
      "[286/00613] train_loss: 0.012204\n",
      "[286/00663] train_loss: 0.013040\n",
      "[286/00713] train_loss: 0.012107\n",
      "[286/00763] train_loss: 0.012874\n",
      "[286/00813] train_loss: 0.013069\n",
      "[286/00863] train_loss: 0.012229\n",
      "[286/00913] train_loss: 0.012445\n",
      "[286/00963] train_loss: 0.013148\n",
      "[286/01013] train_loss: 0.013108\n",
      "[286/01063] train_loss: 0.012572\n",
      "[286/01113] train_loss: 0.012643\n",
      "[286/01163] train_loss: 0.012141\n",
      "[286/01213] train_loss: 0.012666\n",
      "[287/00037] train_loss: 0.012838\n",
      "[287/00087] train_loss: 0.013215\n",
      "[287/00137] train_loss: 0.013587\n",
      "[287/00187] train_loss: 0.012937\n",
      "[287/00237] train_loss: 0.013200\n",
      "[287/00287] train_loss: 0.012970\n",
      "[287/00337] train_loss: 0.012246\n",
      "[287/00387] train_loss: 0.013468\n",
      "[287/00437] train_loss: 0.013485\n",
      "[287/00487] train_loss: 0.013181\n",
      "[287/00537] train_loss: 0.012812\n",
      "[287/00587] train_loss: 0.012753\n",
      "[287/00637] train_loss: 0.013080\n",
      "[287/00687] train_loss: 0.012075\n",
      "[287/00737] train_loss: 0.012703\n",
      "[287/00787] train_loss: 0.012302\n",
      "[287/00837] train_loss: 0.012409\n",
      "[287/00887] train_loss: 0.012202\n",
      "[287/00937] train_loss: 0.012658\n",
      "[287/00987] train_loss: 0.012937\n",
      "[287/01037] train_loss: 0.012346\n",
      "[287/01087] train_loss: 0.013352\n",
      "[287/01137] train_loss: 0.012847\n",
      "[287/01187] train_loss: 0.012399\n",
      "[288/00011] train_loss: 0.012601\n",
      "[288/00061] train_loss: 0.012904\n",
      "[288/00111] train_loss: 0.013413\n",
      "[288/00161] train_loss: 0.012930\n",
      "[288/00211] train_loss: 0.012787\n",
      "[288/00261] train_loss: 0.013145\n",
      "[288/00311] train_loss: 0.013020\n",
      "[288/00361] train_loss: 0.012744\n",
      "[288/00411] train_loss: 0.011718\n",
      "[288/00461] train_loss: 0.013459\n",
      "[288/00511] train_loss: 0.012968\n",
      "[288/00561] train_loss: 0.012634\n",
      "[288/00611] train_loss: 0.013115\n",
      "[288/00661] train_loss: 0.012576\n",
      "[288/00711] train_loss: 0.012440\n",
      "[288/00761] train_loss: 0.012698\n",
      "[288/00811] train_loss: 0.013123\n",
      "[288/00861] train_loss: 0.013038\n",
      "[288/00911] train_loss: 0.012290\n",
      "[288/00961] train_loss: 0.011939\n",
      "[288/01011] train_loss: 0.013351\n",
      "[288/01061] train_loss: 0.012692\n",
      "[288/01111] train_loss: 0.013195\n",
      "[288/01161] train_loss: 0.012397\n",
      "[288/01211] train_loss: 0.012452\n",
      "[289/00035] train_loss: 0.012828\n",
      "[289/00085] train_loss: 0.013653\n",
      "[289/00135] train_loss: 0.012837\n",
      "[289/00185] train_loss: 0.012416\n",
      "[289/00235] train_loss: 0.013804\n",
      "[289/00285] train_loss: 0.012700\n",
      "[289/00335] train_loss: 0.012635\n",
      "[289/00385] train_loss: 0.013665\n",
      "[289/00435] train_loss: 0.013158\n",
      "[289/00485] train_loss: 0.012351\n",
      "[289/00535] train_loss: 0.012184\n",
      "[289/00585] train_loss: 0.012864\n",
      "[289/00635] train_loss: 0.012878\n",
      "[289/00685] train_loss: 0.012712\n",
      "[289/00735] train_loss: 0.012658\n",
      "[289/00785] train_loss: 0.012406\n",
      "[289/00835] train_loss: 0.013037\n",
      "[289/00885] train_loss: 0.012815\n",
      "[289/00935] train_loss: 0.012443\n",
      "[289/00985] train_loss: 0.012945\n",
      "[289/01035] train_loss: 0.012936\n",
      "[289/01085] train_loss: 0.012318\n",
      "[289/01135] train_loss: 0.012139\n",
      "[289/01185] train_loss: 0.012671\n",
      "[290/00009] train_loss: 0.012044\n",
      "[290/00059] train_loss: 0.013705\n",
      "[290/00109] train_loss: 0.013542\n",
      "[290/00159] train_loss: 0.012967\n",
      "[290/00209] train_loss: 0.013726\n",
      "[290/00259] train_loss: 0.013062\n",
      "[290/00309] train_loss: 0.013298\n",
      "[290/00359] train_loss: 0.013963\n",
      "[290/00409] train_loss: 0.012644\n",
      "[290/00459] train_loss: 0.012685\n",
      "[290/00509] train_loss: 0.012422\n",
      "[290/00559] train_loss: 0.012055\n",
      "[290/00609] train_loss: 0.012994\n",
      "[290/00659] train_loss: 0.013166\n",
      "[290/00709] train_loss: 0.013007\n",
      "[290/00759] train_loss: 0.013005\n",
      "[290/00809] train_loss: 0.012204\n",
      "[290/00859] train_loss: 0.012179\n",
      "[290/00909] train_loss: 0.012936\n",
      "[290/00959] train_loss: 0.012589\n",
      "[290/01009] train_loss: 0.012350\n",
      "[290/01059] train_loss: 0.011714\n",
      "[290/01109] train_loss: 0.012231\n",
      "[290/01159] train_loss: 0.012486\n",
      "[290/01209] train_loss: 0.013235\n",
      "[291/00033] train_loss: 0.013168\n",
      "[291/00083] train_loss: 0.012968\n",
      "[291/00133] train_loss: 0.013738\n",
      "[291/00183] train_loss: 0.013393\n",
      "[291/00233] train_loss: 0.013621\n",
      "[291/00283] train_loss: 0.013005\n",
      "[291/00333] train_loss: 0.012787\n",
      "[291/00383] train_loss: 0.012671\n",
      "[291/00433] train_loss: 0.012209\n",
      "[291/00483] train_loss: 0.012357\n",
      "[291/00533] train_loss: 0.013115\n",
      "[291/00583] train_loss: 0.012604\n",
      "[291/00633] train_loss: 0.012878\n",
      "[291/00683] train_loss: 0.013561\n",
      "[291/00733] train_loss: 0.012802\n",
      "[291/00783] train_loss: 0.011876\n",
      "[291/00833] train_loss: 0.011704\n",
      "[291/00883] train_loss: 0.012494\n",
      "[291/00933] train_loss: 0.013169\n",
      "[291/00983] train_loss: 0.013125\n",
      "[291/01033] train_loss: 0.012440\n",
      "[291/01083] train_loss: 0.011898\n",
      "[291/01133] train_loss: 0.012396\n",
      "[291/01183] train_loss: 0.013035\n",
      "[292/00007] train_loss: 0.012182\n",
      "[292/00057] train_loss: 0.013725\n",
      "[292/00107] train_loss: 0.013407\n",
      "[292/00157] train_loss: 0.013518\n",
      "[292/00207] train_loss: 0.012556\n",
      "[292/00257] train_loss: 0.012964\n",
      "[292/00307] train_loss: 0.012785\n",
      "[292/00357] train_loss: 0.013251\n",
      "[292/00407] train_loss: 0.012524\n",
      "[292/00457] train_loss: 0.013597\n",
      "[292/00507] train_loss: 0.012697\n",
      "[292/00557] train_loss: 0.012682\n",
      "[292/00607] train_loss: 0.012176\n",
      "[292/00657] train_loss: 0.012993\n",
      "[292/00707] train_loss: 0.012373\n",
      "[292/00757] train_loss: 0.012584\n",
      "[292/00807] train_loss: 0.012872\n",
      "[292/00857] train_loss: 0.012452\n",
      "[292/00907] train_loss: 0.012631\n",
      "[292/00957] train_loss: 0.013040\n",
      "[292/01007] train_loss: 0.012169\n",
      "[292/01057] train_loss: 0.012326\n",
      "[292/01107] train_loss: 0.012197\n",
      "[292/01157] train_loss: 0.012030\n",
      "[292/01207] train_loss: 0.012967\n",
      "[293/00031] train_loss: 0.013228\n",
      "[293/00081] train_loss: 0.013488\n",
      "[293/00131] train_loss: 0.013518\n",
      "[293/00181] train_loss: 0.013201\n",
      "[293/00231] train_loss: 0.012852\n",
      "[293/00281] train_loss: 0.013074\n",
      "[293/00331] train_loss: 0.012262\n",
      "[293/00381] train_loss: 0.012144\n",
      "[293/00431] train_loss: 0.012580\n",
      "[293/00481] train_loss: 0.012186\n",
      "[293/00531] train_loss: 0.012667\n",
      "[293/00581] train_loss: 0.013081\n",
      "[293/00631] train_loss: 0.012498\n",
      "[293/00681] train_loss: 0.012641\n",
      "[293/00731] train_loss: 0.013016\n",
      "[293/00781] train_loss: 0.013011\n",
      "[293/00831] train_loss: 0.012890\n",
      "[293/00881] train_loss: 0.012261\n",
      "[293/00931] train_loss: 0.013092\n",
      "[293/00981] train_loss: 0.012231\n",
      "[293/01031] train_loss: 0.013248\n",
      "[293/01081] train_loss: 0.012689\n",
      "[293/01131] train_loss: 0.012671\n",
      "[293/01181] train_loss: 0.012217\n",
      "[294/00005] train_loss: 0.012363\n",
      "[294/00055] train_loss: 0.012795\n",
      "[294/00105] train_loss: 0.013136\n",
      "[294/00155] train_loss: 0.013464\n",
      "[294/00205] train_loss: 0.013375\n",
      "[294/00255] train_loss: 0.013189\n",
      "[294/00305] train_loss: 0.013392\n",
      "[294/00355] train_loss: 0.012147\n",
      "[294/00405] train_loss: 0.012443\n",
      "[294/00455] train_loss: 0.012592\n",
      "[294/00505] train_loss: 0.012359\n",
      "[294/00555] train_loss: 0.012947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[294/00605] train_loss: 0.012912\n",
      "[294/00655] train_loss: 0.013134\n",
      "[294/00705] train_loss: 0.012972\n",
      "[294/00755] train_loss: 0.012173\n",
      "[294/00805] train_loss: 0.012521\n",
      "[294/00855] train_loss: 0.012824\n",
      "[294/00905] train_loss: 0.012705\n",
      "[294/00955] train_loss: 0.012778\n",
      "[294/01005] train_loss: 0.012584\n",
      "[294/01055] train_loss: 0.012296\n",
      "[294/01105] train_loss: 0.013113\n",
      "[294/01155] train_loss: 0.012509\n",
      "[294/01205] train_loss: 0.012356\n",
      "[295/00029] train_loss: 0.014063\n",
      "[295/00079] train_loss: 0.013502\n",
      "[295/00129] train_loss: 0.013248\n",
      "[295/00179] train_loss: 0.014051\n",
      "[295/00229] train_loss: 0.013246\n",
      "[295/00279] train_loss: 0.013031\n",
      "[295/00329] train_loss: 0.012934\n",
      "[295/00379] train_loss: 0.012373\n",
      "[295/00429] train_loss: 0.012543\n",
      "[295/00479] train_loss: 0.013212\n",
      "[295/00529] train_loss: 0.012973\n",
      "[295/00579] train_loss: 0.013236\n",
      "[295/00629] train_loss: 0.012879\n",
      "[295/00679] train_loss: 0.012579\n",
      "[295/00729] train_loss: 0.012771\n",
      "[295/00779] train_loss: 0.012734\n",
      "[295/00829] train_loss: 0.012174\n",
      "[295/00879] train_loss: 0.013388\n",
      "[295/00929] train_loss: 0.013159\n",
      "[295/00979] train_loss: 0.011622\n",
      "[295/01029] train_loss: 0.012443\n",
      "[295/01079] train_loss: 0.012813\n",
      "[295/01129] train_loss: 0.012410\n",
      "[295/01179] train_loss: 0.012456\n",
      "[296/00003] train_loss: 0.012302\n",
      "[296/00053] train_loss: 0.013397\n",
      "[296/00103] train_loss: 0.013428\n",
      "[296/00153] train_loss: 0.013273\n",
      "[296/00203] train_loss: 0.012759\n",
      "[296/00253] train_loss: 0.012960\n",
      "[296/00303] train_loss: 0.013656\n",
      "[296/00353] train_loss: 0.012846\n",
      "[296/00403] train_loss: 0.012824\n",
      "[296/00453] train_loss: 0.012602\n",
      "[296/00503] train_loss: 0.012301\n",
      "[296/00553] train_loss: 0.013265\n",
      "[296/00603] train_loss: 0.012680\n",
      "[296/00653] train_loss: 0.013168\n",
      "[296/00703] train_loss: 0.013330\n",
      "[296/00753] train_loss: 0.012787\n",
      "[296/00803] train_loss: 0.012756\n",
      "[296/00853] train_loss: 0.012068\n",
      "[296/00903] train_loss: 0.012005\n",
      "[296/00953] train_loss: 0.012706\n",
      "[296/01003] train_loss: 0.011935\n",
      "[296/01053] train_loss: 0.011913\n",
      "[296/01103] train_loss: 0.013072\n",
      "[296/01153] train_loss: 0.012177\n",
      "[296/01203] train_loss: 0.012616\n",
      "[297/00027] train_loss: 0.012533\n",
      "[297/00077] train_loss: 0.013247\n",
      "[297/00127] train_loss: 0.013929\n",
      "[297/00177] train_loss: 0.012709\n",
      "[297/00227] train_loss: 0.013367\n",
      "[297/00277] train_loss: 0.013323\n",
      "[297/00327] train_loss: 0.013185\n",
      "[297/00377] train_loss: 0.012327\n",
      "[297/00427] train_loss: 0.013247\n",
      "[297/00477] train_loss: 0.012133\n",
      "[297/00527] train_loss: 0.013239\n",
      "[297/00577] train_loss: 0.012983\n",
      "[297/00627] train_loss: 0.012259\n",
      "[297/00677] train_loss: 0.013078\n",
      "[297/00727] train_loss: 0.012384\n",
      "[297/00777] train_loss: 0.012642\n",
      "[297/00827] train_loss: 0.012626\n",
      "[297/00877] train_loss: 0.013099\n",
      "[297/00927] train_loss: 0.013261\n",
      "[297/00977] train_loss: 0.012820\n",
      "[297/01027] train_loss: 0.012171\n",
      "[297/01077] train_loss: 0.013104\n",
      "[297/01127] train_loss: 0.012653\n",
      "[297/01177] train_loss: 0.012440\n",
      "[298/00001] train_loss: 0.012547\n",
      "[298/00051] train_loss: 0.013160\n",
      "[298/00101] train_loss: 0.013148\n",
      "[298/00151] train_loss: 0.012460\n",
      "[298/00201] train_loss: 0.012862\n",
      "[298/00251] train_loss: 0.012927\n",
      "[298/00301] train_loss: 0.013655\n",
      "[298/00351] train_loss: 0.013158\n",
      "[298/00401] train_loss: 0.012807\n",
      "[298/00451] train_loss: 0.012983\n",
      "[298/00501] train_loss: 0.012621\n",
      "[298/00551] train_loss: 0.012617\n",
      "[298/00601] train_loss: 0.012577\n",
      "[298/00651] train_loss: 0.013006\n",
      "[298/00701] train_loss: 0.012107\n",
      "[298/00751] train_loss: 0.013320\n",
      "[298/00801] train_loss: 0.013187\n",
      "[298/00851] train_loss: 0.012400\n",
      "[298/00901] train_loss: 0.012861\n",
      "[298/00951] train_loss: 0.012434\n",
      "[298/01001] train_loss: 0.011948\n",
      "[298/01051] train_loss: 0.012417\n",
      "[298/01101] train_loss: 0.011977\n",
      "[298/01151] train_loss: 0.012709\n",
      "[298/01201] train_loss: 0.012225\n",
      "[299/00025] train_loss: 0.012730\n",
      "[299/00075] train_loss: 0.013409\n",
      "[299/00125] train_loss: 0.013535\n",
      "[299/00175] train_loss: 0.012779\n",
      "[299/00225] train_loss: 0.013304\n",
      "[299/00275] train_loss: 0.012218\n",
      "[299/00325] train_loss: 0.012477\n",
      "[299/00375] train_loss: 0.013526\n",
      "[299/00425] train_loss: 0.013079\n",
      "[299/00475] train_loss: 0.012726\n",
      "[299/00525] train_loss: 0.012287\n",
      "[299/00575] train_loss: 0.013330\n",
      "[299/00625] train_loss: 0.013253\n",
      "[299/00675] train_loss: 0.013023\n",
      "[299/00725] train_loss: 0.013098\n",
      "[299/00775] train_loss: 0.012533\n",
      "[299/00825] train_loss: 0.012783\n",
      "[299/00875] train_loss: 0.012539\n",
      "[299/00925] train_loss: 0.013087\n",
      "[299/00975] train_loss: 0.012614\n",
      "[299/01025] train_loss: 0.012766\n",
      "[299/01075] train_loss: 0.012517\n",
      "[299/01125] train_loss: 0.011969\n",
      "[299/01175] train_loss: 0.012648\n",
      "[299/01225] train_loss: 0.012633\n",
      "[300/00049] train_loss: 0.014478\n",
      "[300/00099] train_loss: 0.013434\n",
      "[300/00149] train_loss: 0.013508\n",
      "[300/00199] train_loss: 0.012460\n",
      "[300/00249] train_loss: 0.012549\n",
      "[300/00299] train_loss: 0.012520\n",
      "[300/00349] train_loss: 0.012944\n",
      "[300/00399] train_loss: 0.013036\n",
      "[300/00449] train_loss: 0.012131\n",
      "[300/00499] train_loss: 0.012594\n",
      "[300/00549] train_loss: 0.013474\n",
      "[300/00599] train_loss: 0.013258\n",
      "[300/00649] train_loss: 0.012180\n",
      "[300/00699] train_loss: 0.012564\n",
      "[300/00749] train_loss: 0.012412\n",
      "[300/00799] train_loss: 0.012967\n",
      "[300/00849] train_loss: 0.012019\n",
      "[300/00899] train_loss: 0.013355\n",
      "[300/00949] train_loss: 0.012071\n",
      "[300/00999] train_loss: 0.012620\n",
      "[300/01049] train_loss: 0.012705\n",
      "[300/01099] train_loss: 0.012856\n",
      "[300/01149] train_loss: 0.012909\n",
      "[300/01199] train_loss: 0.012332\n",
      "[301/00023] train_loss: 0.013140\n",
      "[301/00073] train_loss: 0.013185\n",
      "[301/00123] train_loss: 0.013298\n",
      "[301/00173] train_loss: 0.013646\n",
      "[301/00223] train_loss: 0.013470\n",
      "[301/00273] train_loss: 0.012739\n",
      "[301/00323] train_loss: 0.013009\n",
      "[301/00373] train_loss: 0.013103\n",
      "[301/00423] train_loss: 0.012157\n",
      "[301/00473] train_loss: 0.012077\n",
      "[301/00523] train_loss: 0.012297\n",
      "[301/00573] train_loss: 0.012780\n",
      "[301/00623] train_loss: 0.012573\n",
      "[301/00673] train_loss: 0.013020\n",
      "[301/00723] train_loss: 0.013343\n",
      "[301/00773] train_loss: 0.011937\n",
      "[301/00823] train_loss: 0.012372\n",
      "[301/00873] train_loss: 0.012863\n",
      "[301/00923] train_loss: 0.012397\n",
      "[301/00973] train_loss: 0.012499\n",
      "[301/01023] train_loss: 0.012662\n",
      "[301/01073] train_loss: 0.012441\n",
      "[301/01123] train_loss: 0.012275\n",
      "[301/01173] train_loss: 0.011954\n",
      "[301/01223] train_loss: 0.012234\n",
      "[302/00047] train_loss: 0.013715\n",
      "[302/00097] train_loss: 0.012901\n",
      "[302/00147] train_loss: 0.012410\n",
      "[302/00197] train_loss: 0.013173\n",
      "[302/00247] train_loss: 0.013918\n",
      "[302/00297] train_loss: 0.012384\n",
      "[302/00347] train_loss: 0.011980\n",
      "[302/00397] train_loss: 0.012296\n",
      "[302/00447] train_loss: 0.013105\n",
      "[302/00497] train_loss: 0.012616\n",
      "[302/00547] train_loss: 0.012529\n",
      "[302/00597] train_loss: 0.012655\n",
      "[302/00647] train_loss: 0.011972\n",
      "[302/00697] train_loss: 0.012149\n",
      "[302/00747] train_loss: 0.012418\n",
      "[302/00797] train_loss: 0.012986\n",
      "[302/00847] train_loss: 0.013219\n",
      "[302/00897] train_loss: 0.013048\n",
      "[302/00947] train_loss: 0.013234\n",
      "[302/00997] train_loss: 0.012575\n",
      "[302/01047] train_loss: 0.012965\n",
      "[302/01097] train_loss: 0.012482\n",
      "[302/01147] train_loss: 0.011880\n",
      "[302/01197] train_loss: 0.012878\n",
      "[303/00021] train_loss: 0.013108\n",
      "[303/00071] train_loss: 0.013376\n",
      "[303/00121] train_loss: 0.014199\n",
      "[303/00171] train_loss: 0.012981\n",
      "[303/00221] train_loss: 0.012830\n",
      "[303/00271] train_loss: 0.012610\n",
      "[303/00321] train_loss: 0.012889\n",
      "[303/00371] train_loss: 0.013161\n",
      "[303/00421] train_loss: 0.012661\n",
      "[303/00471] train_loss: 0.012091\n",
      "[303/00521] train_loss: 0.013000\n",
      "[303/00571] train_loss: 0.012604\n",
      "[303/00621] train_loss: 0.012594\n",
      "[303/00671] train_loss: 0.012879\n",
      "[303/00721] train_loss: 0.012901\n",
      "[303/00771] train_loss: 0.012314\n",
      "[303/00821] train_loss: 0.012543\n",
      "[303/00871] train_loss: 0.012935\n",
      "[303/00921] train_loss: 0.012280\n",
      "[303/00971] train_loss: 0.012644\n",
      "[303/01021] train_loss: 0.011903\n",
      "[303/01071] train_loss: 0.012478\n",
      "[303/01121] train_loss: 0.013159\n",
      "[303/01171] train_loss: 0.012427\n",
      "[303/01221] train_loss: 0.012537\n",
      "[304/00045] train_loss: 0.012809\n",
      "[304/00095] train_loss: 0.012561\n",
      "[304/00145] train_loss: 0.012763\n",
      "[304/00195] train_loss: 0.012656\n",
      "[304/00245] train_loss: 0.012680\n",
      "[304/00295] train_loss: 0.012873\n",
      "[304/00345] train_loss: 0.013559\n",
      "[304/00395] train_loss: 0.012740\n",
      "[304/00445] train_loss: 0.012723\n",
      "[304/00495] train_loss: 0.012633\n",
      "[304/00545] train_loss: 0.012841\n",
      "[304/00595] train_loss: 0.013005\n",
      "[304/00645] train_loss: 0.013163\n",
      "[304/00695] train_loss: 0.013413\n",
      "[304/00745] train_loss: 0.011857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[304/00795] train_loss: 0.011722\n",
      "[304/00845] train_loss: 0.012563\n",
      "[304/00895] train_loss: 0.012559\n",
      "[304/00945] train_loss: 0.012444\n",
      "[304/00995] train_loss: 0.012351\n",
      "[304/01045] train_loss: 0.012755\n",
      "[304/01095] train_loss: 0.012267\n",
      "[304/01145] train_loss: 0.012385\n",
      "[304/01195] train_loss: 0.012228\n",
      "[305/00019] train_loss: 0.012506\n",
      "[305/00069] train_loss: 0.013876\n",
      "[305/00119] train_loss: 0.013585\n",
      "[305/00169] train_loss: 0.013598\n",
      "[305/00219] train_loss: 0.013150\n",
      "[305/00269] train_loss: 0.012612\n",
      "[305/00319] train_loss: 0.013003\n",
      "[305/00369] train_loss: 0.013118\n",
      "[305/00419] train_loss: 0.012836\n",
      "[305/00469] train_loss: 0.012497\n",
      "[305/00519] train_loss: 0.012495\n",
      "[305/00569] train_loss: 0.013052\n",
      "[305/00619] train_loss: 0.012175\n",
      "[305/00669] train_loss: 0.012831\n",
      "[305/00719] train_loss: 0.012952\n",
      "[305/00769] train_loss: 0.013030\n",
      "[305/00819] train_loss: 0.012914\n",
      "[305/00869] train_loss: 0.012473\n",
      "[305/00919] train_loss: 0.011976\n",
      "[305/00969] train_loss: 0.012613\n",
      "[305/01019] train_loss: 0.012388\n",
      "[305/01069] train_loss: 0.012556\n",
      "[305/01119] train_loss: 0.012336\n",
      "[305/01169] train_loss: 0.012188\n",
      "[305/01219] train_loss: 0.012402\n",
      "[306/00043] train_loss: 0.013312\n",
      "[306/00093] train_loss: 0.013222\n",
      "[306/00143] train_loss: 0.012774\n",
      "[306/00193] train_loss: 0.012878\n",
      "[306/00243] train_loss: 0.013119\n",
      "[306/00293] train_loss: 0.013051\n",
      "[306/00343] train_loss: 0.013515\n",
      "[306/00393] train_loss: 0.012514\n",
      "[306/00443] train_loss: 0.012719\n",
      "[306/00493] train_loss: 0.012544\n",
      "[306/00543] train_loss: 0.013284\n",
      "[306/00593] train_loss: 0.012725\n",
      "[306/00643] train_loss: 0.012512\n",
      "[306/00693] train_loss: 0.012649\n",
      "[306/00743] train_loss: 0.012340\n",
      "[306/00793] train_loss: 0.012669\n",
      "[306/00843] train_loss: 0.012253\n",
      "[306/00893] train_loss: 0.012499\n",
      "[306/00943] train_loss: 0.012450\n",
      "[306/00993] train_loss: 0.012595\n",
      "[306/01043] train_loss: 0.012169\n",
      "[306/01093] train_loss: 0.012514\n",
      "[306/01143] train_loss: 0.012417\n",
      "[306/01193] train_loss: 0.012478\n",
      "[307/00017] train_loss: 0.012381\n",
      "[307/00067] train_loss: 0.013321\n",
      "[307/00117] train_loss: 0.013800\n",
      "[307/00167] train_loss: 0.013269\n",
      "[307/00217] train_loss: 0.012698\n",
      "[307/00267] train_loss: 0.012588\n",
      "[307/00317] train_loss: 0.012506\n",
      "[307/00367] train_loss: 0.013082\n",
      "[307/00417] train_loss: 0.012024\n",
      "[307/00467] train_loss: 0.013132\n",
      "[307/00517] train_loss: 0.012982\n",
      "[307/00567] train_loss: 0.012574\n",
      "[307/00617] train_loss: 0.012950\n",
      "[307/00667] train_loss: 0.011714\n",
      "[307/00717] train_loss: 0.012399\n",
      "[307/00767] train_loss: 0.012473\n",
      "[307/00817] train_loss: 0.012249\n",
      "[307/00867] train_loss: 0.012488\n",
      "[307/00917] train_loss: 0.013113\n",
      "[307/00967] train_loss: 0.012067\n",
      "[307/01017] train_loss: 0.013261\n",
      "[307/01067] train_loss: 0.011930\n",
      "[307/01117] train_loss: 0.012823\n",
      "[307/01167] train_loss: 0.013736\n",
      "[307/01217] train_loss: 0.012413\n",
      "[308/00041] train_loss: 0.013366\n",
      "[308/00091] train_loss: 0.013048\n",
      "[308/00141] train_loss: 0.013526\n",
      "[308/00191] train_loss: 0.013164\n",
      "[308/00241] train_loss: 0.013037\n",
      "[308/00291] train_loss: 0.012471\n",
      "[308/00341] train_loss: 0.012704\n",
      "[308/00391] train_loss: 0.012708\n",
      "[308/00441] train_loss: 0.012202\n",
      "[308/00491] train_loss: 0.012757\n",
      "[308/00541] train_loss: 0.011775\n",
      "[308/00591] train_loss: 0.012760\n",
      "[308/00641] train_loss: 0.012995\n",
      "[308/00691] train_loss: 0.012122\n",
      "[308/00741] train_loss: 0.013278\n",
      "[308/00791] train_loss: 0.012539\n",
      "[308/00841] train_loss: 0.012251\n",
      "[308/00891] train_loss: 0.012950\n",
      "[308/00941] train_loss: 0.012339\n",
      "[308/00991] train_loss: 0.012177\n",
      "[308/01041] train_loss: 0.012614\n",
      "[308/01091] train_loss: 0.012338\n",
      "[308/01141] train_loss: 0.012891\n",
      "[308/01191] train_loss: 0.012592\n",
      "[309/00015] train_loss: 0.012815\n",
      "[309/00065] train_loss: 0.013713\n",
      "[309/00115] train_loss: 0.013559\n",
      "[309/00165] train_loss: 0.012917\n",
      "[309/00215] train_loss: 0.013083\n",
      "[309/00265] train_loss: 0.012598\n",
      "[309/00315] train_loss: 0.012877\n",
      "[309/00365] train_loss: 0.013060\n",
      "[309/00415] train_loss: 0.012375\n",
      "[309/00465] train_loss: 0.012728\n",
      "[309/00515] train_loss: 0.012929\n",
      "[309/00565] train_loss: 0.012384\n",
      "[309/00615] train_loss: 0.012917\n",
      "[309/00665] train_loss: 0.012854\n",
      "[309/00715] train_loss: 0.012134\n",
      "[309/00765] train_loss: 0.012845\n",
      "[309/00815] train_loss: 0.012509\n",
      "[309/00865] train_loss: 0.012576\n",
      "[309/00915] train_loss: 0.012594\n",
      "[309/00965] train_loss: 0.012565\n",
      "[309/01015] train_loss: 0.012580\n",
      "[309/01065] train_loss: 0.012843\n",
      "[309/01115] train_loss: 0.012686\n",
      "[309/01165] train_loss: 0.012071\n",
      "[309/01215] train_loss: 0.012801\n",
      "[310/00039] train_loss: 0.013068\n",
      "[310/00089] train_loss: 0.013676\n",
      "[310/00139] train_loss: 0.013592\n",
      "[310/00189] train_loss: 0.013010\n",
      "[310/00239] train_loss: 0.012569\n",
      "[310/00289] train_loss: 0.012972\n",
      "[310/00339] train_loss: 0.012171\n",
      "[310/00389] train_loss: 0.012719\n",
      "[310/00439] train_loss: 0.012696\n",
      "[310/00489] train_loss: 0.013386\n",
      "[310/00539] train_loss: 0.012245\n",
      "[310/00589] train_loss: 0.012542\n",
      "[310/00639] train_loss: 0.011896\n",
      "[310/00689] train_loss: 0.012478\n",
      "[310/00739] train_loss: 0.013010\n",
      "[310/00789] train_loss: 0.013472\n",
      "[310/00839] train_loss: 0.012501\n",
      "[310/00889] train_loss: 0.012361\n",
      "[310/00939] train_loss: 0.012517\n",
      "[310/00989] train_loss: 0.012623\n",
      "[310/01039] train_loss: 0.012387\n",
      "[310/01089] train_loss: 0.012683\n",
      "[310/01139] train_loss: 0.012779\n",
      "[310/01189] train_loss: 0.013180\n",
      "[311/00013] train_loss: 0.012484\n",
      "[311/00063] train_loss: 0.012840\n",
      "[311/00113] train_loss: 0.013524\n",
      "[311/00163] train_loss: 0.013073\n",
      "[311/00213] train_loss: 0.012853\n",
      "[311/00263] train_loss: 0.012804\n",
      "[311/00313] train_loss: 0.012264\n",
      "[311/00363] train_loss: 0.012798\n",
      "[311/00413] train_loss: 0.012826\n",
      "[311/00463] train_loss: 0.012188\n",
      "[311/00513] train_loss: 0.012508\n",
      "[311/00563] train_loss: 0.012955\n",
      "[311/00613] train_loss: 0.012465\n",
      "[311/00663] train_loss: 0.012345\n",
      "[311/00713] train_loss: 0.013010\n",
      "[311/00763] train_loss: 0.012133\n",
      "[311/00813] train_loss: 0.013501\n",
      "[311/00863] train_loss: 0.012516\n",
      "[311/00913] train_loss: 0.012188\n",
      "[311/00963] train_loss: 0.013607\n",
      "[311/01013] train_loss: 0.012666\n",
      "[311/01063] train_loss: 0.012782\n",
      "[311/01113] train_loss: 0.012312\n",
      "[311/01163] train_loss: 0.012110\n",
      "[311/01213] train_loss: 0.012878\n",
      "[312/00037] train_loss: 0.013372\n",
      "[312/00087] train_loss: 0.013440\n",
      "[312/00137] train_loss: 0.012960\n",
      "[312/00187] train_loss: 0.012196\n",
      "[312/00237] train_loss: 0.012662\n",
      "[312/00287] train_loss: 0.012924\n",
      "[312/00337] train_loss: 0.012736\n",
      "[312/00387] train_loss: 0.013002\n",
      "[312/00437] train_loss: 0.012799\n",
      "[312/00487] train_loss: 0.012508\n",
      "[312/00537] train_loss: 0.012165\n",
      "[312/00587] train_loss: 0.012440\n",
      "[312/00637] train_loss: 0.012641\n",
      "[312/00687] train_loss: 0.012321\n",
      "[312/00737] train_loss: 0.011888\n",
      "[312/00787] train_loss: 0.012741\n",
      "[312/00837] train_loss: 0.012222\n",
      "[312/00887] train_loss: 0.013026\n",
      "[312/00937] train_loss: 0.012219\n",
      "[312/00987] train_loss: 0.011978\n",
      "[312/01037] train_loss: 0.012530\n",
      "[312/01087] train_loss: 0.012407\n",
      "[312/01137] train_loss: 0.012814\n",
      "[312/01187] train_loss: 0.012826\n",
      "[313/00011] train_loss: 0.012802\n",
      "[313/00061] train_loss: 0.013330\n",
      "[313/00111] train_loss: 0.013602\n",
      "[313/00161] train_loss: 0.012745\n",
      "[313/00211] train_loss: 0.013119\n",
      "[313/00261] train_loss: 0.012613\n",
      "[313/00311] train_loss: 0.012981\n",
      "[313/00361] train_loss: 0.012767\n",
      "[313/00411] train_loss: 0.012585\n",
      "[313/00461] train_loss: 0.013417\n",
      "[313/00511] train_loss: 0.012885\n",
      "[313/00561] train_loss: 0.012458\n",
      "[313/00611] train_loss: 0.012510\n",
      "[313/00661] train_loss: 0.012750\n",
      "[313/00711] train_loss: 0.012692\n",
      "[313/00761] train_loss: 0.012752\n",
      "[313/00811] train_loss: 0.012156\n",
      "[313/00861] train_loss: 0.012597\n",
      "[313/00911] train_loss: 0.012369\n",
      "[313/00961] train_loss: 0.012156\n",
      "[313/01011] train_loss: 0.012541\n",
      "[313/01061] train_loss: 0.012343\n",
      "[313/01111] train_loss: 0.012612\n",
      "[313/01161] train_loss: 0.012895\n",
      "[313/01211] train_loss: 0.013338\n",
      "[314/00035] train_loss: 0.012511\n",
      "[314/00085] train_loss: 0.013720\n",
      "[314/00135] train_loss: 0.012945\n",
      "[314/00185] train_loss: 0.013023\n",
      "[314/00235] train_loss: 0.012557\n",
      "[314/00285] train_loss: 0.013367\n",
      "[314/00335] train_loss: 0.013208\n",
      "[314/00385] train_loss: 0.012876\n",
      "[314/00435] train_loss: 0.012502\n",
      "[314/00485] train_loss: 0.012851\n",
      "[314/00535] train_loss: 0.011841\n",
      "[314/00585] train_loss: 0.011898\n",
      "[314/00635] train_loss: 0.012551\n",
      "[314/00685] train_loss: 0.013366\n",
      "[314/00735] train_loss: 0.012566\n",
      "[314/00785] train_loss: 0.012649\n",
      "[314/00835] train_loss: 0.012779\n",
      "[314/00885] train_loss: 0.012859\n",
      "[314/00935] train_loss: 0.013454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[314/00985] train_loss: 0.012148\n",
      "[314/01035] train_loss: 0.012640\n",
      "[314/01085] train_loss: 0.012013\n",
      "[314/01135] train_loss: 0.012403\n",
      "[314/01185] train_loss: 0.012484\n",
      "[315/00009] train_loss: 0.012699\n",
      "[315/00059] train_loss: 0.013365\n",
      "[315/00109] train_loss: 0.013265\n",
      "[315/00159] train_loss: 0.013001\n",
      "[315/00209] train_loss: 0.013606\n",
      "[315/00259] train_loss: 0.013191\n",
      "[315/00309] train_loss: 0.012855\n",
      "[315/00359] train_loss: 0.012342\n",
      "[315/00409] train_loss: 0.012999\n",
      "[315/00459] train_loss: 0.013367\n",
      "[315/00509] train_loss: 0.012056\n",
      "[315/00559] train_loss: 0.012741\n",
      "[315/00609] train_loss: 0.012492\n",
      "[315/00659] train_loss: 0.012520\n",
      "[315/00709] train_loss: 0.011835\n",
      "[315/00759] train_loss: 0.013350\n",
      "[315/00809] train_loss: 0.012690\n",
      "[315/00859] train_loss: 0.011977\n",
      "[315/00909] train_loss: 0.011961\n",
      "[315/00959] train_loss: 0.012480\n",
      "[315/01009] train_loss: 0.012595\n",
      "[315/01059] train_loss: 0.011951\n",
      "[315/01109] train_loss: 0.012980\n",
      "[315/01159] train_loss: 0.012758\n",
      "[315/01209] train_loss: 0.011795\n",
      "[316/00033] train_loss: 0.012570\n",
      "[316/00083] train_loss: 0.013962\n",
      "[316/00133] train_loss: 0.013233\n",
      "[316/00183] train_loss: 0.012656\n",
      "[316/00233] train_loss: 0.012511\n",
      "[316/00283] train_loss: 0.013216\n",
      "[316/00333] train_loss: 0.012939\n",
      "[316/00383] train_loss: 0.012308\n",
      "[316/00433] train_loss: 0.012767\n",
      "[316/00483] train_loss: 0.013000\n",
      "[316/00533] train_loss: 0.012525\n",
      "[316/00583] train_loss: 0.012477\n",
      "[316/00633] train_loss: 0.012828\n",
      "[316/00683] train_loss: 0.012086\n",
      "[316/00733] train_loss: 0.012113\n",
      "[316/00783] train_loss: 0.012728\n",
      "[316/00833] train_loss: 0.013547\n",
      "[316/00883] train_loss: 0.011810\n",
      "[316/00933] train_loss: 0.012204\n",
      "[316/00983] train_loss: 0.013116\n",
      "[316/01033] train_loss: 0.012385\n",
      "[316/01083] train_loss: 0.012555\n",
      "[316/01133] train_loss: 0.013564\n",
      "[316/01183] train_loss: 0.012510\n",
      "[317/00007] train_loss: 0.013121\n",
      "[317/00057] train_loss: 0.012813\n",
      "[317/00107] train_loss: 0.012893\n",
      "[317/00157] train_loss: 0.012925\n",
      "[317/00207] train_loss: 0.013192\n",
      "[317/00257] train_loss: 0.012951\n",
      "[317/00307] train_loss: 0.012728\n",
      "[317/00357] train_loss: 0.012472\n",
      "[317/00407] train_loss: 0.013032\n",
      "[317/00457] train_loss: 0.012653\n",
      "[317/00507] train_loss: 0.012937\n",
      "[317/00557] train_loss: 0.012910\n",
      "[317/00607] train_loss: 0.012536\n",
      "[317/00657] train_loss: 0.013108\n",
      "[317/00707] train_loss: 0.012338\n",
      "[317/00757] train_loss: 0.013061\n",
      "[317/00807] train_loss: 0.012726\n",
      "[317/00857] train_loss: 0.011764\n",
      "[317/00907] train_loss: 0.013155\n",
      "[317/00957] train_loss: 0.012050\n",
      "[317/01007] train_loss: 0.012185\n",
      "[317/01057] train_loss: 0.011861\n",
      "[317/01107] train_loss: 0.012289\n",
      "[317/01157] train_loss: 0.012002\n",
      "[317/01207] train_loss: 0.012273\n",
      "[318/00031] train_loss: 0.013116\n",
      "[318/00081] train_loss: 0.013390\n",
      "[318/00131] train_loss: 0.013243\n",
      "[318/00181] train_loss: 0.013736\n",
      "[318/00231] train_loss: 0.013026\n",
      "[318/00281] train_loss: 0.013174\n",
      "[318/00331] train_loss: 0.012104\n",
      "[318/00381] train_loss: 0.012481\n",
      "[318/00431] train_loss: 0.012502\n",
      "[318/00481] train_loss: 0.013044\n",
      "[318/00531] train_loss: 0.012775\n",
      "[318/00581] train_loss: 0.012938\n",
      "[318/00631] train_loss: 0.012826\n",
      "[318/00681] train_loss: 0.013031\n",
      "[318/00731] train_loss: 0.012111\n",
      "[318/00781] train_loss: 0.013230\n",
      "[318/00831] train_loss: 0.012711\n",
      "[318/00881] train_loss: 0.012398\n",
      "[318/00931] train_loss: 0.012626\n",
      "[318/00981] train_loss: 0.011728\n",
      "[318/01031] train_loss: 0.013733\n",
      "[318/01081] train_loss: 0.012866\n",
      "[318/01131] train_loss: 0.012324\n",
      "[318/01181] train_loss: 0.012140\n",
      "[319/00005] train_loss: 0.012231\n",
      "[319/00055] train_loss: 0.013059\n",
      "[319/00105] train_loss: 0.012924\n",
      "[319/00155] train_loss: 0.013042\n",
      "[319/00205] train_loss: 0.012927\n",
      "[319/00255] train_loss: 0.012696\n",
      "[319/00305] train_loss: 0.012687\n",
      "[319/00355] train_loss: 0.012464\n",
      "[319/00405] train_loss: 0.012945\n",
      "[319/00455] train_loss: 0.012127\n",
      "[319/00505] train_loss: 0.013231\n",
      "[319/00555] train_loss: 0.012948\n",
      "[319/00605] train_loss: 0.013120\n",
      "[319/00655] train_loss: 0.012778\n",
      "[319/00705] train_loss: 0.012193\n",
      "[319/00755] train_loss: 0.012274\n",
      "[319/00805] train_loss: 0.012124\n",
      "[319/00855] train_loss: 0.012458\n",
      "[319/00905] train_loss: 0.011663\n",
      "[319/00955] train_loss: 0.012429\n",
      "[319/01005] train_loss: 0.012810\n",
      "[319/01055] train_loss: 0.012690\n",
      "[319/01105] train_loss: 0.013448\n",
      "[319/01155] train_loss: 0.012189\n",
      "[319/01205] train_loss: 0.012203\n",
      "[320/00029] train_loss: 0.013376\n",
      "[320/00079] train_loss: 0.013487\n",
      "[320/00129] train_loss: 0.013037\n",
      "[320/00179] train_loss: 0.013169\n",
      "[320/00229] train_loss: 0.012807\n",
      "[320/00279] train_loss: 0.012761\n",
      "[320/00329] train_loss: 0.012631\n",
      "[320/00379] train_loss: 0.012133\n",
      "[320/00429] train_loss: 0.013072\n",
      "[320/00479] train_loss: 0.012002\n",
      "[320/00529] train_loss: 0.012231\n",
      "[320/00579] train_loss: 0.012430\n",
      "[320/00629] train_loss: 0.012294\n",
      "[320/00679] train_loss: 0.013293\n",
      "[320/00729] train_loss: 0.012478\n",
      "[320/00779] train_loss: 0.012675\n",
      "[320/00829] train_loss: 0.013390\n",
      "[320/00879] train_loss: 0.012683\n",
      "[320/00929] train_loss: 0.012837\n",
      "[320/00979] train_loss: 0.012441\n",
      "[320/01029] train_loss: 0.011908\n",
      "[320/01079] train_loss: 0.012696\n",
      "[320/01129] train_loss: 0.012442\n",
      "[320/01179] train_loss: 0.012265\n",
      "[321/00003] train_loss: 0.012488\n",
      "[321/00053] train_loss: 0.012776\n",
      "[321/00103] train_loss: 0.013384\n",
      "[321/00153] train_loss: 0.012834\n",
      "[321/00203] train_loss: 0.012401\n",
      "[321/00253] train_loss: 0.013167\n",
      "[321/00303] train_loss: 0.013400\n",
      "[321/00353] train_loss: 0.012752\n",
      "[321/00403] train_loss: 0.013199\n",
      "[321/00453] train_loss: 0.012808\n",
      "[321/00503] train_loss: 0.012413\n",
      "[321/00553] train_loss: 0.011768\n",
      "[321/00603] train_loss: 0.012090\n",
      "[321/00653] train_loss: 0.013047\n",
      "[321/00703] train_loss: 0.012262\n",
      "[321/00753] train_loss: 0.012861\n",
      "[321/00803] train_loss: 0.012463\n",
      "[321/00853] train_loss: 0.012697\n",
      "[321/00903] train_loss: 0.012177\n",
      "[321/00953] train_loss: 0.012703\n",
      "[321/01003] train_loss: 0.012399\n",
      "[321/01053] train_loss: 0.012286\n",
      "[321/01103] train_loss: 0.012546\n",
      "[321/01153] train_loss: 0.012456\n",
      "[321/01203] train_loss: 0.012191\n",
      "[322/00027] train_loss: 0.012617\n",
      "[322/00077] train_loss: 0.012948\n",
      "[322/00127] train_loss: 0.013137\n",
      "[322/00177] train_loss: 0.012425\n",
      "[322/00227] train_loss: 0.012838\n",
      "[322/00277] train_loss: 0.013036\n",
      "[322/00327] train_loss: 0.012496\n",
      "[322/00377] train_loss: 0.013145\n",
      "[322/00427] train_loss: 0.013417\n",
      "[322/00477] train_loss: 0.014039\n",
      "[322/00527] train_loss: 0.012485\n",
      "[322/00577] train_loss: 0.012393\n",
      "[322/00627] train_loss: 0.012606\n",
      "[322/00677] train_loss: 0.012146\n",
      "[322/00727] train_loss: 0.011654\n",
      "[322/00777] train_loss: 0.012502\n",
      "[322/00827] train_loss: 0.012819\n",
      "[322/00877] train_loss: 0.011674\n",
      "[322/00927] train_loss: 0.012067\n",
      "[322/00977] train_loss: 0.012674\n",
      "[322/01027] train_loss: 0.012750\n",
      "[322/01077] train_loss: 0.012134\n",
      "[322/01127] train_loss: 0.012657\n",
      "[322/01177] train_loss: 0.012465\n",
      "[323/00001] train_loss: 0.012579\n",
      "[323/00051] train_loss: 0.013458\n",
      "[323/00101] train_loss: 0.013568\n",
      "[323/00151] train_loss: 0.012232\n",
      "[323/00201] train_loss: 0.012269\n",
      "[323/00251] train_loss: 0.013022\n",
      "[323/00301] train_loss: 0.012538\n",
      "[323/00351] train_loss: 0.011802\n",
      "[323/00401] train_loss: 0.012273\n",
      "[323/00451] train_loss: 0.012726\n",
      "[323/00501] train_loss: 0.012407\n",
      "[323/00551] train_loss: 0.012262\n",
      "[323/00601] train_loss: 0.012696\n",
      "[323/00651] train_loss: 0.012669\n",
      "[323/00701] train_loss: 0.013118\n",
      "[323/00751] train_loss: 0.012193\n",
      "[323/00801] train_loss: 0.012086\n",
      "[323/00851] train_loss: 0.012497\n",
      "[323/00901] train_loss: 0.012762\n",
      "[323/00951] train_loss: 0.012869\n",
      "[323/01001] train_loss: 0.012739\n",
      "[323/01051] train_loss: 0.012649\n",
      "[323/01101] train_loss: 0.011898\n",
      "[323/01151] train_loss: 0.012773\n",
      "[323/01201] train_loss: 0.012668\n",
      "[324/00025] train_loss: 0.012973\n",
      "[324/00075] train_loss: 0.012786\n",
      "[324/00125] train_loss: 0.012509\n",
      "[324/00175] train_loss: 0.012501\n",
      "[324/00225] train_loss: 0.012619\n",
      "[324/00275] train_loss: 0.012715\n",
      "[324/00325] train_loss: 0.012424\n",
      "[324/00375] train_loss: 0.012793\n",
      "[324/00425] train_loss: 0.012554\n",
      "[324/00475] train_loss: 0.012548\n",
      "[324/00525] train_loss: 0.013504\n",
      "[324/00575] train_loss: 0.012096\n",
      "[324/00625] train_loss: 0.012685\n",
      "[324/00675] train_loss: 0.013507\n",
      "[324/00725] train_loss: 0.012291\n",
      "[324/00775] train_loss: 0.012822\n",
      "[324/00825] train_loss: 0.013324\n",
      "[324/00875] train_loss: 0.012193\n",
      "[324/00925] train_loss: 0.012127\n",
      "[324/00975] train_loss: 0.012470\n",
      "[324/01025] train_loss: 0.012346\n",
      "[324/01075] train_loss: 0.013321\n",
      "[324/01125] train_loss: 0.011875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[324/01175] train_loss: 0.012558\n",
      "[324/01225] train_loss: 0.011564\n",
      "[325/00049] train_loss: 0.013270\n",
      "[325/00099] train_loss: 0.012682\n",
      "[325/00149] train_loss: 0.012464\n",
      "[325/00199] train_loss: 0.013762\n",
      "[325/00249] train_loss: 0.012901\n",
      "[325/00299] train_loss: 0.013090\n",
      "[325/00349] train_loss: 0.013425\n",
      "[325/00399] train_loss: 0.012885\n",
      "[325/00449] train_loss: 0.012297\n",
      "[325/00499] train_loss: 0.012962\n",
      "[325/00549] train_loss: 0.012392\n",
      "[325/00599] train_loss: 0.012780\n",
      "[325/00649] train_loss: 0.012282\n",
      "[325/00699] train_loss: 0.012399\n",
      "[325/00749] train_loss: 0.012898\n",
      "[325/00799] train_loss: 0.012139\n",
      "[325/00849] train_loss: 0.012030\n",
      "[325/00899] train_loss: 0.012313\n",
      "[325/00949] train_loss: 0.013335\n",
      "[325/00999] train_loss: 0.012006\n",
      "[325/01049] train_loss: 0.012782\n",
      "[325/01099] train_loss: 0.012383\n",
      "[325/01149] train_loss: 0.012430\n",
      "[325/01199] train_loss: 0.012468\n",
      "[326/00023] train_loss: 0.012783\n",
      "[326/00073] train_loss: 0.012744\n",
      "[326/00123] train_loss: 0.013101\n",
      "[326/00173] train_loss: 0.012679\n",
      "[326/00223] train_loss: 0.013545\n",
      "[326/00273] train_loss: 0.013373\n",
      "[326/00323] train_loss: 0.012041\n",
      "[326/00373] train_loss: 0.012853\n",
      "[326/00423] train_loss: 0.012559\n",
      "[326/00473] train_loss: 0.012557\n",
      "[326/00523] train_loss: 0.012840\n",
      "[326/00573] train_loss: 0.012661\n",
      "[326/00623] train_loss: 0.012645\n",
      "[326/00673] train_loss: 0.012941\n",
      "[326/00723] train_loss: 0.012519\n",
      "[326/00773] train_loss: 0.012341\n",
      "[326/00823] train_loss: 0.012516\n",
      "[326/00873] train_loss: 0.012843\n",
      "[326/00923] train_loss: 0.011842\n",
      "[326/00973] train_loss: 0.013000\n",
      "[326/01023] train_loss: 0.012837\n",
      "[326/01073] train_loss: 0.012242\n",
      "[326/01123] train_loss: 0.012790\n",
      "[326/01173] train_loss: 0.012353\n",
      "[326/01223] train_loss: 0.012896\n",
      "[327/00047] train_loss: 0.013566\n",
      "[327/00097] train_loss: 0.013380\n",
      "[327/00147] train_loss: 0.012955\n",
      "[327/00197] train_loss: 0.012931\n",
      "[327/00247] train_loss: 0.012874\n",
      "[327/00297] train_loss: 0.012364\n",
      "[327/00347] train_loss: 0.012979\n",
      "[327/00397] train_loss: 0.012538\n",
      "[327/00447] train_loss: 0.012788\n",
      "[327/00497] train_loss: 0.012866\n",
      "[327/00547] train_loss: 0.013048\n",
      "[327/00597] train_loss: 0.013182\n",
      "[327/00647] train_loss: 0.012257\n",
      "[327/00697] train_loss: 0.012364\n",
      "[327/00747] train_loss: 0.012929\n",
      "[327/00797] train_loss: 0.012191\n",
      "[327/00847] train_loss: 0.012967\n",
      "[327/00897] train_loss: 0.012275\n",
      "[327/00947] train_loss: 0.011948\n",
      "[327/00997] train_loss: 0.013221\n",
      "[327/01047] train_loss: 0.012766\n",
      "[327/01097] train_loss: 0.012022\n",
      "[327/01147] train_loss: 0.012375\n",
      "[327/01197] train_loss: 0.013148\n",
      "[328/00021] train_loss: 0.012829\n",
      "[328/00071] train_loss: 0.012968\n",
      "[328/00121] train_loss: 0.013109\n",
      "[328/00171] train_loss: 0.012637\n",
      "[328/00221] train_loss: 0.012963\n",
      "[328/00271] train_loss: 0.012946\n",
      "[328/00321] train_loss: 0.012067\n",
      "[328/00371] train_loss: 0.012625\n",
      "[328/00421] train_loss: 0.012812\n",
      "[328/00471] train_loss: 0.012648\n",
      "[328/00521] train_loss: 0.012690\n",
      "[328/00571] train_loss: 0.012532\n",
      "[328/00621] train_loss: 0.012995\n",
      "[328/00671] train_loss: 0.012652\n",
      "[328/00721] train_loss: 0.012623\n",
      "[328/00771] train_loss: 0.012907\n",
      "[328/00821] train_loss: 0.012341\n",
      "[328/00871] train_loss: 0.012254\n",
      "[328/00921] train_loss: 0.012499\n",
      "[328/00971] train_loss: 0.012209\n",
      "[328/01021] train_loss: 0.012107\n",
      "[328/01071] train_loss: 0.012074\n",
      "[328/01121] train_loss: 0.012897\n",
      "[328/01171] train_loss: 0.012306\n",
      "[328/01221] train_loss: 0.011887\n",
      "[329/00045] train_loss: 0.012656\n",
      "[329/00095] train_loss: 0.013241\n",
      "[329/00145] train_loss: 0.012587\n",
      "[329/00195] train_loss: 0.012651\n",
      "[329/00245] train_loss: 0.012889\n",
      "[329/00295] train_loss: 0.012855\n",
      "[329/00345] train_loss: 0.013438\n",
      "[329/00395] train_loss: 0.011977\n",
      "[329/00445] train_loss: 0.012537\n",
      "[329/00495] train_loss: 0.012536\n",
      "[329/00545] train_loss: 0.012847\n",
      "[329/00595] train_loss: 0.012813\n",
      "[329/00645] train_loss: 0.012480\n",
      "[329/00695] train_loss: 0.013596\n",
      "[329/00745] train_loss: 0.012757\n",
      "[329/00795] train_loss: 0.013999\n",
      "[329/00845] train_loss: 0.012627\n",
      "[329/00895] train_loss: 0.012377\n",
      "[329/00945] train_loss: 0.012265\n",
      "[329/00995] train_loss: 0.011995\n",
      "[329/01045] train_loss: 0.012735\n",
      "[329/01095] train_loss: 0.012652\n",
      "[329/01145] train_loss: 0.013479\n",
      "[329/01195] train_loss: 0.012534\n",
      "[330/00019] train_loss: 0.012397\n",
      "[330/00069] train_loss: 0.012836\n",
      "[330/00119] train_loss: 0.012947\n",
      "[330/00169] train_loss: 0.012505\n",
      "[330/00219] train_loss: 0.013586\n",
      "[330/00269] train_loss: 0.012800\n",
      "[330/00319] train_loss: 0.012528\n",
      "[330/00369] train_loss: 0.012607\n",
      "[330/00419] train_loss: 0.011986\n",
      "[330/00469] train_loss: 0.012953\n",
      "[330/00519] train_loss: 0.012864\n",
      "[330/00569] train_loss: 0.013119\n",
      "[330/00619] train_loss: 0.012982\n",
      "[330/00669] train_loss: 0.012664\n",
      "[330/00719] train_loss: 0.012245\n",
      "[330/00769] train_loss: 0.012294\n",
      "[330/00819] train_loss: 0.012447\n",
      "[330/00869] train_loss: 0.012654\n",
      "[330/00919] train_loss: 0.012288\n",
      "[330/00969] train_loss: 0.012113\n",
      "[330/01019] train_loss: 0.012128\n",
      "[330/01069] train_loss: 0.012609\n",
      "[330/01119] train_loss: 0.012883\n",
      "[330/01169] train_loss: 0.012128\n",
      "[330/01219] train_loss: 0.012929\n",
      "[331/00043] train_loss: 0.013304\n",
      "[331/00093] train_loss: 0.012910\n",
      "[331/00143] train_loss: 0.012666\n",
      "[331/00193] train_loss: 0.012621\n",
      "[331/00243] train_loss: 0.013368\n",
      "[331/00293] train_loss: 0.012391\n",
      "[331/00343] train_loss: 0.012566\n",
      "[331/00393] train_loss: 0.012046\n",
      "[331/00443] train_loss: 0.013027\n",
      "[331/00493] train_loss: 0.012722\n",
      "[331/00543] train_loss: 0.011869\n",
      "[331/00593] train_loss: 0.012183\n",
      "[331/00643] train_loss: 0.012546\n",
      "[331/00693] train_loss: 0.013008\n",
      "[331/00743] train_loss: 0.012173\n",
      "[331/00793] train_loss: 0.013097\n",
      "[331/00843] train_loss: 0.013099\n",
      "[331/00893] train_loss: 0.013087\n",
      "[331/00943] train_loss: 0.012652\n",
      "[331/00993] train_loss: 0.011947\n",
      "[331/01043] train_loss: 0.011737\n",
      "[331/01093] train_loss: 0.013121\n",
      "[331/01143] train_loss: 0.012291\n",
      "[331/01193] train_loss: 0.012918\n",
      "[332/00017] train_loss: 0.013381\n",
      "[332/00067] train_loss: 0.012657\n",
      "[332/00117] train_loss: 0.012459\n",
      "[332/00167] train_loss: 0.013290\n",
      "[332/00217] train_loss: 0.012745\n",
      "[332/00267] train_loss: 0.012866\n",
      "[332/00317] train_loss: 0.012891\n",
      "[332/00367] train_loss: 0.012903\n",
      "[332/00417] train_loss: 0.012645\n",
      "[332/00467] train_loss: 0.012386\n",
      "[332/00517] train_loss: 0.013076\n",
      "[332/00567] train_loss: 0.012587\n",
      "[332/00617] train_loss: 0.013084\n",
      "[332/00667] train_loss: 0.012103\n",
      "[332/00717] train_loss: 0.012303\n",
      "[332/00767] train_loss: 0.012764\n",
      "[332/00817] train_loss: 0.012345\n",
      "[332/00867] train_loss: 0.012366\n",
      "[332/00917] train_loss: 0.012702\n",
      "[332/00967] train_loss: 0.012298\n",
      "[332/01017] train_loss: 0.012683\n",
      "[332/01067] train_loss: 0.012888\n",
      "[332/01117] train_loss: 0.012454\n",
      "[332/01167] train_loss: 0.012604\n",
      "[332/01217] train_loss: 0.011721\n",
      "[333/00041] train_loss: 0.013315\n",
      "[333/00091] train_loss: 0.013444\n",
      "[333/00141] train_loss: 0.013155\n",
      "[333/00191] train_loss: 0.012985\n",
      "[333/00241] train_loss: 0.012791\n",
      "[333/00291] train_loss: 0.012251\n",
      "[333/00341] train_loss: 0.012739\n",
      "[333/00391] train_loss: 0.012199\n",
      "[333/00441] train_loss: 0.012333\n",
      "[333/00491] train_loss: 0.012747\n",
      "[333/00541] train_loss: 0.012348\n",
      "[333/00591] train_loss: 0.012507\n",
      "[333/00641] train_loss: 0.012894\n",
      "[333/00691] train_loss: 0.012445\n",
      "[333/00741] train_loss: 0.012440\n",
      "[333/00791] train_loss: 0.011862\n",
      "[333/00841] train_loss: 0.012637\n",
      "[333/00891] train_loss: 0.012528\n",
      "[333/00941] train_loss: 0.012551\n",
      "[333/00991] train_loss: 0.012192\n",
      "[333/01041] train_loss: 0.012355\n",
      "[333/01091] train_loss: 0.012124\n",
      "[333/01141] train_loss: 0.012382\n",
      "[333/01191] train_loss: 0.012228\n",
      "[334/00015] train_loss: 0.013384\n",
      "[334/00065] train_loss: 0.013947\n",
      "[334/00115] train_loss: 0.013915\n",
      "[334/00165] train_loss: 0.013055\n",
      "[334/00215] train_loss: 0.013100\n",
      "[334/00265] train_loss: 0.012518\n",
      "[334/00315] train_loss: 0.012383\n",
      "[334/00365] train_loss: 0.013346\n",
      "[334/00415] train_loss: 0.012682\n",
      "[334/00465] train_loss: 0.012407\n",
      "[334/00515] train_loss: 0.012458\n",
      "[334/00565] train_loss: 0.013230\n",
      "[334/00615] train_loss: 0.012571\n",
      "[334/00665] train_loss: 0.013439\n",
      "[334/00715] train_loss: 0.011933\n",
      "[334/00765] train_loss: 0.012122\n",
      "[334/00815] train_loss: 0.012140\n",
      "[334/00865] train_loss: 0.013138\n",
      "[334/00915] train_loss: 0.012380\n",
      "[334/00965] train_loss: 0.012552\n",
      "[334/01015] train_loss: 0.012630\n",
      "[334/01065] train_loss: 0.012570\n",
      "[334/01115] train_loss: 0.012245\n",
      "[334/01165] train_loss: 0.012646\n",
      "[334/01215] train_loss: 0.011960\n",
      "[335/00039] train_loss: 0.012875\n",
      "[335/00089] train_loss: 0.012912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[335/00139] train_loss: 0.013226\n",
      "[335/00189] train_loss: 0.012772\n",
      "[335/00239] train_loss: 0.012122\n",
      "[335/00289] train_loss: 0.012521\n",
      "[335/00339] train_loss: 0.012282\n",
      "[335/00389] train_loss: 0.012229\n",
      "[335/00439] train_loss: 0.012816\n",
      "[335/00489] train_loss: 0.012178\n",
      "[335/00539] train_loss: 0.012839\n",
      "[335/00589] train_loss: 0.012280\n",
      "[335/00639] train_loss: 0.012695\n",
      "[335/00689] train_loss: 0.012806\n",
      "[335/00739] train_loss: 0.012939\n",
      "[335/00789] train_loss: 0.012375\n",
      "[335/00839] train_loss: 0.011749\n",
      "[335/00889] train_loss: 0.012232\n",
      "[335/00939] train_loss: 0.012911\n",
      "[335/00989] train_loss: 0.013066\n",
      "[335/01039] train_loss: 0.013201\n",
      "[335/01089] train_loss: 0.012433\n",
      "[335/01139] train_loss: 0.011517\n",
      "[335/01189] train_loss: 0.012904\n",
      "[336/00013] train_loss: 0.012584\n",
      "[336/00063] train_loss: 0.013682\n",
      "[336/00113] train_loss: 0.012931\n",
      "[336/00163] train_loss: 0.012960\n",
      "[336/00213] train_loss: 0.013544\n",
      "[336/00263] train_loss: 0.012731\n",
      "[336/00313] train_loss: 0.013140\n",
      "[336/00363] train_loss: 0.012126\n",
      "[336/00413] train_loss: 0.012211\n",
      "[336/00463] train_loss: 0.012604\n",
      "[336/00513] train_loss: 0.012558\n",
      "[336/00563] train_loss: 0.012691\n",
      "[336/00613] train_loss: 0.012683\n",
      "[336/00663] train_loss: 0.012557\n",
      "[336/00713] train_loss: 0.012311\n",
      "[336/00763] train_loss: 0.012880\n",
      "[336/00813] train_loss: 0.013304\n",
      "[336/00863] train_loss: 0.012575\n",
      "[336/00913] train_loss: 0.011796\n",
      "[336/00963] train_loss: 0.011887\n",
      "[336/01013] train_loss: 0.013076\n",
      "[336/01063] train_loss: 0.012262\n",
      "[336/01113] train_loss: 0.012112\n",
      "[336/01163] train_loss: 0.011965\n",
      "[336/01213] train_loss: 0.011972\n",
      "[337/00037] train_loss: 0.012231\n",
      "[337/00087] train_loss: 0.013126\n",
      "[337/00137] train_loss: 0.012573\n",
      "[337/00187] train_loss: 0.013173\n",
      "[337/00237] train_loss: 0.012354\n",
      "[337/00287] train_loss: 0.012193\n",
      "[337/00337] train_loss: 0.012504\n",
      "[337/00387] train_loss: 0.012426\n",
      "[337/00437] train_loss: 0.012827\n",
      "[337/00487] train_loss: 0.012397\n",
      "[337/00537] train_loss: 0.012463\n",
      "[337/00587] train_loss: 0.012554\n",
      "[337/00637] train_loss: 0.012582\n",
      "[337/00687] train_loss: 0.012460\n",
      "[337/00737] train_loss: 0.013733\n",
      "[337/00787] train_loss: 0.012340\n",
      "[337/00837] train_loss: 0.012613\n",
      "[337/00887] train_loss: 0.012827\n",
      "[337/00937] train_loss: 0.012240\n",
      "[337/00987] train_loss: 0.013222\n",
      "[337/01037] train_loss: 0.012313\n",
      "[337/01087] train_loss: 0.012320\n",
      "[337/01137] train_loss: 0.012453\n",
      "[337/01187] train_loss: 0.012552\n",
      "[338/00011] train_loss: 0.012673\n",
      "[338/00061] train_loss: 0.013193\n",
      "[338/00111] train_loss: 0.013212\n",
      "[338/00161] train_loss: 0.013075\n",
      "[338/00211] train_loss: 0.012847\n",
      "[338/00261] train_loss: 0.012789\n",
      "[338/00311] train_loss: 0.013332\n",
      "[338/00361] train_loss: 0.013701\n",
      "[338/00411] train_loss: 0.012211\n",
      "[338/00461] train_loss: 0.012001\n",
      "[338/00511] train_loss: 0.012161\n",
      "[338/00561] train_loss: 0.012221\n",
      "[338/00611] train_loss: 0.012019\n",
      "[338/00661] train_loss: 0.013345\n",
      "[338/00711] train_loss: 0.012654\n",
      "[338/00761] train_loss: 0.012787\n",
      "[338/00811] train_loss: 0.012131\n",
      "[338/00861] train_loss: 0.012236\n",
      "[338/00911] train_loss: 0.012888\n",
      "[338/00961] train_loss: 0.012385\n",
      "[338/01011] train_loss: 0.012247\n",
      "[338/01061] train_loss: 0.012685\n",
      "[338/01111] train_loss: 0.012780\n",
      "[338/01161] train_loss: 0.012157\n",
      "[338/01211] train_loss: 0.011729\n",
      "[339/00035] train_loss: 0.012933\n",
      "[339/00085] train_loss: 0.014122\n",
      "[339/00135] train_loss: 0.012516\n",
      "[339/00185] train_loss: 0.012665\n",
      "[339/00235] train_loss: 0.012983\n",
      "[339/00285] train_loss: 0.012868\n",
      "[339/00335] train_loss: 0.012884\n",
      "[339/00385] train_loss: 0.013047\n",
      "[339/00435] train_loss: 0.012207\n",
      "[339/00485] train_loss: 0.012666\n",
      "[339/00535] train_loss: 0.012114\n",
      "[339/00585] train_loss: 0.011879\n",
      "[339/00635] train_loss: 0.012727\n",
      "[339/00685] train_loss: 0.012492\n",
      "[339/00735] train_loss: 0.012295\n",
      "[339/00785] train_loss: 0.012590\n",
      "[339/00835] train_loss: 0.011873\n",
      "[339/00885] train_loss: 0.012900\n",
      "[339/00935] train_loss: 0.011614\n",
      "[339/00985] train_loss: 0.012919\n",
      "[339/01035] train_loss: 0.012442\n",
      "[339/01085] train_loss: 0.011825\n",
      "[339/01135] train_loss: 0.012317\n",
      "[339/01185] train_loss: 0.013032\n",
      "[340/00009] train_loss: 0.012871\n",
      "[340/00059] train_loss: 0.013981\n",
      "[340/00109] train_loss: 0.013322\n",
      "[340/00159] train_loss: 0.012301\n",
      "[340/00209] train_loss: 0.012814\n",
      "[340/00259] train_loss: 0.012847\n",
      "[340/00309] train_loss: 0.013083\n",
      "[340/00359] train_loss: 0.012961\n",
      "[340/00409] train_loss: 0.012625\n",
      "[340/00459] train_loss: 0.012973\n",
      "[340/00509] train_loss: 0.012412\n",
      "[340/00559] train_loss: 0.013240\n",
      "[340/00609] train_loss: 0.012758\n",
      "[340/00659] train_loss: 0.012542\n",
      "[340/00709] train_loss: 0.011853\n",
      "[340/00759] train_loss: 0.012421\n",
      "[340/00809] train_loss: 0.012535\n",
      "[340/00859] train_loss: 0.012291\n",
      "[340/00909] train_loss: 0.012866\n",
      "[340/00959] train_loss: 0.012166\n",
      "[340/01009] train_loss: 0.011618\n",
      "[340/01059] train_loss: 0.012684\n",
      "[340/01109] train_loss: 0.011925\n",
      "[340/01159] train_loss: 0.012210\n",
      "[340/01209] train_loss: 0.011424\n",
      "[341/00033] train_loss: 0.012958\n",
      "[341/00083] train_loss: 0.013330\n",
      "[341/00133] train_loss: 0.013080\n",
      "[341/00183] train_loss: 0.012806\n",
      "[341/00233] train_loss: 0.013354\n",
      "[341/00283] train_loss: 0.012529\n",
      "[341/00333] train_loss: 0.012686\n",
      "[341/00383] train_loss: 0.012534\n",
      "[341/00433] train_loss: 0.012409\n",
      "[341/00483] train_loss: 0.012775\n",
      "[341/00533] train_loss: 0.012347\n",
      "[341/00583] train_loss: 0.012504\n",
      "[341/00633] train_loss: 0.012397\n",
      "[341/00683] train_loss: 0.012859\n",
      "[341/00733] train_loss: 0.012679\n",
      "[341/00783] train_loss: 0.012296\n",
      "[341/00833] train_loss: 0.012682\n",
      "[341/00883] train_loss: 0.011968\n",
      "[341/00933] train_loss: 0.012076\n",
      "[341/00983] train_loss: 0.012494\n",
      "[341/01033] train_loss: 0.012392\n",
      "[341/01083] train_loss: 0.012201\n",
      "[341/01133] train_loss: 0.012355\n",
      "[341/01183] train_loss: 0.013044\n",
      "[342/00007] train_loss: 0.012807\n",
      "[342/00057] train_loss: 0.013235\n",
      "[342/00107] train_loss: 0.013284\n",
      "[342/00157] train_loss: 0.013335\n",
      "[342/00207] train_loss: 0.013468\n",
      "[342/00257] train_loss: 0.013043\n",
      "[342/00307] train_loss: 0.013111\n",
      "[342/00357] train_loss: 0.012437\n",
      "[342/00407] train_loss: 0.012135\n",
      "[342/00457] train_loss: 0.012929\n",
      "[342/00507] train_loss: 0.012713\n",
      "[342/00557] train_loss: 0.012910\n",
      "[342/00607] train_loss: 0.012457\n",
      "[342/00657] train_loss: 0.012689\n",
      "[342/00707] train_loss: 0.011662\n",
      "[342/00757] train_loss: 0.012314\n",
      "[342/00807] train_loss: 0.012162\n",
      "[342/00857] train_loss: 0.011454\n",
      "[342/00907] train_loss: 0.012687\n",
      "[342/00957] train_loss: 0.013282\n",
      "[342/01007] train_loss: 0.011731\n",
      "[342/01057] train_loss: 0.012768\n",
      "[342/01107] train_loss: 0.011919\n",
      "[342/01157] train_loss: 0.012668\n",
      "[342/01207] train_loss: 0.012518\n",
      "[343/00031] train_loss: 0.013267\n",
      "[343/00081] train_loss: 0.012939\n",
      "[343/00131] train_loss: 0.013183\n",
      "[343/00181] train_loss: 0.012360\n",
      "[343/00231] train_loss: 0.012597\n",
      "[343/00281] train_loss: 0.012541\n",
      "[343/00331] train_loss: 0.013433\n",
      "[343/00381] train_loss: 0.013398\n",
      "[343/00431] train_loss: 0.013125\n",
      "[343/00481] train_loss: 0.012501\n",
      "[343/00531] train_loss: 0.011605\n",
      "[343/00581] train_loss: 0.012336\n",
      "[343/00631] train_loss: 0.011727\n",
      "[343/00681] train_loss: 0.012331\n",
      "[343/00731] train_loss: 0.011959\n",
      "[343/00781] train_loss: 0.012057\n",
      "[343/00831] train_loss: 0.012954\n",
      "[343/00881] train_loss: 0.012432\n",
      "[343/00931] train_loss: 0.012559\n",
      "[343/00981] train_loss: 0.012353\n",
      "[343/01031] train_loss: 0.013165\n",
      "[343/01081] train_loss: 0.012450\n",
      "[343/01131] train_loss: 0.012478\n",
      "[343/01181] train_loss: 0.011672\n",
      "[344/00005] train_loss: 0.013670\n",
      "[344/00055] train_loss: 0.013876\n",
      "[344/00105] train_loss: 0.013431\n",
      "[344/00155] train_loss: 0.013161\n",
      "[344/00205] train_loss: 0.012625\n",
      "[344/00255] train_loss: 0.012314\n",
      "[344/00305] train_loss: 0.012732\n",
      "[344/00355] train_loss: 0.012727\n",
      "[344/00405] train_loss: 0.011898\n",
      "[344/00455] train_loss: 0.011965\n",
      "[344/00505] train_loss: 0.011970\n",
      "[344/00555] train_loss: 0.012219\n",
      "[344/00605] train_loss: 0.012678\n",
      "[344/00655] train_loss: 0.011036\n",
      "[344/00705] train_loss: 0.012340\n",
      "[344/00755] train_loss: 0.012344\n",
      "[344/00805] train_loss: 0.012377\n",
      "[344/00855] train_loss: 0.011595\n",
      "[344/00905] train_loss: 0.011432\n",
      "[344/00955] train_loss: 0.012143\n",
      "[344/01005] train_loss: 0.012560\n",
      "[344/01055] train_loss: 0.013030\n",
      "[344/01105] train_loss: 0.012412\n",
      "[344/01155] train_loss: 0.012679\n",
      "[344/01205] train_loss: 0.012511\n",
      "[345/00029] train_loss: 0.013632\n",
      "[345/00079] train_loss: 0.012896\n",
      "[345/00129] train_loss: 0.013356\n",
      "[345/00179] train_loss: 0.012978\n",
      "[345/00229] train_loss: 0.012665\n",
      "[345/00279] train_loss: 0.012081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[345/00329] train_loss: 0.013060\n",
      "[345/00379] train_loss: 0.013110\n",
      "[345/00429] train_loss: 0.012139\n",
      "[345/00479] train_loss: 0.013249\n",
      "[345/00529] train_loss: 0.012789\n",
      "[345/00579] train_loss: 0.012489\n",
      "[345/00629] train_loss: 0.012715\n",
      "[345/00679] train_loss: 0.012614\n",
      "[345/00729] train_loss: 0.012555\n",
      "[345/00779] train_loss: 0.012805\n",
      "[345/00829] train_loss: 0.012606\n",
      "[345/00879] train_loss: 0.012094\n",
      "[345/00929] train_loss: 0.012566\n",
      "[345/00979] train_loss: 0.011565\n",
      "[345/01029] train_loss: 0.012289\n",
      "[345/01079] train_loss: 0.012208\n",
      "[345/01129] train_loss: 0.012204\n",
      "[345/01179] train_loss: 0.011969\n",
      "[346/00003] train_loss: 0.012390\n",
      "[346/00053] train_loss: 0.013182\n",
      "[346/00103] train_loss: 0.013316\n",
      "[346/00153] train_loss: 0.012544\n",
      "[346/00203] train_loss: 0.013021\n",
      "[346/00253] train_loss: 0.013402\n",
      "[346/00303] train_loss: 0.013013\n",
      "[346/00353] train_loss: 0.013282\n",
      "[346/00403] train_loss: 0.012035\n",
      "[346/00453] train_loss: 0.012761\n",
      "[346/00503] train_loss: 0.012490\n",
      "[346/00553] train_loss: 0.012531\n",
      "[346/00603] train_loss: 0.012203\n",
      "[346/00653] train_loss: 0.011905\n",
      "[346/00703] train_loss: 0.012939\n",
      "[346/00753] train_loss: 0.012268\n",
      "[346/00803] train_loss: 0.011807\n",
      "[346/00853] train_loss: 0.012401\n",
      "[346/00903] train_loss: 0.012483\n",
      "[346/00953] train_loss: 0.011814\n",
      "[346/01003] train_loss: 0.012188\n",
      "[346/01053] train_loss: 0.012002\n",
      "[346/01103] train_loss: 0.012934\n",
      "[346/01153] train_loss: 0.012274\n",
      "[346/01203] train_loss: 0.012360\n",
      "[347/00027] train_loss: 0.012891\n",
      "[347/00077] train_loss: 0.012691\n",
      "[347/00127] train_loss: 0.012561\n",
      "[347/00177] train_loss: 0.012103\n",
      "[347/00227] train_loss: 0.012933\n",
      "[347/00277] train_loss: 0.012533\n",
      "[347/00327] train_loss: 0.012884\n",
      "[347/00377] train_loss: 0.012194\n",
      "[347/00427] train_loss: 0.012015\n",
      "[347/00477] train_loss: 0.012751\n",
      "[347/00527] train_loss: 0.012518\n",
      "[347/00577] train_loss: 0.013123\n",
      "[347/00627] train_loss: 0.011931\n",
      "[347/00677] train_loss: 0.012969\n",
      "[347/00727] train_loss: 0.012919\n",
      "[347/00777] train_loss: 0.012855\n",
      "[347/00827] train_loss: 0.012995\n",
      "[347/00877] train_loss: 0.013006\n",
      "[347/00927] train_loss: 0.013085\n",
      "[347/00977] train_loss: 0.011835\n",
      "[347/01027] train_loss: 0.012572\n",
      "[347/01077] train_loss: 0.013087\n",
      "[347/01127] train_loss: 0.012665\n",
      "[347/01177] train_loss: 0.012260\n",
      "[348/00001] train_loss: 0.012821\n",
      "[348/00051] train_loss: 0.012874\n",
      "[348/00101] train_loss: 0.013010\n",
      "[348/00151] train_loss: 0.012392\n",
      "[348/00201] train_loss: 0.012368\n",
      "[348/00251] train_loss: 0.012285\n",
      "[348/00301] train_loss: 0.013243\n",
      "[348/00351] train_loss: 0.012696\n",
      "[348/00401] train_loss: 0.012815\n",
      "[348/00451] train_loss: 0.012301\n",
      "[348/00501] train_loss: 0.013153\n",
      "[348/00551] train_loss: 0.012580\n",
      "[348/00601] train_loss: 0.012718\n",
      "[348/00651] train_loss: 0.012190\n",
      "[348/00701] train_loss: 0.012000\n",
      "[348/00751] train_loss: 0.012666\n",
      "[348/00801] train_loss: 0.012225\n",
      "[348/00851] train_loss: 0.012616\n",
      "[348/00901] train_loss: 0.011843\n",
      "[348/00951] train_loss: 0.012876\n",
      "[348/01001] train_loss: 0.012662\n",
      "[348/01051] train_loss: 0.012087\n",
      "[348/01101] train_loss: 0.011961\n",
      "[348/01151] train_loss: 0.012330\n",
      "[348/01201] train_loss: 0.012222\n",
      "[349/00025] train_loss: 0.012605\n",
      "[349/00075] train_loss: 0.012558\n",
      "[349/00125] train_loss: 0.013824\n",
      "[349/00175] train_loss: 0.013057\n",
      "[349/00225] train_loss: 0.013004\n",
      "[349/00275] train_loss: 0.012916\n",
      "[349/00325] train_loss: 0.012138\n",
      "[349/00375] train_loss: 0.013027\n",
      "[349/00425] train_loss: 0.012859\n",
      "[349/00475] train_loss: 0.011963\n",
      "[349/00525] train_loss: 0.012489\n",
      "[349/00575] train_loss: 0.012510\n",
      "[349/00625] train_loss: 0.012967\n",
      "[349/00675] train_loss: 0.012258\n",
      "[349/00725] train_loss: 0.011912\n",
      "[349/00775] train_loss: 0.012300\n",
      "[349/00825] train_loss: 0.012375\n",
      "[349/00875] train_loss: 0.012497\n",
      "[349/00925] train_loss: 0.011467\n",
      "[349/00975] train_loss: 0.012117\n",
      "[349/01025] train_loss: 0.013123\n",
      "[349/01075] train_loss: 0.012315\n",
      "[349/01125] train_loss: 0.012840\n",
      "[349/01175] train_loss: 0.012296\n",
      "[349/01225] train_loss: 0.013013\n",
      "[350/00049] train_loss: 0.013943\n",
      "[350/00099] train_loss: 0.012946\n",
      "[350/00149] train_loss: 0.012988\n",
      "[350/00199] train_loss: 0.013155\n",
      "[350/00249] train_loss: 0.012485\n",
      "[350/00299] train_loss: 0.012181\n",
      "[350/00349] train_loss: 0.013117\n",
      "[350/00399] train_loss: 0.013043\n",
      "[350/00449] train_loss: 0.012021\n",
      "[350/00499] train_loss: 0.012433\n",
      "[350/00549] train_loss: 0.012270\n",
      "[350/00599] train_loss: 0.012166\n",
      "[350/00649] train_loss: 0.012137\n",
      "[350/00699] train_loss: 0.012999\n",
      "[350/00749] train_loss: 0.012535\n",
      "[350/00799] train_loss: 0.012290\n",
      "[350/00849] train_loss: 0.012396\n",
      "[350/00899] train_loss: 0.012280\n",
      "[350/00949] train_loss: 0.012140\n",
      "[350/00999] train_loss: 0.012196\n",
      "[350/01049] train_loss: 0.012738\n",
      "[350/01099] train_loss: 0.012113\n",
      "[350/01149] train_loss: 0.012573\n",
      "[350/01199] train_loss: 0.012525\n",
      "[351/00023] train_loss: 0.012718\n",
      "[351/00073] train_loss: 0.012923\n",
      "[351/00123] train_loss: 0.012728\n",
      "[351/00173] train_loss: 0.012490\n",
      "[351/00223] train_loss: 0.012588\n",
      "[351/00273] train_loss: 0.012502\n",
      "[351/00323] train_loss: 0.012225\n",
      "[351/00373] train_loss: 0.012498\n",
      "[351/00423] train_loss: 0.012233\n",
      "[351/00473] train_loss: 0.012602\n",
      "[351/00523] train_loss: 0.012857\n",
      "[351/00573] train_loss: 0.012095\n",
      "[351/00623] train_loss: 0.012080\n",
      "[351/00673] train_loss: 0.012504\n",
      "[351/00723] train_loss: 0.012160\n",
      "[351/00773] train_loss: 0.012680\n",
      "[351/00823] train_loss: 0.012852\n",
      "[351/00873] train_loss: 0.012674\n",
      "[351/00923] train_loss: 0.012643\n",
      "[351/00973] train_loss: 0.012010\n",
      "[351/01023] train_loss: 0.012593\n",
      "[351/01073] train_loss: 0.012911\n",
      "[351/01123] train_loss: 0.012257\n",
      "[351/01173] train_loss: 0.012487\n",
      "[351/01223] train_loss: 0.011658\n",
      "[352/00047] train_loss: 0.013702\n",
      "[352/00097] train_loss: 0.012988\n",
      "[352/00147] train_loss: 0.012665\n",
      "[352/00197] train_loss: 0.012944\n",
      "[352/00247] train_loss: 0.012654\n",
      "[352/00297] train_loss: 0.012555\n",
      "[352/00347] train_loss: 0.012156\n",
      "[352/00397] train_loss: 0.012188\n",
      "[352/00447] train_loss: 0.011811\n",
      "[352/00497] train_loss: 0.012224\n",
      "[352/00547] train_loss: 0.012642\n",
      "[352/00597] train_loss: 0.012474\n",
      "[352/00647] train_loss: 0.012281\n",
      "[352/00697] train_loss: 0.012607\n",
      "[352/00747] train_loss: 0.012710\n",
      "[352/00797] train_loss: 0.012559\n",
      "[352/00847] train_loss: 0.012021\n",
      "[352/00897] train_loss: 0.013821\n",
      "[352/00947] train_loss: 0.012982\n",
      "[352/00997] train_loss: 0.012560\n",
      "[352/01047] train_loss: 0.012252\n",
      "[352/01097] train_loss: 0.012755\n",
      "[352/01147] train_loss: 0.012597\n",
      "[352/01197] train_loss: 0.012018\n",
      "[353/00021] train_loss: 0.012949\n",
      "[353/00071] train_loss: 0.013268\n",
      "[353/00121] train_loss: 0.012759\n",
      "[353/00171] train_loss: 0.012692\n",
      "[353/00221] train_loss: 0.013158\n",
      "[353/00271] train_loss: 0.012745\n",
      "[353/00321] train_loss: 0.012485\n",
      "[353/00371] train_loss: 0.012683\n",
      "[353/00421] train_loss: 0.012263\n",
      "[353/00471] train_loss: 0.012842\n",
      "[353/00521] train_loss: 0.012205\n",
      "[353/00571] train_loss: 0.012862\n",
      "[353/00621] train_loss: 0.012634\n",
      "[353/00671] train_loss: 0.012413\n",
      "[353/00721] train_loss: 0.012705\n",
      "[353/00771] train_loss: 0.012511\n",
      "[353/00821] train_loss: 0.012233\n",
      "[353/00871] train_loss: 0.011959\n",
      "[353/00921] train_loss: 0.012678\n",
      "[353/00971] train_loss: 0.012255\n",
      "[353/01021] train_loss: 0.012961\n",
      "[353/01071] train_loss: 0.011697\n",
      "[353/01121] train_loss: 0.012042\n",
      "[353/01171] train_loss: 0.012309\n",
      "[353/01221] train_loss: 0.012637\n",
      "[354/00045] train_loss: 0.013112\n",
      "[354/00095] train_loss: 0.013147\n",
      "[354/00145] train_loss: 0.013435\n",
      "[354/00195] train_loss: 0.012804\n",
      "[354/00245] train_loss: 0.011880\n",
      "[354/00295] train_loss: 0.012492\n",
      "[354/00345] train_loss: 0.013274\n",
      "[354/00395] train_loss: 0.012043\n",
      "[354/00445] train_loss: 0.012100\n",
      "[354/00495] train_loss: 0.013025\n",
      "[354/00545] train_loss: 0.011877\n",
      "[354/00595] train_loss: 0.012183\n",
      "[354/00645] train_loss: 0.011945\n",
      "[354/00695] train_loss: 0.012512\n",
      "[354/00745] train_loss: 0.012274\n",
      "[354/00795] train_loss: 0.012606\n",
      "[354/00845] train_loss: 0.012724\n",
      "[354/00895] train_loss: 0.011541\n",
      "[354/00945] train_loss: 0.012717\n",
      "[354/00995] train_loss: 0.012000\n",
      "[354/01045] train_loss: 0.012545\n",
      "[354/01095] train_loss: 0.012841\n",
      "[354/01145] train_loss: 0.012070\n",
      "[354/01195] train_loss: 0.012943\n",
      "[355/00019] train_loss: 0.012370\n",
      "[355/00069] train_loss: 0.013346\n",
      "[355/00119] train_loss: 0.012495\n",
      "[355/00169] train_loss: 0.012987\n",
      "[355/00219] train_loss: 0.013421\n",
      "[355/00269] train_loss: 0.012660\n",
      "[355/00319] train_loss: 0.012359\n",
      "[355/00369] train_loss: 0.012501\n",
      "[355/00419] train_loss: 0.012529\n",
      "[355/00469] train_loss: 0.012693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[355/00519] train_loss: 0.012083\n",
      "[355/00569] train_loss: 0.012427\n",
      "[355/00619] train_loss: 0.011979\n",
      "[355/00669] train_loss: 0.012362\n",
      "[355/00719] train_loss: 0.013024\n",
      "[355/00769] train_loss: 0.012760\n",
      "[355/00819] train_loss: 0.011438\n",
      "[355/00869] train_loss: 0.011922\n",
      "[355/00919] train_loss: 0.012539\n",
      "[355/00969] train_loss: 0.013042\n",
      "[355/01019] train_loss: 0.013318\n",
      "[355/01069] train_loss: 0.012654\n",
      "[355/01119] train_loss: 0.011710\n",
      "[355/01169] train_loss: 0.012645\n",
      "[355/01219] train_loss: 0.011774\n",
      "[356/00043] train_loss: 0.013124\n",
      "[356/00093] train_loss: 0.013150\n",
      "[356/00143] train_loss: 0.012971\n",
      "[356/00193] train_loss: 0.013152\n",
      "[356/00243] train_loss: 0.012846\n",
      "[356/00293] train_loss: 0.012348\n",
      "[356/00343] train_loss: 0.012334\n",
      "[356/00393] train_loss: 0.012901\n",
      "[356/00443] train_loss: 0.012938\n",
      "[356/00493] train_loss: 0.013593\n",
      "[356/00543] train_loss: 0.012289\n",
      "[356/00593] train_loss: 0.012182\n",
      "[356/00643] train_loss: 0.012280\n",
      "[356/00693] train_loss: 0.012526\n",
      "[356/00743] train_loss: 0.012205\n",
      "[356/00793] train_loss: 0.011882\n",
      "[356/00843] train_loss: 0.012269\n",
      "[356/00893] train_loss: 0.012504\n",
      "[356/00943] train_loss: 0.012627\n",
      "[356/00993] train_loss: 0.011799\n",
      "[356/01043] train_loss: 0.011833\n",
      "[356/01093] train_loss: 0.011878\n",
      "[356/01143] train_loss: 0.011934\n",
      "[356/01193] train_loss: 0.011860\n",
      "[357/00017] train_loss: 0.012879\n",
      "[357/00067] train_loss: 0.012981\n",
      "[357/00117] train_loss: 0.012796\n",
      "[357/00167] train_loss: 0.012112\n",
      "[357/00217] train_loss: 0.012440\n",
      "[357/00267] train_loss: 0.012363\n",
      "[357/00317] train_loss: 0.011789\n",
      "[357/00367] train_loss: 0.013228\n",
      "[357/00417] train_loss: 0.012081\n",
      "[357/00467] train_loss: 0.012306\n",
      "[357/00517] train_loss: 0.013051\n",
      "[357/00567] train_loss: 0.012543\n",
      "[357/00617] train_loss: 0.012090\n",
      "[357/00667] train_loss: 0.013056\n",
      "[357/00717] train_loss: 0.012471\n",
      "[357/00767] train_loss: 0.012441\n",
      "[357/00817] train_loss: 0.012021\n",
      "[357/00867] train_loss: 0.012715\n",
      "[357/00917] train_loss: 0.012573\n",
      "[357/00967] train_loss: 0.011837\n",
      "[357/01017] train_loss: 0.011935\n",
      "[357/01067] train_loss: 0.011828\n",
      "[357/01117] train_loss: 0.012610\n",
      "[357/01167] train_loss: 0.012257\n",
      "[357/01217] train_loss: 0.012909\n",
      "[358/00041] train_loss: 0.012765\n",
      "[358/00091] train_loss: 0.012157\n",
      "[358/00141] train_loss: 0.013304\n",
      "[358/00191] train_loss: 0.012882\n",
      "[358/00241] train_loss: 0.012133\n",
      "[358/00291] train_loss: 0.012411\n",
      "[358/00341] train_loss: 0.012443\n",
      "[358/00391] train_loss: 0.012521\n",
      "[358/00441] train_loss: 0.012099\n",
      "[358/00491] train_loss: 0.012494\n",
      "[358/00541] train_loss: 0.011907\n",
      "[358/00591] train_loss: 0.012531\n",
      "[358/00641] train_loss: 0.012142\n",
      "[358/00691] train_loss: 0.012209\n",
      "[358/00741] train_loss: 0.011981\n",
      "[358/00791] train_loss: 0.012993\n",
      "[358/00841] train_loss: 0.012652\n",
      "[358/00891] train_loss: 0.012974\n",
      "[358/00941] train_loss: 0.012371\n",
      "[358/00991] train_loss: 0.012527\n",
      "[358/01041] train_loss: 0.012519\n",
      "[358/01091] train_loss: 0.012040\n",
      "[358/01141] train_loss: 0.011798\n",
      "[358/01191] train_loss: 0.012609\n",
      "[359/00015] train_loss: 0.012595\n",
      "[359/00065] train_loss: 0.013214\n",
      "[359/00115] train_loss: 0.012874\n",
      "[359/00165] train_loss: 0.012685\n",
      "[359/00215] train_loss: 0.012895\n",
      "[359/00265] train_loss: 0.013209\n",
      "[359/00315] train_loss: 0.012386\n",
      "[359/00365] train_loss: 0.012484\n",
      "[359/00415] train_loss: 0.011902\n",
      "[359/00465] train_loss: 0.012543\n",
      "[359/00515] train_loss: 0.012754\n",
      "[359/00565] train_loss: 0.013161\n",
      "[359/00615] train_loss: 0.013295\n",
      "[359/00665] train_loss: 0.012806\n",
      "[359/00715] train_loss: 0.012213\n",
      "[359/00765] train_loss: 0.012823\n",
      "[359/00815] train_loss: 0.012195\n",
      "[359/00865] train_loss: 0.012466\n",
      "[359/00915] train_loss: 0.012525\n",
      "[359/00965] train_loss: 0.012388\n",
      "[359/01015] train_loss: 0.012576\n",
      "[359/01065] train_loss: 0.012547\n",
      "[359/01115] train_loss: 0.011760\n",
      "[359/01165] train_loss: 0.012269\n",
      "[359/01215] train_loss: 0.012477\n",
      "[360/00039] train_loss: 0.013313\n",
      "[360/00089] train_loss: 0.012535\n",
      "[360/00139] train_loss: 0.012602\n",
      "[360/00189] train_loss: 0.012963\n",
      "[360/00239] train_loss: 0.013079\n",
      "[360/00289] train_loss: 0.012272\n",
      "[360/00339] train_loss: 0.012692\n",
      "[360/00389] train_loss: 0.012963\n",
      "[360/00439] train_loss: 0.011872\n",
      "[360/00489] train_loss: 0.012822\n",
      "[360/00539] train_loss: 0.012142\n",
      "[360/00589] train_loss: 0.011805\n",
      "[360/00639] train_loss: 0.012183\n",
      "[360/00689] train_loss: 0.012197\n",
      "[360/00739] train_loss: 0.012266\n",
      "[360/00789] train_loss: 0.012688\n",
      "[360/00839] train_loss: 0.012295\n",
      "[360/00889] train_loss: 0.012395\n",
      "[360/00939] train_loss: 0.011634\n",
      "[360/00989] train_loss: 0.012392\n",
      "[360/01039] train_loss: 0.013001\n",
      "[360/01089] train_loss: 0.012502\n",
      "[360/01139] train_loss: 0.012271\n",
      "[360/01189] train_loss: 0.011571\n",
      "[361/00013] train_loss: 0.012556\n",
      "[361/00063] train_loss: 0.012781\n",
      "[361/00113] train_loss: 0.013073\n",
      "[361/00163] train_loss: 0.012770\n",
      "[361/00213] train_loss: 0.012835\n",
      "[361/00263] train_loss: 0.012888\n",
      "[361/00313] train_loss: 0.012228\n",
      "[361/00363] train_loss: 0.012626\n",
      "[361/00413] train_loss: 0.012006\n",
      "[361/00463] train_loss: 0.012879\n",
      "[361/00513] train_loss: 0.012422\n",
      "[361/00563] train_loss: 0.013155\n",
      "[361/00613] train_loss: 0.012166\n",
      "[361/00663] train_loss: 0.013073\n",
      "[361/00713] train_loss: 0.012088\n",
      "[361/00763] train_loss: 0.012799\n",
      "[361/00813] train_loss: 0.012390\n",
      "[361/00863] train_loss: 0.011568\n",
      "[361/00913] train_loss: 0.012486\n",
      "[361/00963] train_loss: 0.012988\n",
      "[361/01013] train_loss: 0.011913\n",
      "[361/01063] train_loss: 0.012865\n",
      "[361/01113] train_loss: 0.012620\n",
      "[361/01163] train_loss: 0.011982\n",
      "[361/01213] train_loss: 0.011954\n",
      "[362/00037] train_loss: 0.012972\n",
      "[362/00087] train_loss: 0.013140\n",
      "[362/00137] train_loss: 0.012968\n",
      "[362/00187] train_loss: 0.012814\n",
      "[362/00237] train_loss: 0.012487\n",
      "[362/00287] train_loss: 0.012363\n",
      "[362/00337] train_loss: 0.012231\n",
      "[362/00387] train_loss: 0.013121\n",
      "[362/00437] train_loss: 0.013029\n",
      "[362/00487] train_loss: 0.012848\n",
      "[362/00537] train_loss: 0.011927\n",
      "[362/00587] train_loss: 0.012102\n",
      "[362/00637] train_loss: 0.012024\n",
      "[362/00687] train_loss: 0.013041\n",
      "[362/00737] train_loss: 0.012207\n",
      "[362/00787] train_loss: 0.011872\n",
      "[362/00837] train_loss: 0.012397\n",
      "[362/00887] train_loss: 0.012781\n",
      "[362/00937] train_loss: 0.011797\n",
      "[362/00987] train_loss: 0.011443\n",
      "[362/01037] train_loss: 0.011684\n",
      "[362/01087] train_loss: 0.012438\n",
      "[362/01137] train_loss: 0.012234\n",
      "[362/01187] train_loss: 0.012633\n",
      "[363/00011] train_loss: 0.012389\n",
      "[363/00061] train_loss: 0.013008\n",
      "[363/00111] train_loss: 0.012572\n",
      "[363/00161] train_loss: 0.013147\n",
      "[363/00211] train_loss: 0.012664\n",
      "[363/00261] train_loss: 0.011815\n",
      "[363/00311] train_loss: 0.012065\n",
      "[363/00361] train_loss: 0.013148\n",
      "[363/00411] train_loss: 0.013011\n",
      "[363/00461] train_loss: 0.012618\n",
      "[363/00511] train_loss: 0.012694\n",
      "[363/00561] train_loss: 0.011862\n",
      "[363/00611] train_loss: 0.012714\n",
      "[363/00661] train_loss: 0.012712\n",
      "[363/00711] train_loss: 0.012772\n",
      "[363/00761] train_loss: 0.012504\n",
      "[363/00811] train_loss: 0.012618\n",
      "[363/00861] train_loss: 0.012539\n",
      "[363/00911] train_loss: 0.011786\n",
      "[363/00961] train_loss: 0.012258\n",
      "[363/01011] train_loss: 0.012935\n",
      "[363/01061] train_loss: 0.012359\n",
      "[363/01111] train_loss: 0.012519\n",
      "[363/01161] train_loss: 0.013154\n",
      "[363/01211] train_loss: 0.011265\n",
      "[364/00035] train_loss: 0.013009\n",
      "[364/00085] train_loss: 0.013201\n",
      "[364/00135] train_loss: 0.013545\n",
      "[364/00185] train_loss: 0.013027\n",
      "[364/00235] train_loss: 0.012903\n",
      "[364/00285] train_loss: 0.012677\n",
      "[364/00335] train_loss: 0.012909\n",
      "[364/00385] train_loss: 0.012311\n",
      "[364/00435] train_loss: 0.013053\n",
      "[364/00485] train_loss: 0.012420\n",
      "[364/00535] train_loss: 0.012350\n",
      "[364/00585] train_loss: 0.011977\n",
      "[364/00635] train_loss: 0.011902\n",
      "[364/00685] train_loss: 0.011946\n",
      "[364/00735] train_loss: 0.012620\n",
      "[364/00785] train_loss: 0.012300\n",
      "[364/00835] train_loss: 0.012558\n",
      "[364/00885] train_loss: 0.012187\n",
      "[364/00935] train_loss: 0.011933\n",
      "[364/00985] train_loss: 0.012186\n",
      "[364/01035] train_loss: 0.012478\n",
      "[364/01085] train_loss: 0.011705\n",
      "[364/01135] train_loss: 0.011746\n",
      "[364/01185] train_loss: 0.012027\n",
      "[365/00009] train_loss: 0.011990\n",
      "[365/00059] train_loss: 0.012720\n",
      "[365/00109] train_loss: 0.012962\n",
      "[365/00159] train_loss: 0.013530\n",
      "[365/00209] train_loss: 0.013005\n",
      "[365/00259] train_loss: 0.013196\n",
      "[365/00309] train_loss: 0.013444\n",
      "[365/00359] train_loss: 0.011880\n",
      "[365/00409] train_loss: 0.012007\n",
      "[365/00459] train_loss: 0.012355\n",
      "[365/00509] train_loss: 0.012465\n",
      "[365/00559] train_loss: 0.012202\n",
      "[365/00609] train_loss: 0.011979\n",
      "[365/00659] train_loss: 0.013299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[365/00709] train_loss: 0.012372\n",
      "[365/00759] train_loss: 0.012508\n",
      "[365/00809] train_loss: 0.012537\n",
      "[365/00859] train_loss: 0.012187\n",
      "[365/00909] train_loss: 0.012306\n",
      "[365/00959] train_loss: 0.011633\n",
      "[365/01009] train_loss: 0.012942\n",
      "[365/01059] train_loss: 0.012227\n",
      "[365/01109] train_loss: 0.012137\n",
      "[365/01159] train_loss: 0.012053\n",
      "[365/01209] train_loss: 0.011359\n",
      "[366/00033] train_loss: 0.013301\n",
      "[366/00083] train_loss: 0.012177\n",
      "[366/00133] train_loss: 0.013155\n",
      "[366/00183] train_loss: 0.013381\n",
      "[366/00233] train_loss: 0.011942\n",
      "[366/00283] train_loss: 0.012208\n",
      "[366/00333] train_loss: 0.012196\n",
      "[366/00383] train_loss: 0.012321\n",
      "[366/00433] train_loss: 0.012252\n",
      "[366/00483] train_loss: 0.011733\n",
      "[366/00533] train_loss: 0.013005\n",
      "[366/00583] train_loss: 0.012112\n",
      "[366/00633] train_loss: 0.012413\n",
      "[366/00683] train_loss: 0.011617\n",
      "[366/00733] train_loss: 0.013117\n",
      "[366/00783] train_loss: 0.012325\n",
      "[366/00833] train_loss: 0.012143\n",
      "[366/00883] train_loss: 0.012679\n",
      "[366/00933] train_loss: 0.012617\n",
      "[366/00983] train_loss: 0.011926\n",
      "[366/01033] train_loss: 0.012394\n",
      "[366/01083] train_loss: 0.011954\n",
      "[366/01133] train_loss: 0.012644\n",
      "[366/01183] train_loss: 0.011637\n",
      "[367/00007] train_loss: 0.012006\n",
      "[367/00057] train_loss: 0.013113\n",
      "[367/00107] train_loss: 0.012288\n",
      "[367/00157] train_loss: 0.013127\n",
      "[367/00207] train_loss: 0.013062\n",
      "[367/00257] train_loss: 0.012826\n",
      "[367/00307] train_loss: 0.012578\n",
      "[367/00357] train_loss: 0.012322\n",
      "[367/00407] train_loss: 0.012337\n",
      "[367/00457] train_loss: 0.013179\n",
      "[367/00507] train_loss: 0.012142\n",
      "[367/00557] train_loss: 0.012624\n",
      "[367/00607] train_loss: 0.013031\n",
      "[367/00657] train_loss: 0.012416\n",
      "[367/00707] train_loss: 0.011696\n",
      "[367/00757] train_loss: 0.012279\n",
      "[367/00807] train_loss: 0.012681\n",
      "[367/00857] train_loss: 0.013236\n",
      "[367/00907] train_loss: 0.011513\n",
      "[367/00957] train_loss: 0.012372\n",
      "[367/01007] train_loss: 0.012880\n",
      "[367/01057] train_loss: 0.012213\n",
      "[367/01107] train_loss: 0.012166\n",
      "[367/01157] train_loss: 0.011931\n",
      "[367/01207] train_loss: 0.011810\n",
      "[368/00031] train_loss: 0.012015\n",
      "[368/00081] train_loss: 0.013222\n",
      "[368/00131] train_loss: 0.012716\n",
      "[368/00181] train_loss: 0.012906\n",
      "[368/00231] train_loss: 0.012793\n",
      "[368/00281] train_loss: 0.012557\n",
      "[368/00331] train_loss: 0.012175\n",
      "[368/00381] train_loss: 0.012308\n",
      "[368/00431] train_loss: 0.011732\n",
      "[368/00481] train_loss: 0.012209\n",
      "[368/00531] train_loss: 0.012516\n",
      "[368/00581] train_loss: 0.012111\n",
      "[368/00631] train_loss: 0.011545\n",
      "[368/00681] train_loss: 0.012766\n",
      "[368/00731] train_loss: 0.013227\n",
      "[368/00781] train_loss: 0.012273\n",
      "[368/00831] train_loss: 0.012934\n",
      "[368/00881] train_loss: 0.012810\n",
      "[368/00931] train_loss: 0.012295\n",
      "[368/00981] train_loss: 0.012020\n",
      "[368/01031] train_loss: 0.012589\n",
      "[368/01081] train_loss: 0.012501\n",
      "[368/01131] train_loss: 0.012247\n",
      "[368/01181] train_loss: 0.011706\n",
      "[369/00005] train_loss: 0.012396\n",
      "[369/00055] train_loss: 0.013296\n",
      "[369/00105] train_loss: 0.013020\n",
      "[369/00155] train_loss: 0.012296\n",
      "[369/00205] train_loss: 0.012295\n",
      "[369/00255] train_loss: 0.012780\n",
      "[369/00305] train_loss: 0.012629\n",
      "[369/00355] train_loss: 0.012294\n",
      "[369/00405] train_loss: 0.012819\n",
      "[369/00455] train_loss: 0.012854\n",
      "[369/00505] train_loss: 0.012164\n",
      "[369/00555] train_loss: 0.012249\n",
      "[369/00605] train_loss: 0.012672\n",
      "[369/00655] train_loss: 0.011832\n",
      "[369/00705] train_loss: 0.012879\n",
      "[369/00755] train_loss: 0.012342\n",
      "[369/00805] train_loss: 0.012556\n",
      "[369/00855] train_loss: 0.012431\n",
      "[369/00905] train_loss: 0.012036\n",
      "[369/00955] train_loss: 0.011822\n",
      "[369/01005] train_loss: 0.013012\n",
      "[369/01055] train_loss: 0.012480\n",
      "[369/01105] train_loss: 0.011830\n",
      "[369/01155] train_loss: 0.011300\n",
      "[369/01205] train_loss: 0.012134\n",
      "[370/00029] train_loss: 0.012804\n",
      "[370/00079] train_loss: 0.013596\n",
      "[370/00129] train_loss: 0.013133\n",
      "[370/00179] train_loss: 0.013247\n",
      "[370/00229] train_loss: 0.012362\n",
      "[370/00279] train_loss: 0.012797\n",
      "[370/00329] train_loss: 0.012801\n",
      "[370/00379] train_loss: 0.012459\n",
      "[370/00429] train_loss: 0.012421\n",
      "[370/00479] train_loss: 0.012438\n",
      "[370/00529] train_loss: 0.012869\n",
      "[370/00579] train_loss: 0.012098\n",
      "[370/00629] train_loss: 0.012613\n",
      "[370/00679] train_loss: 0.012063\n",
      "[370/00729] train_loss: 0.012517\n",
      "[370/00779] train_loss: 0.012883\n",
      "[370/00829] train_loss: 0.012517\n",
      "[370/00879] train_loss: 0.011986\n",
      "[370/00929] train_loss: 0.012687\n",
      "[370/00979] train_loss: 0.012523\n",
      "[370/01029] train_loss: 0.012372\n",
      "[370/01079] train_loss: 0.012705\n",
      "[370/01129] train_loss: 0.011515\n",
      "[370/01179] train_loss: 0.011751\n",
      "[371/00003] train_loss: 0.012041\n",
      "[371/00053] train_loss: 0.012637\n",
      "[371/00103] train_loss: 0.013216\n",
      "[371/00153] train_loss: 0.012571\n",
      "[371/00203] train_loss: 0.012649\n",
      "[371/00253] train_loss: 0.012669\n",
      "[371/00303] train_loss: 0.012134\n",
      "[371/00353] train_loss: 0.013105\n",
      "[371/00403] train_loss: 0.011963\n",
      "[371/00453] train_loss: 0.012255\n",
      "[371/00503] train_loss: 0.012702\n",
      "[371/00553] train_loss: 0.013001\n",
      "[371/00603] train_loss: 0.012709\n",
      "[371/00653] train_loss: 0.012474\n",
      "[371/00703] train_loss: 0.013452\n",
      "[371/00753] train_loss: 0.012329\n",
      "[371/00803] train_loss: 0.012445\n",
      "[371/00853] train_loss: 0.012253\n",
      "[371/00903] train_loss: 0.012363\n",
      "[371/00953] train_loss: 0.012432\n",
      "[371/01003] train_loss: 0.011984\n",
      "[371/01053] train_loss: 0.011124\n",
      "[371/01103] train_loss: 0.012683\n",
      "[371/01153] train_loss: 0.012361\n",
      "[371/01203] train_loss: 0.012386\n",
      "[372/00027] train_loss: 0.013527\n",
      "[372/00077] train_loss: 0.013082\n",
      "[372/00127] train_loss: 0.012729\n",
      "[372/00177] train_loss: 0.012828\n",
      "[372/00227] train_loss: 0.012682\n",
      "[372/00277] train_loss: 0.012406\n",
      "[372/00327] train_loss: 0.012531\n",
      "[372/00377] train_loss: 0.012614\n",
      "[372/00427] train_loss: 0.012350\n",
      "[372/00477] train_loss: 0.012693\n",
      "[372/00527] train_loss: 0.012200\n",
      "[372/00577] train_loss: 0.012255\n",
      "[372/00627] train_loss: 0.012260\n",
      "[372/00677] train_loss: 0.011822\n",
      "[372/00727] train_loss: 0.011919\n",
      "[372/00777] train_loss: 0.012774\n",
      "[372/00827] train_loss: 0.012144\n",
      "[372/00877] train_loss: 0.013058\n",
      "[372/00927] train_loss: 0.012755\n",
      "[372/00977] train_loss: 0.011916\n",
      "[372/01027] train_loss: 0.012316\n",
      "[372/01077] train_loss: 0.012645\n",
      "[372/01127] train_loss: 0.012672\n",
      "[372/01177] train_loss: 0.012101\n",
      "[373/00001] train_loss: 0.011755\n",
      "[373/00051] train_loss: 0.013659\n",
      "[373/00101] train_loss: 0.013078\n",
      "[373/00151] train_loss: 0.012812\n",
      "[373/00201] train_loss: 0.012149\n",
      "[373/00251] train_loss: 0.012343\n",
      "[373/00301] train_loss: 0.012334\n",
      "[373/00351] train_loss: 0.012534\n",
      "[373/00401] train_loss: 0.011877\n",
      "[373/00451] train_loss: 0.012783\n",
      "[373/00501] train_loss: 0.011779\n",
      "[373/00551] train_loss: 0.012538\n",
      "[373/00601] train_loss: 0.012813\n",
      "[373/00651] train_loss: 0.012114\n",
      "[373/00701] train_loss: 0.012923\n",
      "[373/00751] train_loss: 0.012887\n",
      "[373/00801] train_loss: 0.013171\n",
      "[373/00851] train_loss: 0.012608\n",
      "[373/00901] train_loss: 0.012874\n",
      "[373/00951] train_loss: 0.011792\n",
      "[373/01001] train_loss: 0.012303\n",
      "[373/01051] train_loss: 0.012608\n",
      "[373/01101] train_loss: 0.012342\n",
      "[373/01151] train_loss: 0.011691\n",
      "[373/01201] train_loss: 0.012351\n",
      "[374/00025] train_loss: 0.012839\n",
      "[374/00075] train_loss: 0.012932\n",
      "[374/00125] train_loss: 0.013129\n",
      "[374/00175] train_loss: 0.012364\n",
      "[374/00225] train_loss: 0.012691\n",
      "[374/00275] train_loss: 0.011984\n",
      "[374/00325] train_loss: 0.012188\n",
      "[374/00375] train_loss: 0.012036\n",
      "[374/00425] train_loss: 0.011846\n",
      "[374/00475] train_loss: 0.012035\n",
      "[374/00525] train_loss: 0.012666\n",
      "[374/00575] train_loss: 0.012073\n",
      "[374/00625] train_loss: 0.013244\n",
      "[374/00675] train_loss: 0.012229\n",
      "[374/00725] train_loss: 0.012853\n",
      "[374/00775] train_loss: 0.011893\n",
      "[374/00825] train_loss: 0.012145\n",
      "[374/00875] train_loss: 0.012751\n",
      "[374/00925] train_loss: 0.012249\n",
      "[374/00975] train_loss: 0.012902\n",
      "[374/01025] train_loss: 0.012439\n",
      "[374/01075] train_loss: 0.011821\n",
      "[374/01125] train_loss: 0.011962\n",
      "[374/01175] train_loss: 0.012561\n",
      "[374/01225] train_loss: 0.012144\n",
      "[375/00049] train_loss: 0.013450\n",
      "[375/00099] train_loss: 0.012443\n",
      "[375/00149] train_loss: 0.012461\n",
      "[375/00199] train_loss: 0.012958\n",
      "[375/00249] train_loss: 0.012829\n",
      "[375/00299] train_loss: 0.012432\n",
      "[375/00349] train_loss: 0.011882\n",
      "[375/00399] train_loss: 0.011858\n",
      "[375/00449] train_loss: 0.012504\n",
      "[375/00499] train_loss: 0.012631\n",
      "[375/00549] train_loss: 0.012207\n",
      "[375/00599] train_loss: 0.012713\n",
      "[375/00649] train_loss: 0.012793\n",
      "[375/00699] train_loss: 0.012717\n",
      "[375/00749] train_loss: 0.012507\n",
      "[375/00799] train_loss: 0.012260\n",
      "[375/00849] train_loss: 0.011598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[375/00899] train_loss: 0.012326\n",
      "[375/00949] train_loss: 0.012676\n",
      "[375/00999] train_loss: 0.012220\n",
      "[375/01049] train_loss: 0.012080\n",
      "[375/01099] train_loss: 0.012068\n",
      "[375/01149] train_loss: 0.012088\n",
      "[375/01199] train_loss: 0.012312\n",
      "[376/00023] train_loss: 0.012730\n",
      "[376/00073] train_loss: 0.012425\n",
      "[376/00123] train_loss: 0.012976\n",
      "[376/00173] train_loss: 0.012990\n",
      "[376/00223] train_loss: 0.012755\n",
      "[376/00273] train_loss: 0.012991\n",
      "[376/00323] train_loss: 0.012273\n",
      "[376/00373] train_loss: 0.012057\n",
      "[376/00423] train_loss: 0.012909\n",
      "[376/00473] train_loss: 0.012758\n",
      "[376/00523] train_loss: 0.012100\n",
      "[376/00573] train_loss: 0.012175\n",
      "[376/00623] train_loss: 0.012632\n",
      "[376/00673] train_loss: 0.011421\n",
      "[376/00723] train_loss: 0.012052\n",
      "[376/00773] train_loss: 0.011978\n",
      "[376/00823] train_loss: 0.013348\n",
      "[376/00873] train_loss: 0.012325\n",
      "[376/00923] train_loss: 0.012321\n",
      "[376/00973] train_loss: 0.012821\n",
      "[376/01023] train_loss: 0.011982\n",
      "[376/01073] train_loss: 0.012262\n",
      "[376/01123] train_loss: 0.012675\n",
      "[376/01173] train_loss: 0.012480\n",
      "[376/01223] train_loss: 0.012240\n",
      "[377/00047] train_loss: 0.013180\n",
      "[377/00097] train_loss: 0.012661\n",
      "[377/00147] train_loss: 0.013009\n",
      "[377/00197] train_loss: 0.012346\n",
      "[377/00247] train_loss: 0.013212\n",
      "[377/00297] train_loss: 0.011791\n",
      "[377/00347] train_loss: 0.012472\n",
      "[377/00397] train_loss: 0.012682\n",
      "[377/00447] train_loss: 0.012381\n",
      "[377/00497] train_loss: 0.013400\n",
      "[377/00547] train_loss: 0.012665\n",
      "[377/00597] train_loss: 0.013059\n",
      "[377/00647] train_loss: 0.012106\n",
      "[377/00697] train_loss: 0.012099\n",
      "[377/00747] train_loss: 0.012408\n",
      "[377/00797] train_loss: 0.011762\n",
      "[377/00847] train_loss: 0.012299\n",
      "[377/00897] train_loss: 0.012087\n",
      "[377/00947] train_loss: 0.012673\n",
      "[377/00997] train_loss: 0.012409\n",
      "[377/01047] train_loss: 0.012084\n",
      "[377/01097] train_loss: 0.011126\n",
      "[377/01147] train_loss: 0.012620\n",
      "[377/01197] train_loss: 0.011865\n",
      "[378/00021] train_loss: 0.013064\n",
      "[378/00071] train_loss: 0.013158\n",
      "[378/00121] train_loss: 0.013031\n",
      "[378/00171] train_loss: 0.013196\n",
      "[378/00221] train_loss: 0.012773\n",
      "[378/00271] train_loss: 0.011843\n",
      "[378/00321] train_loss: 0.011697\n",
      "[378/00371] train_loss: 0.013151\n",
      "[378/00421] train_loss: 0.011763\n",
      "[378/00471] train_loss: 0.012315\n",
      "[378/00521] train_loss: 0.012700\n",
      "[378/00571] train_loss: 0.012186\n",
      "[378/00621] train_loss: 0.011860\n",
      "[378/00671] train_loss: 0.012403\n",
      "[378/00721] train_loss: 0.012378\n",
      "[378/00771] train_loss: 0.012217\n",
      "[378/00821] train_loss: 0.012113\n",
      "[378/00871] train_loss: 0.012645\n",
      "[378/00921] train_loss: 0.011999\n",
      "[378/00971] train_loss: 0.012521\n",
      "[378/01021] train_loss: 0.011750\n",
      "[378/01071] train_loss: 0.012957\n",
      "[378/01121] train_loss: 0.011963\n",
      "[378/01171] train_loss: 0.011821\n",
      "[378/01221] train_loss: 0.011908\n",
      "[379/00045] train_loss: 0.013303\n",
      "[379/00095] train_loss: 0.012952\n",
      "[379/00145] train_loss: 0.012523\n",
      "[379/00195] train_loss: 0.012547\n",
      "[379/00245] train_loss: 0.012945\n",
      "[379/00295] train_loss: 0.012594\n",
      "[379/00345] train_loss: 0.012646\n",
      "[379/00395] train_loss: 0.012110\n",
      "[379/00445] train_loss: 0.011847\n",
      "[379/00495] train_loss: 0.012448\n",
      "[379/00545] train_loss: 0.012427\n",
      "[379/00595] train_loss: 0.012083\n",
      "[379/00645] train_loss: 0.012148\n",
      "[379/00695] train_loss: 0.012619\n",
      "[379/00745] train_loss: 0.012283\n",
      "[379/00795] train_loss: 0.012831\n",
      "[379/00845] train_loss: 0.012426\n",
      "[379/00895] train_loss: 0.012262\n",
      "[379/00945] train_loss: 0.012372\n",
      "[379/00995] train_loss: 0.011794\n",
      "[379/01045] train_loss: 0.012325\n",
      "[379/01095] train_loss: 0.011707\n",
      "[379/01145] train_loss: 0.012052\n",
      "[379/01195] train_loss: 0.012120\n",
      "[380/00019] train_loss: 0.012050\n",
      "[380/00069] train_loss: 0.012520\n",
      "[380/00119] train_loss: 0.012721\n",
      "[380/00169] train_loss: 0.012578\n",
      "[380/00219] train_loss: 0.012434\n",
      "[380/00269] train_loss: 0.012855\n",
      "[380/00319] train_loss: 0.013086\n",
      "[380/00369] train_loss: 0.012259\n",
      "[380/00419] train_loss: 0.012215\n",
      "[380/00469] train_loss: 0.012679\n",
      "[380/00519] train_loss: 0.011548\n",
      "[380/00569] train_loss: 0.012116\n",
      "[380/00619] train_loss: 0.012141\n",
      "[380/00669] train_loss: 0.012241\n",
      "[380/00719] train_loss: 0.012626\n",
      "[380/00769] train_loss: 0.013153\n",
      "[380/00819] train_loss: 0.011741\n",
      "[380/00869] train_loss: 0.012627\n",
      "[380/00919] train_loss: 0.012380\n",
      "[380/00969] train_loss: 0.011725\n",
      "[380/01019] train_loss: 0.012831\n",
      "[380/01069] train_loss: 0.012463\n",
      "[380/01119] train_loss: 0.011731\n",
      "[380/01169] train_loss: 0.012705\n",
      "[380/01219] train_loss: 0.012445\n",
      "[381/00043] train_loss: 0.012973\n",
      "[381/00093] train_loss: 0.012421\n",
      "[381/00143] train_loss: 0.012502\n",
      "[381/00193] train_loss: 0.011978\n",
      "[381/00243] train_loss: 0.012204\n",
      "[381/00293] train_loss: 0.012221\n",
      "[381/00343] train_loss: 0.012679\n",
      "[381/00393] train_loss: 0.012094\n",
      "[381/00443] train_loss: 0.012779\n",
      "[381/00493] train_loss: 0.012697\n",
      "[381/00543] train_loss: 0.012283\n",
      "[381/00593] train_loss: 0.013095\n",
      "[381/00643] train_loss: 0.012872\n",
      "[381/00693] train_loss: 0.013190\n",
      "[381/00743] train_loss: 0.013070\n",
      "[381/00793] train_loss: 0.011802\n",
      "[381/00843] train_loss: 0.012186\n",
      "[381/00893] train_loss: 0.012731\n",
      "[381/00943] train_loss: 0.012491\n",
      "[381/00993] train_loss: 0.013390\n",
      "[381/01043] train_loss: 0.012199\n",
      "[381/01093] train_loss: 0.013077\n",
      "[381/01143] train_loss: 0.012348\n",
      "[381/01193] train_loss: 0.011827\n",
      "[382/00017] train_loss: 0.012517\n",
      "[382/00067] train_loss: 0.012891\n",
      "[382/00117] train_loss: 0.013065\n",
      "[382/00167] train_loss: 0.012506\n",
      "[382/00217] train_loss: 0.012384\n",
      "[382/00267] train_loss: 0.013198\n",
      "[382/00317] train_loss: 0.012040\n",
      "[382/00367] train_loss: 0.012399\n",
      "[382/00417] train_loss: 0.012878\n",
      "[382/00467] train_loss: 0.011954\n",
      "[382/00517] train_loss: 0.012978\n",
      "[382/00567] train_loss: 0.012712\n",
      "[382/00617] train_loss: 0.012680\n",
      "[382/00667] train_loss: 0.012096\n",
      "[382/00717] train_loss: 0.012085\n",
      "[382/00767] train_loss: 0.012608\n",
      "[382/00817] train_loss: 0.012315\n",
      "[382/00867] train_loss: 0.012788\n",
      "[382/00917] train_loss: 0.012820\n",
      "[382/00967] train_loss: 0.011998\n",
      "[382/01017] train_loss: 0.012591\n",
      "[382/01067] train_loss: 0.011663\n",
      "[382/01117] train_loss: 0.012044\n",
      "[382/01167] train_loss: 0.012063\n",
      "[382/01217] train_loss: 0.012948\n",
      "[383/00041] train_loss: 0.012609\n",
      "[383/00091] train_loss: 0.012664\n",
      "[383/00141] train_loss: 0.012755\n",
      "[383/00191] train_loss: 0.013195\n",
      "[383/00241] train_loss: 0.013059\n",
      "[383/00291] train_loss: 0.012281\n",
      "[383/00341] train_loss: 0.012439\n",
      "[383/00391] train_loss: 0.012180\n",
      "[383/00441] train_loss: 0.013017\n",
      "[383/00491] train_loss: 0.013059\n",
      "[383/00541] train_loss: 0.012034\n",
      "[383/00591] train_loss: 0.012304\n",
      "[383/00641] train_loss: 0.011446\n",
      "[383/00691] train_loss: 0.012007\n",
      "[383/00741] train_loss: 0.011702\n",
      "[383/00791] train_loss: 0.012433\n",
      "[383/00841] train_loss: 0.011986\n",
      "[383/00891] train_loss: 0.011839\n",
      "[383/00941] train_loss: 0.012402\n",
      "[383/00991] train_loss: 0.012154\n",
      "[383/01041] train_loss: 0.012518\n",
      "[383/01091] train_loss: 0.012460\n",
      "[383/01141] train_loss: 0.012213\n",
      "[383/01191] train_loss: 0.011793\n",
      "[384/00015] train_loss: 0.012945\n",
      "[384/00065] train_loss: 0.012938\n",
      "[384/00115] train_loss: 0.012388\n",
      "[384/00165] train_loss: 0.013614\n",
      "[384/00215] train_loss: 0.012183\n",
      "[384/00265] train_loss: 0.012174\n",
      "[384/00315] train_loss: 0.012123\n",
      "[384/00365] train_loss: 0.012460\n",
      "[384/00415] train_loss: 0.012912\n",
      "[384/00465] train_loss: 0.012169\n",
      "[384/00515] train_loss: 0.012857\n",
      "[384/00565] train_loss: 0.012610\n",
      "[384/00615] train_loss: 0.012689\n",
      "[384/00665] train_loss: 0.012066\n",
      "[384/00715] train_loss: 0.012396\n",
      "[384/00765] train_loss: 0.012007\n",
      "[384/00815] train_loss: 0.012567\n",
      "[384/00865] train_loss: 0.012461\n",
      "[384/00915] train_loss: 0.011548\n",
      "[384/00965] train_loss: 0.012806\n",
      "[384/01015] train_loss: 0.012000\n",
      "[384/01065] train_loss: 0.012804\n",
      "[384/01115] train_loss: 0.011810\n",
      "[384/01165] train_loss: 0.012154\n",
      "[384/01215] train_loss: 0.013162\n",
      "[385/00039] train_loss: 0.012564\n",
      "[385/00089] train_loss: 0.013109\n",
      "[385/00139] train_loss: 0.012870\n",
      "[385/00189] train_loss: 0.012593\n",
      "[385/00239] train_loss: 0.012657\n",
      "[385/00289] train_loss: 0.012579\n",
      "[385/00339] train_loss: 0.011779\n",
      "[385/00389] train_loss: 0.012310\n",
      "[385/00439] train_loss: 0.012579\n",
      "[385/00489] train_loss: 0.013015\n",
      "[385/00539] train_loss: 0.012161\n",
      "[385/00589] train_loss: 0.012314\n",
      "[385/00639] train_loss: 0.012507\n",
      "[385/00689] train_loss: 0.011873\n",
      "[385/00739] train_loss: 0.012369\n",
      "[385/00789] train_loss: 0.011608\n",
      "[385/00839] train_loss: 0.012251\n",
      "[385/00889] train_loss: 0.012785\n",
      "[385/00939] train_loss: 0.011527\n",
      "[385/00989] train_loss: 0.011966\n",
      "[385/01039] train_loss: 0.011938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[385/01089] train_loss: 0.012927\n",
      "[385/01139] train_loss: 0.011846\n",
      "[385/01189] train_loss: 0.012770\n",
      "[386/00013] train_loss: 0.012340\n",
      "[386/00063] train_loss: 0.013627\n",
      "[386/00113] train_loss: 0.013229\n",
      "[386/00163] train_loss: 0.012649\n",
      "[386/00213] train_loss: 0.012829\n",
      "[386/00263] train_loss: 0.012394\n",
      "[386/00313] train_loss: 0.011749\n",
      "[386/00363] train_loss: 0.011858\n",
      "[386/00413] train_loss: 0.012604\n",
      "[386/00463] train_loss: 0.011866\n",
      "[386/00513] train_loss: 0.012684\n",
      "[386/00563] train_loss: 0.012377\n",
      "[386/00613] train_loss: 0.011764\n",
      "[386/00663] train_loss: 0.012486\n",
      "[386/00713] train_loss: 0.012718\n",
      "[386/00763] train_loss: 0.012798\n",
      "[386/00813] train_loss: 0.011705\n",
      "[386/00863] train_loss: 0.012659\n",
      "[386/00913] train_loss: 0.012221\n",
      "[386/00963] train_loss: 0.012454\n",
      "[386/01013] train_loss: 0.012280\n",
      "[386/01063] train_loss: 0.012334\n",
      "[386/01113] train_loss: 0.011873\n",
      "[386/01163] train_loss: 0.011569\n",
      "[386/01213] train_loss: 0.012778\n",
      "[387/00037] train_loss: 0.012774\n",
      "[387/00087] train_loss: 0.012787\n",
      "[387/00137] train_loss: 0.012254\n",
      "[387/00187] train_loss: 0.012650\n",
      "[387/00237] train_loss: 0.013544\n",
      "[387/00287] train_loss: 0.012328\n",
      "[387/00337] train_loss: 0.012692\n",
      "[387/00387] train_loss: 0.012085\n",
      "[387/00437] train_loss: 0.012458\n",
      "[387/00487] train_loss: 0.013166\n",
      "[387/00537] train_loss: 0.012328\n",
      "[387/00587] train_loss: 0.011771\n",
      "[387/00637] train_loss: 0.012600\n",
      "[387/00687] train_loss: 0.012492\n",
      "[387/00737] train_loss: 0.013273\n",
      "[387/00787] train_loss: 0.011810\n",
      "[387/00837] train_loss: 0.012800\n",
      "[387/00887] train_loss: 0.012723\n",
      "[387/00937] train_loss: 0.011489\n",
      "[387/00987] train_loss: 0.012144\n",
      "[387/01037] train_loss: 0.011860\n",
      "[387/01087] train_loss: 0.011779\n",
      "[387/01137] train_loss: 0.011648\n",
      "[387/01187] train_loss: 0.012408\n",
      "[388/00011] train_loss: 0.012407\n",
      "[388/00061] train_loss: 0.012686\n",
      "[388/00111] train_loss: 0.012787\n",
      "[388/00161] train_loss: 0.012475\n",
      "[388/00211] train_loss: 0.012793\n",
      "[388/00261] train_loss: 0.012894\n",
      "[388/00311] train_loss: 0.011440\n",
      "[388/00361] train_loss: 0.012749\n",
      "[388/00411] train_loss: 0.011968\n",
      "[388/00461] train_loss: 0.011756\n",
      "[388/00511] train_loss: 0.012533\n",
      "[388/00561] train_loss: 0.011801\n",
      "[388/00611] train_loss: 0.011540\n",
      "[388/00661] train_loss: 0.012857\n",
      "[388/00711] train_loss: 0.012425\n",
      "[388/00761] train_loss: 0.012772\n",
      "[388/00811] train_loss: 0.011757\n",
      "[388/00861] train_loss: 0.012761\n",
      "[388/00911] train_loss: 0.012419\n",
      "[388/00961] train_loss: 0.012329\n",
      "[388/01011] train_loss: 0.012433\n",
      "[388/01061] train_loss: 0.013079\n",
      "[388/01111] train_loss: 0.012539\n",
      "[388/01161] train_loss: 0.012083\n",
      "[388/01211] train_loss: 0.012322\n",
      "[389/00035] train_loss: 0.012587\n",
      "[389/00085] train_loss: 0.012417\n",
      "[389/00135] train_loss: 0.013154\n",
      "[389/00185] train_loss: 0.012305\n",
      "[389/00235] train_loss: 0.012400\n",
      "[389/00285] train_loss: 0.011935\n",
      "[389/00335] train_loss: 0.013265\n",
      "[389/00385] train_loss: 0.012190\n",
      "[389/00435] train_loss: 0.012732\n",
      "[389/00485] train_loss: 0.011935\n",
      "[389/00535] train_loss: 0.011995\n",
      "[389/00585] train_loss: 0.012560\n",
      "[389/00635] train_loss: 0.012949\n",
      "[389/00685] train_loss: 0.013094\n",
      "[389/00735] train_loss: 0.011985\n",
      "[389/00785] train_loss: 0.011467\n",
      "[389/00835] train_loss: 0.012658\n",
      "[389/00885] train_loss: 0.011359\n",
      "[389/00935] train_loss: 0.011771\n",
      "[389/00985] train_loss: 0.012410\n",
      "[389/01035] train_loss: 0.012112\n",
      "[389/01085] train_loss: 0.012467\n",
      "[389/01135] train_loss: 0.012348\n",
      "[389/01185] train_loss: 0.012081\n",
      "[390/00009] train_loss: 0.012400\n",
      "[390/00059] train_loss: 0.012086\n",
      "[390/00109] train_loss: 0.012838\n",
      "[390/00159] train_loss: 0.012686\n",
      "[390/00209] train_loss: 0.012563\n",
      "[390/00259] train_loss: 0.012689\n",
      "[390/00309] train_loss: 0.012502\n",
      "[390/00359] train_loss: 0.011736\n",
      "[390/00409] train_loss: 0.012051\n",
      "[390/00459] train_loss: 0.012727\n",
      "[390/00509] train_loss: 0.013126\n",
      "[390/00559] train_loss: 0.012783\n",
      "[390/00609] train_loss: 0.012198\n",
      "[390/00659] train_loss: 0.011711\n",
      "[390/00709] train_loss: 0.012229\n",
      "[390/00759] train_loss: 0.011891\n",
      "[390/00809] train_loss: 0.012851\n",
      "[390/00859] train_loss: 0.012138\n",
      "[390/00909] train_loss: 0.012636\n",
      "[390/00959] train_loss: 0.012547\n",
      "[390/01009] train_loss: 0.011977\n",
      "[390/01059] train_loss: 0.012217\n",
      "[390/01109] train_loss: 0.012277\n",
      "[390/01159] train_loss: 0.012038\n",
      "[390/01209] train_loss: 0.011943\n",
      "[391/00033] train_loss: 0.012841\n",
      "[391/00083] train_loss: 0.013470\n",
      "[391/00133] train_loss: 0.012544\n",
      "[391/00183] train_loss: 0.012682\n",
      "[391/00233] train_loss: 0.012572\n",
      "[391/00283] train_loss: 0.012365\n",
      "[391/00333] train_loss: 0.012693\n",
      "[391/00383] train_loss: 0.012169\n",
      "[391/00433] train_loss: 0.013232\n",
      "[391/00483] train_loss: 0.012214\n",
      "[391/00533] train_loss: 0.012473\n",
      "[391/00583] train_loss: 0.012025\n",
      "[391/00633] train_loss: 0.012529\n",
      "[391/00683] train_loss: 0.012033\n",
      "[391/00733] train_loss: 0.012368\n",
      "[391/00783] train_loss: 0.012337\n",
      "[391/00833] train_loss: 0.012175\n",
      "[391/00883] train_loss: 0.012560\n",
      "[391/00933] train_loss: 0.012271\n",
      "[391/00983] train_loss: 0.012266\n",
      "[391/01033] train_loss: 0.012506\n",
      "[391/01083] train_loss: 0.011596\n",
      "[391/01133] train_loss: 0.011855\n",
      "[391/01183] train_loss: 0.012406\n",
      "[392/00007] train_loss: 0.011811\n",
      "[392/00057] train_loss: 0.013943\n",
      "[392/00107] train_loss: 0.013122\n",
      "[392/00157] train_loss: 0.012501\n",
      "[392/00207] train_loss: 0.012919\n",
      "[392/00257] train_loss: 0.012949\n",
      "[392/00307] train_loss: 0.013166\n",
      "[392/00357] train_loss: 0.012578\n",
      "[392/00407] train_loss: 0.012090\n",
      "[392/00457] train_loss: 0.012552\n",
      "[392/00507] train_loss: 0.012253\n",
      "[392/00557] train_loss: 0.012383\n",
      "[392/00607] train_loss: 0.012066\n",
      "[392/00657] train_loss: 0.011933\n",
      "[392/00707] train_loss: 0.012000\n",
      "[392/00757] train_loss: 0.012528\n",
      "[392/00807] train_loss: 0.011675\n",
      "[392/00857] train_loss: 0.012366\n",
      "[392/00907] train_loss: 0.012112\n",
      "[392/00957] train_loss: 0.011974\n",
      "[392/01007] train_loss: 0.011356\n",
      "[392/01057] train_loss: 0.012468\n",
      "[392/01107] train_loss: 0.012540\n",
      "[392/01157] train_loss: 0.012157\n",
      "[392/01207] train_loss: 0.012052\n",
      "[393/00031] train_loss: 0.012864\n",
      "[393/00081] train_loss: 0.012603\n",
      "[393/00131] train_loss: 0.012345\n",
      "[393/00181] train_loss: 0.013133\n",
      "[393/00231] train_loss: 0.012621\n",
      "[393/00281] train_loss: 0.012593\n",
      "[393/00331] train_loss: 0.012258\n",
      "[393/00381] train_loss: 0.012514\n",
      "[393/00431] train_loss: 0.011770\n",
      "[393/00481] train_loss: 0.011663\n",
      "[393/00531] train_loss: 0.012336\n",
      "[393/00581] train_loss: 0.013170\n",
      "[393/00631] train_loss: 0.011916\n",
      "[393/00681] train_loss: 0.012552\n",
      "[393/00731] train_loss: 0.012321\n",
      "[393/00781] train_loss: 0.012077\n",
      "[393/00831] train_loss: 0.011573\n",
      "[393/00881] train_loss: 0.013055\n",
      "[393/00931] train_loss: 0.012339\n",
      "[393/00981] train_loss: 0.011904\n",
      "[393/01031] train_loss: 0.011696\n",
      "[393/01081] train_loss: 0.011970\n",
      "[393/01131] train_loss: 0.012013\n",
      "[393/01181] train_loss: 0.011524\n",
      "[394/00005] train_loss: 0.012795\n",
      "[394/00055] train_loss: 0.012890\n",
      "[394/00105] train_loss: 0.012801\n",
      "[394/00155] train_loss: 0.013122\n",
      "[394/00205] train_loss: 0.012013\n",
      "[394/00255] train_loss: 0.012685\n",
      "[394/00305] train_loss: 0.012332\n",
      "[394/00355] train_loss: 0.012460\n",
      "[394/00405] train_loss: 0.012306\n",
      "[394/00455] train_loss: 0.012463\n",
      "[394/00505] train_loss: 0.012047\n",
      "[394/00555] train_loss: 0.012223\n",
      "[394/00605] train_loss: 0.012353\n",
      "[394/00655] train_loss: 0.012099\n",
      "[394/00705] train_loss: 0.011900\n",
      "[394/00755] train_loss: 0.012514\n",
      "[394/00805] train_loss: 0.011944\n",
      "[394/00855] train_loss: 0.012360\n",
      "[394/00905] train_loss: 0.011872\n",
      "[394/00955] train_loss: 0.011252\n",
      "[394/01005] train_loss: 0.011751\n",
      "[394/01055] train_loss: 0.012007\n",
      "[394/01105] train_loss: 0.013075\n",
      "[394/01155] train_loss: 0.012575\n",
      "[394/01205] train_loss: 0.013013\n",
      "[395/00029] train_loss: 0.012938\n",
      "[395/00079] train_loss: 0.012694\n",
      "[395/00129] train_loss: 0.013104\n",
      "[395/00179] train_loss: 0.012065\n",
      "[395/00229] train_loss: 0.012159\n",
      "[395/00279] train_loss: 0.013335\n",
      "[395/00329] train_loss: 0.012925\n",
      "[395/00379] train_loss: 0.012253\n",
      "[395/00429] train_loss: 0.012774\n",
      "[395/00479] train_loss: 0.012489\n",
      "[395/00529] train_loss: 0.011900\n",
      "[395/00579] train_loss: 0.011887\n",
      "[395/00629] train_loss: 0.011967\n",
      "[395/00679] train_loss: 0.011710\n",
      "[395/00729] train_loss: 0.012165\n",
      "[395/00779] train_loss: 0.011752\n",
      "[395/00829] train_loss: 0.012027\n",
      "[395/00879] train_loss: 0.012649\n",
      "[395/00929] train_loss: 0.011670\n",
      "[395/00979] train_loss: 0.012718\n",
      "[395/01029] train_loss: 0.012520\n",
      "[395/01079] train_loss: 0.011796\n",
      "[395/01129] train_loss: 0.011914\n",
      "[395/01179] train_loss: 0.012095\n",
      "[396/00003] train_loss: 0.012413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[396/00053] train_loss: 0.012746\n",
      "[396/00103] train_loss: 0.013117\n",
      "[396/00153] train_loss: 0.012417\n",
      "[396/00203] train_loss: 0.012916\n",
      "[396/00253] train_loss: 0.013190\n",
      "[396/00303] train_loss: 0.012287\n",
      "[396/00353] train_loss: 0.012028\n",
      "[396/00403] train_loss: 0.012703\n",
      "[396/00453] train_loss: 0.012310\n",
      "[396/00503] train_loss: 0.012544\n",
      "[396/00553] train_loss: 0.012769\n",
      "[396/00603] train_loss: 0.013118\n",
      "[396/00653] train_loss: 0.012186\n",
      "[396/00703] train_loss: 0.011923\n",
      "[396/00753] train_loss: 0.012128\n",
      "[396/00803] train_loss: 0.011605\n",
      "[396/00853] train_loss: 0.012123\n",
      "[396/00903] train_loss: 0.012156\n",
      "[396/00953] train_loss: 0.011346\n",
      "[396/01003] train_loss: 0.011625\n",
      "[396/01053] train_loss: 0.011707\n",
      "[396/01103] train_loss: 0.012244\n",
      "[396/01153] train_loss: 0.012237\n",
      "[396/01203] train_loss: 0.012483\n",
      "[397/00027] train_loss: 0.013024\n",
      "[397/00077] train_loss: 0.012669\n",
      "[397/00127] train_loss: 0.013263\n",
      "[397/00177] train_loss: 0.012286\n",
      "[397/00227] train_loss: 0.012730\n",
      "[397/00277] train_loss: 0.012642\n",
      "[397/00327] train_loss: 0.011989\n",
      "[397/00377] train_loss: 0.012968\n",
      "[397/00427] train_loss: 0.012783\n",
      "[397/00477] train_loss: 0.012189\n",
      "[397/00527] train_loss: 0.012857\n",
      "[397/00577] train_loss: 0.011753\n",
      "[397/00627] train_loss: 0.012228\n",
      "[397/00677] train_loss: 0.012055\n",
      "[397/00727] train_loss: 0.012214\n",
      "[397/00777] train_loss: 0.012529\n",
      "[397/00827] train_loss: 0.012603\n",
      "[397/00877] train_loss: 0.012174\n",
      "[397/00927] train_loss: 0.011986\n",
      "[397/00977] train_loss: 0.011875\n",
      "[397/01027] train_loss: 0.012503\n",
      "[397/01077] train_loss: 0.012190\n",
      "[397/01127] train_loss: 0.011758\n",
      "[397/01177] train_loss: 0.012120\n",
      "[398/00001] train_loss: 0.012261\n",
      "[398/00051] train_loss: 0.012769\n",
      "[398/00101] train_loss: 0.012594\n",
      "[398/00151] train_loss: 0.012756\n",
      "[398/00201] train_loss: 0.012280\n",
      "[398/00251] train_loss: 0.012027\n",
      "[398/00301] train_loss: 0.012671\n",
      "[398/00351] train_loss: 0.012203\n",
      "[398/00401] train_loss: 0.012033\n",
      "[398/00451] train_loss: 0.012096\n",
      "[398/00501] train_loss: 0.012781\n",
      "[398/00551] train_loss: 0.012512\n",
      "[398/00601] train_loss: 0.012915\n",
      "[398/00651] train_loss: 0.012367\n",
      "[398/00701] train_loss: 0.012332\n",
      "[398/00751] train_loss: 0.011476\n",
      "[398/00801] train_loss: 0.012128\n",
      "[398/00851] train_loss: 0.012195\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-e5d0b30a260d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m }\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrain_deepsdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneralization_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/ml3d/E3/exercise_3/training/train_deepsdf.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/ml3d/E3/exercise_3/training/train_deepsdf.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, latent_vectors, train_dataloader, device, config)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;31m# loss logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mtrain_loss_running\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0miteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from exercise_3.training import train_deepsdf\n",
    "\n",
    "generalization_config = {\n",
    "    'experiment_name': '3_2_deepsdf_generalization',\n",
    "    'device': 'cuda:0',  # run this on a gpu for a reasonable training time\n",
    "    'is_overfit': False,\n",
    "    'num_sample_points': 4096, # you can adjust this such that the model fits on your gpu\n",
    "    'latent_code_length': 256,\n",
    "    'batch_size': 1,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate_model': 0.0005,\n",
    "    'learning_rate_code': 0.001,\n",
    "    'lambda_code_regularization': 0.0001,\n",
    "    'max_epochs': 2000,  # not necessary to run for 2000 epochs if you're short on time, at 500 epochs you should start to see reasonable results\n",
    "    'print_every_n': 50,\n",
    "    'visualize_every_n': 5000,\n",
    "}\n",
    "\n",
    "train_deepsdf.main(generalization_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f) Inference using the trained model on observed SDF values\n",
    "\n",
    "Fill in the inference script `exercise_3/inference/infer_deepsdf.py`. Note that it's not simply a forward pass, but an optimization of the latent code such that we have lowest error on observed SDF values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_3.inference.infer_deepsdf import InferenceHandlerDeepSDF\n",
    "\n",
    "device = torch.device('cuda:0')  # change this to cpu if you're not using a gpu\n",
    "\n",
    "inference_handler = InferenceHandlerDeepSDF(256, \"exercise_3/runs/3_2_deepsdf_generalization\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we try inference on a shape from validation set, for which we have a complete observation of sdf values. This is an easier problem as compared to shape completion,\n",
    "since we have all the information already in the input.\n",
    "\n",
    "Let's visualize the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations with negative SDF (inside)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c576ec892767454da813760570b4a638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations with positive SDF (outside)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d80f2a092b0a42c1a95b7aa9c67e0474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get observed data\n",
    "points, sdf = ShapeImplicit.get_all_sdf_samples(\"b351e06f5826444c19fb4103277a6b93\")\n",
    "\n",
    "inside_points = points[sdf[:, 0] < 0, :].numpy()\n",
    "outside_points = points[sdf[:, 0] > 0, :].numpy()\n",
    "\n",
    "# visualize observed points; you'll observe that the observations are very complete\n",
    "print('Observations with negative SDF (inside)')\n",
    "visualize_pointcloud(inside_points, 0.025, flip_axes=True)\n",
    "print('Observations with positive SDF (outside)')\n",
    "visualize_pointcloud(outside_points, 0.025, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Reconstruction on these observations with the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00000] optim_loss: 0.031528\n",
      "[00050] optim_loss: 0.007804\n",
      "[00100] optim_loss: 0.006389\n",
      "[00150] optim_loss: 0.006109\n",
      "[00200] optim_loss: 0.005739\n",
      "[00250] optim_loss: 0.005900\n",
      "[00300] optim_loss: 0.005837\n",
      "[00350] optim_loss: 0.005562\n",
      "[00400] optim_loss: 0.005492\n",
      "[00450] optim_loss: 0.005538\n",
      "[00500] optim_loss: 0.005596\n",
      "[00550] optim_loss: 0.005637\n",
      "[00600] optim_loss: 0.005580\n",
      "[00650] optim_loss: 0.005271\n",
      "[00700] optim_loss: 0.005461\n",
      "[00750] optim_loss: 0.005309\n",
      "Optimization complete.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2efa2dd492d4022a185f9c7fcb3a08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reconstruct\n",
    "vertices, faces = inference_handler.reconstruct(points, sdf, 800)\n",
    "# visualize\n",
    "visualize_mesh(vertices, faces, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can try the shape completion task, i.e., inference on a shape from validation set, for which we do not have a complete observation of sdf values. The observed points are visualized below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations with negative SDF (inside)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44bfa60f2c434524a502bee11c5cd763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations with positive SDF (outside)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf13fc145a644b98a0bd32cee165109b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get observed data\n",
    "points, sdf = ShapeImplicit.get_all_sdf_samples(\"b351e06f5826444c19fb4103277a6b93_incomplete\")\n",
    "\n",
    "inside_points = points[sdf[:, 0] < 0, :].numpy()\n",
    "outside_points = points[sdf[:, 0] > 0, :].numpy()\n",
    "\n",
    "# visualize observed points; you'll observe that the observations are incomplete\n",
    "# making this is a shape completion task\n",
    "print('Observations with negative SDF (inside)')\n",
    "visualize_pointcloud(inside_points, 0.025, flip_axes=True)\n",
    "print('Observations with positive SDF (outside)')\n",
    "visualize_pointcloud(outside_points, 0.025, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape completion using the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00000] optim_loss: 0.030561\n",
      "[00050] optim_loss: 0.008198\n",
      "[00100] optim_loss: 0.005934\n",
      "[00150] optim_loss: 0.005514\n",
      "[00200] optim_loss: 0.005232\n",
      "[00250] optim_loss: 0.005235\n",
      "[00300] optim_loss: 0.005409\n",
      "[00350] optim_loss: 0.005180\n",
      "[00400] optim_loss: 0.005258\n",
      "[00450] optim_loss: 0.005003\n",
      "[00500] optim_loss: 0.005119\n",
      "[00550] optim_loss: 0.004951\n",
      "[00600] optim_loss: 0.004798\n",
      "[00650] optim_loss: 0.005049\n",
      "[00700] optim_loss: 0.005031\n",
      "[00750] optim_loss: 0.004873\n",
      "Optimization complete.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b0b4f868bd4d00bac4e3bf57a1c5de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reconstruct\n",
    "vertices, faces = inference_handler.reconstruct(points, sdf, 800)\n",
    "# visualize\n",
    "visualize_mesh(vertices, faces, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (g) Latent space interpolation\n",
    "\n",
    "The latent space learned by DeepSDF is interpolatable, meaning that decoding latent codes from this space produced meaningful shapes. Given two latent codes, a linearly interpolatable latent space will decode\n",
    "each of the intermediate codes to some valid shape. Let's see if this holds for our trained model.\n",
    "\n",
    "We'll pick two shapes from the train set as visualized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT Shape A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f2c193a10a468096f529be8421edcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT Shape B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a631607ad045fe81c7e33de6dbb0f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from exercise_3.data.shape_implicit import ShapeImplicit\n",
    "from exercise_3.util.visualization import visualize_mesh\n",
    "\n",
    "mesh = ShapeImplicit.get_mesh(\"494fe53da65650b8c358765b76c296\")\n",
    "print('GT Shape A')\n",
    "visualize_mesh(mesh.vertices, mesh.faces, flip_axes=True)\n",
    "\n",
    "mesh = ShapeImplicit.get_mesh(\"5ca1ef55ff5f68501921e7a85cf9da35\")\n",
    "print('GT Shape B')\n",
    "visualize_mesh(mesh.vertices, mesh.faces, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Implement the missing parts in `exercise_3/inference/infer_deepsdf.py` such that it interpolates two given latent vectors, and run the code fragement below once done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_3.inference.infer_deepsdf import InferenceHandlerDeepSDF\n",
    "\n",
    "inference_handler = InferenceHandlerDeepSDF(256, \"exercise_3/runs/3_2_deepsdf_generalization\", torch.device('cuda:0'))\n",
    "# interpolate; also exports interpolated meshes to disk\n",
    "inference_handler.interpolate('494fe53da65650b8c358765b76c296', '5ca1ef55ff5f68501921e7a85cf9da35', 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the interpolation below. If everything works out correctly, you should see a smooth transformation between the shapes, with all intermediate shapes being valid sofas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_3.util.mesh_collection_to_gif import  meshes_to_gif\n",
    "from exercise_3.util.misc import show_gif\n",
    "\n",
    "# create list of meshes (just exported) to be visualized\n",
    "mesh_paths = sorted([x for x in Path(\"exercise_3/runs/3_2_deepsdf_generalization/interpolation\").iterdir() if int(x.name.split('.')[0].split(\"_\")[1]) == 0], key=lambda x: int(x.name.split('.')[0].split(\"_\")[0]))\n",
    "mesh_paths = mesh_paths + mesh_paths[::-1]\n",
    "\n",
    "# create a visualization of the interpolation process\n",
    "meshes_to_gif(mesh_paths, \"exercise_3/runs/3_2_deepsdf_generalization/latent_interp.gif\", 20)\n",
    "show_gif(\"exercise_3/runs/3_2_deepsdf_generalization/latent_interp.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Submission\n",
    "\n",
    "This is the end of exercise 3 🙂. Please create a zip containing all files we provided, everything you modified, your visualization images/gif (no need to submit generated OBJs), including your checkpoints. Name it with your matriculation number(s) as described in exercise 1. Make sure this notebook can be run without problems. Then, submit via Moodle.\n",
    "\n",
    "**Note**: The maximum submission file size limit for Moodle is 100M. You do not need to submit your overfitting checkpoints; however, the generalization checkpoint will be >200M. The easiest way to still be able to submit that one is to split it with zip like this: `zip -s 100M model_best.ckpt.zip model_best.ckpt` which creates a `.zip` and a `.z01`. You can then submit both files alongside another zip containing all your code and outputs.\n",
    "\n",
    "**Submission Deadline**: 09.06.2021, 23:55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "[1] Dai, Angela, Charles Ruizhongtai Qi, and Matthias Nießner. \"Shape completion using 3d-encoder-predictor cnns and shape synthesis.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.\n",
    "\n",
    "[2] Park, Jeong Joon, et al. \"Deepsdf: Learning continuous signed distance functions for shape representation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
